{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29907,"status":"ok","timestamp":1707597280359,"user":{"displayName":"Divyanshu Verma","userId":"14078105191069123729"},"user_tz":300},"id":"y8qT_9YmuIb8","outputId":"df0eb3e1-b3f8-43c2-8119-22071efaad01"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1336,"status":"ok","timestamp":1714069135266,"user":{"displayName":"Divyanshu Verma","userId":"14078105191069123729"},"user_tz":240},"id":"HyYsb0jfOekC","outputId":"79b18b85-f3b4-482f-b777-aba58ac831ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'vldb2021_fauce'...\n","remote: Enumerating objects: 512, done.\u001b[K\n","remote: Counting objects: 100% (70/70), done.\u001b[K\n","remote: Compressing objects: 100% (44/44), done.\u001b[K\n","remote: Total 512 (delta 34), reused 50 (delta 26), pack-reused 442\u001b[K\n","Receiving objects: 100% (512/512), 4.21 MiB | 12.38 MiB/s, done.\n","Resolving deltas: 100% (254/254), done.\n"]}],"source":["!git clone -b dverma/update_fauce https://github.com/vermadivyanshu/vldb2021_fauce"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23806,"status":"ok","timestamp":1707597334561,"user":{"displayName":"Divyanshu Verma","userId":"14078105191069123729"},"user_tz":300},"id":"hxRuaMDVOmD7","outputId":"b8c080c2-cf4b-4431-c68f-4e88d41810c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 13.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package logrotate.\n","(Reading database ... 121749 files and directories currently installed.)\n","Preparing to unpack .../00-logrotate_3.19.0-1ubuntu1.1_amd64.deb ...\n","Unpacking logrotate (3.19.0-1ubuntu1.1) ...\n","Selecting previously unselected package netbase.\n","Preparing to unpack .../01-netbase_6.3_all.deb ...\n","Unpacking netbase (6.3) ...\n","Selecting previously unselected package libcommon-sense-perl:amd64.\n","Preparing to unpack .../02-libcommon-sense-perl_3.75-2build1_amd64.deb ...\n","Unpacking libcommon-sense-perl:amd64 (3.75-2build1) ...\n","Selecting previously unselected package libjson-perl.\n","Preparing to unpack .../03-libjson-perl_4.04000-1_all.deb ...\n","Unpacking libjson-perl (4.04000-1) ...\n","Selecting previously unselected package libtypes-serialiser-perl.\n","Preparing to unpack .../04-libtypes-serialiser-perl_1.01-1_all.deb ...\n","Unpacking libtypes-serialiser-perl (1.01-1) ...\n","Selecting previously unselected package libjson-xs-perl.\n","Preparing to unpack .../05-libjson-xs-perl_4.030-1build3_amd64.deb ...\n","Unpacking libjson-xs-perl (4.030-1build3) ...\n","Selecting previously unselected package postgresql-client-common.\n","Preparing to unpack .../06-postgresql-client-common_238_all.deb ...\n","Unpacking postgresql-client-common (238) ...\n","Selecting previously unselected package postgresql-client-14.\n","Preparing to unpack .../07-postgresql-client-14_14.10-0ubuntu0.22.04.1_amd64.deb ...\n","Unpacking postgresql-client-14 (14.10-0ubuntu0.22.04.1) ...\n","Selecting previously unselected package ssl-cert.\n","Preparing to unpack .../08-ssl-cert_1.1.2_all.deb ...\n","Unpacking ssl-cert (1.1.2) ...\n","Selecting previously unselected package postgresql-common.\n","Preparing to unpack .../09-postgresql-common_238_all.deb ...\n","Adding 'diversion of /usr/bin/pg_config to /usr/bin/pg_config.libpq-dev by postgresql-common'\n","Unpacking postgresql-common (238) ...\n","Selecting previously unselected package postgresql-14.\n","Preparing to unpack .../10-postgresql-14_14.10-0ubuntu0.22.04.1_amd64.deb ...\n","Unpacking postgresql-14 (14.10-0ubuntu0.22.04.1) ...\n","Selecting previously unselected package postgresql.\n","Preparing to unpack .../11-postgresql_14+238_all.deb ...\n","Unpacking postgresql (14+238) ...\n","Selecting previously unselected package sysstat.\n","Preparing to unpack .../12-sysstat_12.5.2-2ubuntu0.2_amd64.deb ...\n","Unpacking sysstat (12.5.2-2ubuntu0.2) ...\n","Setting up logrotate (3.19.0-1ubuntu1.1) ...\n","Created symlink /etc/systemd/system/timers.target.wants/logrotate.timer → /lib/systemd/system/logrotate.timer.\n","Setting up libcommon-sense-perl:amd64 (3.75-2build1) ...\n","Setting up ssl-cert (1.1.2) ...\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n","debconf: falling back to frontend: Readline\n","Setting up libtypes-serialiser-perl (1.01-1) ...\n","Setting up libjson-perl (4.04000-1) ...\n","Setting up netbase (6.3) ...\n","Setting up sysstat (12.5.2-2ubuntu0.2) ...\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n","debconf: falling back to frontend: Readline\n","\n","Creating config file /etc/default/sysstat with new version\n","update-alternatives: using /usr/bin/sar.sysstat to provide /usr/bin/sar (sar) in auto mode\n","Created symlink /etc/systemd/system/sysstat.service.wants/sysstat-collect.timer → /lib/systemd/system/sysstat-collect.timer.\n","Created symlink /etc/systemd/system/sysstat.service.wants/sysstat-summary.timer → /lib/systemd/system/sysstat-summary.timer.\n","Created symlink /etc/systemd/system/multi-user.target.wants/sysstat.service → /lib/systemd/system/sysstat.service.\n","Setting up postgresql-client-common (238) ...\n","Setting up libjson-xs-perl (4.030-1build3) ...\n","Setting up postgresql-client-14 (14.10-0ubuntu0.22.04.1) ...\n","update-alternatives: using /usr/share/postgresql/14/man/man1/psql.1.gz to provide /usr/share/man/man1/psql.1.gz (psql.1.gz) in auto mode\n","Setting up postgresql-common (238) ...\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n","debconf: falling back to frontend: Readline\n","Adding user postgres to group ssl-cert\n","\n","Creating config file /etc/postgresql-common/createcluster.conf with new version\n","Building PostgreSQL dictionaries from installed myspell/hunspell packages...\n","Removing obsolete dictionary files:\n","Created symlink /etc/systemd/system/multi-user.target.wants/postgresql.service → /lib/systemd/system/postgresql.service.\n","Setting up postgresql-14 (14.10-0ubuntu0.22.04.1) ...\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n","debconf: falling back to frontend: Readline\n","Creating new PostgreSQL cluster 14/main ...\n","/usr/lib/postgresql/14/bin/initdb -D /var/lib/postgresql/14/main --auth-local peer --auth-host scram-sha-256 --no-instructions\n","The files belonging to this database system will be owned by user \"postgres\".\n","This user must also own the server process.\n","\n","The database cluster will be initialized with locale \"en_US.UTF-8\".\n","The default database encoding has accordingly been set to \"UTF8\".\n","The default text search configuration will be set to \"english\".\n","\n","Data page checksums are disabled.\n","\n","fixing permissions on existing directory /var/lib/postgresql/14/main ... ok\n","creating subdirectories ... ok\n","selecting dynamic shared memory implementation ... posix\n","selecting default max_connections ... 100\n","selecting default shared_buffers ... 128MB\n","selecting default time zone ... Etc/UTC\n","creating configuration files ... ok\n","running bootstrap script ... ok\n","performing post-bootstrap initialization ... ok\n","syncing data to disk ... ok\n","update-alternatives: using /usr/share/postgresql/14/man/man1/postmaster.1.gz to provide /usr/share/man/man1/postmaster.1.gz (postmaster.1.gz) in auto mode\n","invoke-rc.d: could not determine current runlevel\n","invoke-rc.d: policy-rc.d denied execution of start.\n","Setting up postgresql (14+238) ...\n","Processing triggers for man-db (2.10.2-1) ...\n"," * Starting PostgreSQL 14 database server\n","   ...done.\n","ALTER ROLE\n","NOTICE:  database \"imdbload\" does not exist, skipping\n","DROP DATABASE\n","CREATE DATABASE\n"]}],"source":["# Install postgresql server\n","!sudo apt-get -y -qq update\n","!sudo apt-get -y -qq install postgresql\n","!sudo service postgresql start\n","\n","# Setup a password `postgres` for username `postgres`\n","!sudo -u postgres psql -U postgres -c \"ALTER USER postgres PASSWORD 'postgres';\"\n","\n","# Setup a database with name `imdbload` to be used\n","!sudo -u postgres psql -U postgres -c 'DROP DATABASE IF EXISTS imdbload;'\n","!sudo -u postgres psql -U postgres -c 'CREATE DATABASE imdbload;'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":339,"status":"ok","timestamp":1707597341026,"user":{"displayName":"Divyanshu Verma","userId":"14078105191069123729"},"user_tz":300},"id":"SfNhsoZecggj","outputId":"76d680c0-7017-4119-fe96-6cd31e087f11"},"outputs":[{"name":"stdout","output_type":"stream","text":["CREATE ROLE\n","ALTER ROLE\n"]}],"source":["!sudo -u postgres psql -U postgres -c \"CREATE ROLE root WITH SUPERUSER;\"\n","!sudo -u postgres psql -U postgres -c \"ALTER ROLE root WITH LOGIN;\"\n","# !sudo -u postgres psql -U postgres -c \"CREATE ROLE postgres WITH PASSWORD 'postgres';\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":340,"status":"ok","timestamp":1707597710317,"user":{"displayName":"Divyanshu Verma","userId":"14078105191069123729"},"user_tz":300},"id":"M2bl9bGua76P","outputId":"794aaabd-cc05-4cb7-948e-bf5142a3b310"},"outputs":[{"name":"stdout","output_type":"stream","text":["GRANT\n"]}],"source":["!sudo -u postgres psql -U postgres -c 'GRANT ALL PRIVILEGES ON DATABASE imdbload TO postgres;'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":327,"status":"ok","timestamp":1707597712490,"user":{"displayName":"Divyanshu Verma","userId":"14078105191069123729"},"user_tz":300},"id":"ZaXAQ1o1bczj","outputId":"39a4d3ab-2c82-482f-8a8d-a15af61df2b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["GRANT\n"]}],"source":["!sudo -u postgres psql -U postgres -c 'GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO postgres;'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V9HG2e5Zj5Jv"},"outputs":[],"source":["!set PGPASSWORD=\"postgres\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2679,"status":"ok","timestamp":1707597721129,"user":{"displayName":"Divyanshu Verma","userId":"14078105191069123729"},"user_tz":300},"id":"4dXApOAQpl2O","outputId":"1fbe1a2b-e3e4-4755-9089-16f0264c77ca"},"outputs":[{"name":"stdout","output_type":"stream","text":[" * Restarting PostgreSQL 14 database server\n","   ...done.\n"]}],"source":["# Define the new authentication method\n","new_auth_method = \"trust\"\n","\n","# Define the path to pg_hba.conf\n","pg_hba_conf_path = \"/etc/postgresql/14/main/pg_hba.conf\"  # Replace <version> with your PostgreSQL version\n","\n","# Construct the sed command to replace 'peer' with 'trust' in pg_hba.conf\n","sed_command = f\"sed -i 's/^\\s*local\\s*all\\s*all\\s*peer\\s*$/local   all   all   {new_auth_method}/' {pg_hba_conf_path}\"\n","\n","# Execute the sed command\n","!{sed_command}\n","\n","# Restart PostgreSQL service\n","!service postgresql restart"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1365602,"status":"ok","timestamp":1707599700174,"user":{"displayName":"Divyanshu Verma","userId":"14078105191069123729"},"user_tz":300},"id":"ZJVDhTG_xJnK","outputId":"cadf5875-9f9f-4e31-f16c-ae0f5bc1d835"},"outputs":[{"name":"stdout","output_type":"stream","text":["SET\n","SET\n","SET\n","SET\n","SET\n"," set_config \n","------------\n"," \n","(1 row)\n","\n","SET\n","SET\n","SET\n","SET\n","SET\n","SET\n","CREATE TABLE\n","ALTER TABLE\n","CREATE SEQUENCE\n","ALTER TABLE\n","ALTER SEQUENCE\n","CREATE TABLE\n","ALTER TABLE\n","CREATE SEQUENCE\n","ALTER TABLE\n","ALTER SEQUENCE\n","CREATE TABLE\n","ALTER TABLE\n","CREATE SEQUENCE\n","ALTER TABLE\n","ALTER SEQUENCE\n","CREATE TABLE\n","ALTER TABLE\n","CREATE SEQUENCE\n","ALTER TABLE\n","ALTER SEQUENCE\n","CREATE TABLE\n","ALTER TABLE\n","CREATE SEQUENCE\n","ALTER TABLE\n","ALTER SEQUENCE\n","CREATE TABLE\n","ALTER TABLE\n","CREATE SEQUENCE\n","ALTER TABLE\n","ALTER SEQUENCE\n","CREATE TABLE\n","ALTER TABLE\n","CREATE SEQUENCE\n","ALTER TABLE\n","ALTER SEQUENCE\n","CREATE TABLE\n","ALTER TABLE\n","CREATE SEQUENCE\n","ALTER TABLE\n","ALTER SEQUENCE\n","CREATE TABLE\n","ALTER TABLE\n","CREATE SEQUENCE\n","ALTER TABLE\n","ALTER SEQUENCE\n","CREATE TABLE\n","ALTER TABLE\n","CREATE SEQUENCE\n","ALTER TABLE\n","ALTER SEQUENCE\n","CREATE TABLE\n","ALTER TABLE\n","CREATE SEQUENCE\n","ALTER TABLE\n","ALTER SEQUENCE\n","CREATE TABLE\n","ALTER TABLE\n","CREATE SEQUENCE\n","ALTER TABLE\n","ALTER SEQUENCE\n","CREATE TABLE\n","ALTER TABLE\n","CREATE SEQUENCE\n","ALTER TABLE\n","ALTER SEQUENCE\n","CREATE TABLE\n","ALTER TABLE\n","CREATE SEQUENCE\n","ALTER TABLE\n","ALTER SEQUENCE\n","CREATE TABLE\n","ALTER TABLE\n","CREATE SEQUENCE\n","ALTER TABLE\n","ALTER SEQUENCE\n","CREATE TABLE\n","ALTER TABLE\n","CREATE SEQUENCE\n","ALTER TABLE\n","ALTER SEQUENCE\n","CREATE TABLE\n","ALTER TABLE\n","CREATE SEQUENCE\n","ALTER TABLE\n","ALTER SEQUENCE\n","CREATE TABLE\n","ALTER TABLE\n","CREATE SEQUENCE\n","ALTER TABLE\n","ALTER SEQUENCE\n","CREATE TABLE\n","ALTER TABLE\n","CREATE SEQUENCE\n","ALTER TABLE\n","ALTER SEQUENCE\n","CREATE TABLE\n","ALTER TABLE\n","CREATE SEQUENCE\n","ALTER TABLE\n","ALTER SEQUENCE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","COPY 1312273\n","COPY 528297\n","COPY 63475835\n","COPY 4314864\n","COPY 4\n","COPY 362131\n","COPY 4\n","COPY 135086\n","COPY 113\n","COPY 236627\n","COPY 10\n","COPY 18\n","COPY 4958296\n","COPY 29774718\n","COPY 7480087\n","COPY 2585150\n","COPY 6379740\n","COPY 4130207\n","COPY 12\n","COPY 4636996\n"," setval  \n","---------\n"," 1312273\n","(1 row)\n","\n"," setval \n","--------\n","      1\n","(1 row)\n","\n","  setval  \n","----------\n"," 63475835\n","(1 row)\n","\n"," setval \n","--------\n","      1\n","(1 row)\n","\n"," setval \n","--------\n","      4\n","(1 row)\n","\n"," setval \n","--------\n","      1\n","(1 row)\n","\n"," setval \n","--------\n","      4\n","(1 row)\n","\n"," setval \n","--------\n"," 135086\n","(1 row)\n","\n"," setval \n","--------\n","    113\n","(1 row)\n","\n"," setval \n","--------\n","      1\n","(1 row)\n","\n"," setval \n","--------\n","     10\n","(1 row)\n","\n"," setval \n","--------\n","     18\n","(1 row)\n","\n"," setval  \n","---------\n"," 4958296\n","(1 row)\n","\n","  setval  \n","----------\n"," 29774718\n","(1 row)\n","\n"," setval  \n","---------\n"," 7480087\n","(1 row)\n","\n"," setval  \n","---------\n"," 2585150\n","(1 row)\n","\n"," setval \n","--------\n","      1\n","(1 row)\n","\n"," setval  \n","---------\n"," 4130207\n","(1 row)\n","\n"," setval \n","--------\n","     12\n","(1 row)\n","\n"," setval \n","--------\n","      1\n","(1 row)\n","\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","ALTER TABLE\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n","CREATE INDEX\n"]}],"source":["!psql imdbload < /content/gdrive/MyDrive/fauce/imdbload_20240203.sql"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13054,"status":"ok","timestamp":1714069154957,"user":{"displayName":"Divyanshu Verma","userId":"14078105191069123729"},"user_tz":240},"id":"Pf3VcT6XATi-","outputId":"c730da10-e31f-4ac9-a78c-8ec74ab6774a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pipenv\n","  Downloading pipenv-2023.12.1-py3-none-any.whl (3.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pipenv) (2024.2.2)\n","Requirement already satisfied: setuptools>=67 in /usr/local/lib/python3.10/dist-packages (from pipenv) (67.7.2)\n","Collecting virtualenv>=20.24.2 (from pipenv)\n","  Downloading virtualenv-20.26.0-py3-none-any.whl (3.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting distlib<1,>=0.3.7 (from virtualenv>=20.24.2->pipenv)\n","  Downloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.24.2->pipenv) (3.13.4)\n","Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.24.2->pipenv) (4.2.0)\n","Installing collected packages: distlib, virtualenv, pipenv\n","Successfully installed distlib-0.3.8 pipenv-2023.12.1 virtualenv-20.26.0\n"]}],"source":["!pip3 install pipenv"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":97796,"status":"ok","timestamp":1714069252750,"user":{"displayName":"Divyanshu Verma","userId":"14078105191069123729"},"user_tz":240},"id":"cKZfQmO_BT71","outputId":"7feffb2e-1e2a-46b1-f0ab-f52011294ef0"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1mCreating a virtualenv for this project...\u001b[0m\n","Pipfile: \u001b[33m\u001b[1m/content/vldb2021_fauce/Pipfile\u001b[0m\n","\u001b[1mUsing\u001b[0m \u001b[33m\u001b[1m/usr/local/bin/python\u001b[0m \u001b[32m(3.10.12)\u001b[0m \u001b[1mto create virtualenv...\u001b[0m\n","\u001b[2K\u001b[32m⠹\u001b[0m Creating virtual environment...\u001b[36mcreated virtual environment CPython3.10.12.final.0-64 in 1507ms\n","  creator CPython3Posix(dest=/root/.local/share/virtualenvs/vldb2021_fauce-rT6f7ILy, clear=False, no_vcs_ignore=False, global=False)\n","  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n","    added seed packages: pip==24.0, setuptools==69.5.1, wheel==0.43.0\n","  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n","\u001b[0m\n","✔ Successfully created virtual environment!\n","\u001b[2K\u001b[32m⠸\u001b[0m Creating virtual environment...\n","\u001b[1A\u001b[2K\u001b[32mVirtualenv location: /root/.local/share/virtualenvs/vldb2021_fauce-rT6f7ILy\u001b[0m\n","\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1me31acf\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n","To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n","Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n"]}],"source":["!cd vldb2021_fauce/ && pipenv install"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KYEgQ_78gxpr"},"outputs":[],"source":["!cd vldb2021_fauce && pipenv shell"]},{"cell_type":"code","source":["!cd vldb2021_fauce/models_training && pipenv run python train.py --dataset /content/combined_df.csv --output /content/output_combined_df.csv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"08URtTXVe_Rl","executionInfo":{"status":"ok","timestamp":1714078873690,"user_tz":240,"elapsed":698004,"user":{"displayName":"Divyanshu Verma","userId":"14078105191069123729"}},"outputId":"d54e2286-03c3-41f6-f149-6abfcd864ec8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-04-25 18:29:34.368234: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-25 18:29:34.368323: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-25 18:29:34.371616: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-04-25 18:29:34.387275: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-04-25 18:29:35.988454: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","WARNING:tensorflow:From /root/.local/share/virtualenvs/vldb2021_fauce-rT6f7ILy/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:108: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n","2208\n","246\n","The shape of the data_x is: (2208, 245)\n","[[ 1.09861229]\n"," [13.54837071]\n"," [ 3.40119738]\n"," ...\n"," [21.72199204]\n"," [19.37286807]\n"," [17.90493902]]\n","self.min_val is: 0.0\n","self.max_val is: 25.91068069062784\n","/content/vldb2021_fauce/models_training/utils.py:45: RuntimeWarning: invalid value encountered in divide\n","  self.data_x_standardized = (self.data_x - self.input_mean)/self.input_std\n","The shape of the train_data_x is: (1987, 245)\n","The var_values is: Tensor(\"mul_1:0\", shape=(?, 1), dtype=float32)\n","WARNING:tensorflow:From /root/.local/share/virtualenvs/vldb2021_fauce-rT6f7ILy/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1260: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n","self.nll_gradients is: Tensor(\"gradients/MatMul_grad/MatMul:0\", shape=(?, 245), dtype=float32)\n","The var_values is: Tensor(\"mul_8:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(245, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","WARNING:tensorflow:From /root/.local/share/virtualenvs/vldb2021_fauce-rT6f7ILy/lib/python3.10/site-packages/tensorflow/python/training/rmsprop.py:188: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","The var_values is: Tensor(\"mul_15:0\", shape=(?, 1), dtype=float32)\n","self.nll_gradients is: Tensor(\"gradients_2/MatMul_10_grad/MatMul:0\", shape=(?, 245), dtype=float32)\n","The var_values is: Tensor(\"mul_22:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(245, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","model1MLP/weights_0:0\n","(245, 128)\n","model1MLP/biases_0:0\n","(128,)\n","model1MLP/weights_1:0\n","(128, 256)\n","model1MLP/biases_1:0\n","(256,)\n","model1MLP/weights_2:0\n","(256, 512)\n","model1MLP/biases_2:0\n","(512,)\n","model1MLP/weights_3:0\n","(512, 512)\n","model1MLP/biases_3:0\n","(512,)\n","model1MLP/weights_4:0\n","(512, 2)\n","model1MLP/biases_4:0\n","(2,)\n","The var_values is: Tensor(\"mul_29:0\", shape=(?, 1), dtype=float32)\n","self.nll_gradients is: Tensor(\"gradients_4/MatMul_20_grad/MatMul:0\", shape=(?, 245), dtype=float32)\n","The var_values is: Tensor(\"mul_36:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(245, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","model1MLP/weights_0:0\n","(245, 128)\n","model1MLP/biases_0:0\n","(128,)\n","model1MLP/weights_1:0\n","(128, 256)\n","model1MLP/biases_1:0\n","(256,)\n","model1MLP/weights_2:0\n","(256, 512)\n","model1MLP/biases_2:0\n","(512,)\n","model1MLP/weights_3:0\n","(512, 512)\n","model1MLP/biases_3:0\n","(512,)\n","model1MLP/weights_4:0\n","(512, 2)\n","model1MLP/biases_4:0\n","(2,)\n","model2MLP/weights_0:0\n","(245, 128)\n","model2MLP/biases_0:0\n","(128,)\n","model2MLP/weights_1:0\n","(128, 256)\n","model2MLP/biases_1:0\n","(256,)\n","model2MLP/weights_2:0\n","(256, 512)\n","model2MLP/biases_2:0\n","(512,)\n","model2MLP/weights_3:0\n","(512, 512)\n","model2MLP/biases_3:0\n","(512,)\n","model2MLP/weights_4:0\n","(512, 2)\n","model2MLP/biases_4:0\n","(2,)\n","The var_values is: Tensor(\"mul_43:0\", shape=(?, 1), dtype=float32)\n","self.nll_gradients is: Tensor(\"gradients_6/MatMul_30_grad/MatMul:0\", shape=(?, 245), dtype=float32)\n","The var_values is: Tensor(\"mul_50:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(245, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","model1MLP/weights_0:0\n","(245, 128)\n","model1MLP/biases_0:0\n","(128,)\n","model1MLP/weights_1:0\n","(128, 256)\n","model1MLP/biases_1:0\n","(256,)\n","model1MLP/weights_2:0\n","(256, 512)\n","model1MLP/biases_2:0\n","(512,)\n","model1MLP/weights_3:0\n","(512, 512)\n","model1MLP/biases_3:0\n","(512,)\n","model1MLP/weights_4:0\n","(512, 2)\n","model1MLP/biases_4:0\n","(2,)\n","model2MLP/weights_0:0\n","(245, 128)\n","model2MLP/biases_0:0\n","(128,)\n","model2MLP/weights_1:0\n","(128, 256)\n","model2MLP/biases_1:0\n","(256,)\n","model2MLP/weights_2:0\n","(256, 512)\n","model2MLP/biases_2:0\n","(512,)\n","model2MLP/weights_3:0\n","(512, 512)\n","model2MLP/biases_3:0\n","(512,)\n","model2MLP/weights_4:0\n","(512, 2)\n","model2MLP/biases_4:0\n","(2,)\n","model3MLP/weights_0:0\n","(245, 128)\n","model3MLP/biases_0:0\n","(128,)\n","model3MLP/weights_1:0\n","(128, 256)\n","model3MLP/biases_1:0\n","(256,)\n","model3MLP/weights_2:0\n","(256, 512)\n","model3MLP/biases_2:0\n","(512,)\n","model3MLP/weights_3:0\n","(512, 512)\n","model3MLP/biases_3:0\n","(512,)\n","model3MLP/weights_4:0\n","(512, 2)\n","model3MLP/biases_4:0\n","(2,)\n","The var_values is: Tensor(\"mul_57:0\", shape=(?, 1), dtype=float32)\n","self.nll_gradients is: Tensor(\"gradients_8/MatMul_40_grad/MatMul:0\", shape=(?, 245), dtype=float32)\n","The var_values is: Tensor(\"mul_64:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(245, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","model1MLP/weights_0:0\n","(245, 128)\n","model1MLP/biases_0:0\n","(128,)\n","model1MLP/weights_1:0\n","(128, 256)\n","model1MLP/biases_1:0\n","(256,)\n","model1MLP/weights_2:0\n","(256, 512)\n","model1MLP/biases_2:0\n","(512,)\n","model1MLP/weights_3:0\n","(512, 512)\n","model1MLP/biases_3:0\n","(512,)\n","model1MLP/weights_4:0\n","(512, 2)\n","model1MLP/biases_4:0\n","(2,)\n","model2MLP/weights_0:0\n","(245, 128)\n","model2MLP/biases_0:0\n","(128,)\n","model2MLP/weights_1:0\n","(128, 256)\n","model2MLP/biases_1:0\n","(256,)\n","model2MLP/weights_2:0\n","(256, 512)\n","model2MLP/biases_2:0\n","(512,)\n","model2MLP/weights_3:0\n","(512, 512)\n","model2MLP/biases_3:0\n","(512,)\n","model2MLP/weights_4:0\n","(512, 2)\n","model2MLP/biases_4:0\n","(2,)\n","model3MLP/weights_0:0\n","(245, 128)\n","model3MLP/biases_0:0\n","(128,)\n","model3MLP/weights_1:0\n","(128, 256)\n","model3MLP/biases_1:0\n","(256,)\n","model3MLP/weights_2:0\n","(256, 512)\n","model3MLP/biases_2:0\n","(512,)\n","model3MLP/weights_3:0\n","(512, 512)\n","model3MLP/biases_3:0\n","(512,)\n","model3MLP/weights_4:0\n","(512, 2)\n","model3MLP/biases_4:0\n","(2,)\n","model4MLP/weights_0:0\n","(245, 128)\n","model4MLP/biases_0:0\n","(128,)\n","model4MLP/weights_1:0\n","(128, 256)\n","model4MLP/biases_1:0\n","(256,)\n","model4MLP/weights_2:0\n","(256, 512)\n","model4MLP/biases_2:0\n","(512,)\n","model4MLP/weights_3:0\n","(512, 512)\n","model4MLP/biases_3:0\n","(512,)\n","model4MLP/weights_4:0\n","(512, 2)\n","model4MLP/biases_4:0\n","(2,)\n","The var_values is: Tensor(\"mul_71:0\", shape=(?, 1), dtype=float32)\n","self.nll_gradients is: Tensor(\"gradients_10/MatMul_50_grad/MatMul:0\", shape=(?, 245), dtype=float32)\n","The var_values is: Tensor(\"mul_78:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(245, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","model1MLP/weights_0:0\n","(245, 128)\n","model1MLP/biases_0:0\n","(128,)\n","model1MLP/weights_1:0\n","(128, 256)\n","model1MLP/biases_1:0\n","(256,)\n","model1MLP/weights_2:0\n","(256, 512)\n","model1MLP/biases_2:0\n","(512,)\n","model1MLP/weights_3:0\n","(512, 512)\n","model1MLP/biases_3:0\n","(512,)\n","model1MLP/weights_4:0\n","(512, 2)\n","model1MLP/biases_4:0\n","(2,)\n","model2MLP/weights_0:0\n","(245, 128)\n","model2MLP/biases_0:0\n","(128,)\n","model2MLP/weights_1:0\n","(128, 256)\n","model2MLP/biases_1:0\n","(256,)\n","model2MLP/weights_2:0\n","(256, 512)\n","model2MLP/biases_2:0\n","(512,)\n","model2MLP/weights_3:0\n","(512, 512)\n","model2MLP/biases_3:0\n","(512,)\n","model2MLP/weights_4:0\n","(512, 2)\n","model2MLP/biases_4:0\n","(2,)\n","model3MLP/weights_0:0\n","(245, 128)\n","model3MLP/biases_0:0\n","(128,)\n","model3MLP/weights_1:0\n","(128, 256)\n","model3MLP/biases_1:0\n","(256,)\n","model3MLP/weights_2:0\n","(256, 512)\n","model3MLP/biases_2:0\n","(512,)\n","model3MLP/weights_3:0\n","(512, 512)\n","model3MLP/biases_3:0\n","(512,)\n","model3MLP/weights_4:0\n","(512, 2)\n","model3MLP/biases_4:0\n","(2,)\n","model4MLP/weights_0:0\n","(245, 128)\n","model4MLP/biases_0:0\n","(128,)\n","model4MLP/weights_1:0\n","(128, 256)\n","model4MLP/biases_1:0\n","(256,)\n","model4MLP/weights_2:0\n","(256, 512)\n","model4MLP/biases_2:0\n","(512,)\n","model4MLP/weights_3:0\n","(512, 512)\n","model4MLP/biases_3:0\n","(512,)\n","model4MLP/weights_4:0\n","(512, 2)\n","model4MLP/biases_4:0\n","(2,)\n","model5MLP/weights_0:0\n","(245, 128)\n","model5MLP/biases_0:0\n","(128,)\n","model5MLP/weights_1:0\n","(128, 256)\n","model5MLP/biases_1:0\n","(256,)\n","model5MLP/weights_2:0\n","(256, 512)\n","model5MLP/biases_2:0\n","(512,)\n","model5MLP/weights_3:0\n","(512, 512)\n","model5MLP/biases_3:0\n","(512,)\n","model5MLP/weights_4:0\n","(512, 2)\n","model5MLP/biases_4:0\n","(2,)\n","The var_values is: Tensor(\"mul_85:0\", shape=(?, 1), dtype=float32)\n","self.nll_gradients is: Tensor(\"gradients_12/MatMul_60_grad/MatMul:0\", shape=(?, 245), dtype=float32)\n","The var_values is: Tensor(\"mul_92:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(245, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","model1MLP/weights_0:0\n","(245, 128)\n","model1MLP/biases_0:0\n","(128,)\n","model1MLP/weights_1:0\n","(128, 256)\n","model1MLP/biases_1:0\n","(256,)\n","model1MLP/weights_2:0\n","(256, 512)\n","model1MLP/biases_2:0\n","(512,)\n","model1MLP/weights_3:0\n","(512, 512)\n","model1MLP/biases_3:0\n","(512,)\n","model1MLP/weights_4:0\n","(512, 2)\n","model1MLP/biases_4:0\n","(2,)\n","model2MLP/weights_0:0\n","(245, 128)\n","model2MLP/biases_0:0\n","(128,)\n","model2MLP/weights_1:0\n","(128, 256)\n","model2MLP/biases_1:0\n","(256,)\n","model2MLP/weights_2:0\n","(256, 512)\n","model2MLP/biases_2:0\n","(512,)\n","model2MLP/weights_3:0\n","(512, 512)\n","model2MLP/biases_3:0\n","(512,)\n","model2MLP/weights_4:0\n","(512, 2)\n","model2MLP/biases_4:0\n","(2,)\n","model3MLP/weights_0:0\n","(245, 128)\n","model3MLP/biases_0:0\n","(128,)\n","model3MLP/weights_1:0\n","(128, 256)\n","model3MLP/biases_1:0\n","(256,)\n","model3MLP/weights_2:0\n","(256, 512)\n","model3MLP/biases_2:0\n","(512,)\n","model3MLP/weights_3:0\n","(512, 512)\n","model3MLP/biases_3:0\n","(512,)\n","model3MLP/weights_4:0\n","(512, 2)\n","model3MLP/biases_4:0\n","(2,)\n","model4MLP/weights_0:0\n","(245, 128)\n","model4MLP/biases_0:0\n","(128,)\n","model4MLP/weights_1:0\n","(128, 256)\n","model4MLP/biases_1:0\n","(256,)\n","model4MLP/weights_2:0\n","(256, 512)\n","model4MLP/biases_2:0\n","(512,)\n","model4MLP/weights_3:0\n","(512, 512)\n","model4MLP/biases_3:0\n","(512,)\n","model4MLP/weights_4:0\n","(512, 2)\n","model4MLP/biases_4:0\n","(2,)\n","model5MLP/weights_0:0\n","(245, 128)\n","model5MLP/biases_0:0\n","(128,)\n","model5MLP/weights_1:0\n","(128, 256)\n","model5MLP/biases_1:0\n","(256,)\n","model5MLP/weights_2:0\n","(256, 512)\n","model5MLP/biases_2:0\n","(512,)\n","model5MLP/weights_3:0\n","(512, 512)\n","model5MLP/biases_3:0\n","(512,)\n","model5MLP/weights_4:0\n","(512, 2)\n","model5MLP/biases_4:0\n","(2,)\n","model6MLP/weights_0:0\n","(245, 128)\n","model6MLP/biases_0:0\n","(128,)\n","model6MLP/weights_1:0\n","(128, 256)\n","model6MLP/biases_1:0\n","(256,)\n","model6MLP/weights_2:0\n","(256, 512)\n","model6MLP/biases_2:0\n","(512,)\n","model6MLP/weights_3:0\n","(512, 512)\n","model6MLP/biases_3:0\n","(512,)\n","model6MLP/weights_4:0\n","(512, 2)\n","model6MLP/biases_4:0\n","(2,)\n","The var_values is: Tensor(\"mul_99:0\", shape=(?, 1), dtype=float32)\n","self.nll_gradients is: Tensor(\"gradients_14/MatMul_70_grad/MatMul:0\", shape=(?, 245), dtype=float32)\n","The var_values is: Tensor(\"mul_106:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(245, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","model1MLP/weights_0:0\n","(245, 128)\n","model1MLP/biases_0:0\n","(128,)\n","model1MLP/weights_1:0\n","(128, 256)\n","model1MLP/biases_1:0\n","(256,)\n","model1MLP/weights_2:0\n","(256, 512)\n","model1MLP/biases_2:0\n","(512,)\n","model1MLP/weights_3:0\n","(512, 512)\n","model1MLP/biases_3:0\n","(512,)\n","model1MLP/weights_4:0\n","(512, 2)\n","model1MLP/biases_4:0\n","(2,)\n","model2MLP/weights_0:0\n","(245, 128)\n","model2MLP/biases_0:0\n","(128,)\n","model2MLP/weights_1:0\n","(128, 256)\n","model2MLP/biases_1:0\n","(256,)\n","model2MLP/weights_2:0\n","(256, 512)\n","model2MLP/biases_2:0\n","(512,)\n","model2MLP/weights_3:0\n","(512, 512)\n","model2MLP/biases_3:0\n","(512,)\n","model2MLP/weights_4:0\n","(512, 2)\n","model2MLP/biases_4:0\n","(2,)\n","model3MLP/weights_0:0\n","(245, 128)\n","model3MLP/biases_0:0\n","(128,)\n","model3MLP/weights_1:0\n","(128, 256)\n","model3MLP/biases_1:0\n","(256,)\n","model3MLP/weights_2:0\n","(256, 512)\n","model3MLP/biases_2:0\n","(512,)\n","model3MLP/weights_3:0\n","(512, 512)\n","model3MLP/biases_3:0\n","(512,)\n","model3MLP/weights_4:0\n","(512, 2)\n","model3MLP/biases_4:0\n","(2,)\n","model4MLP/weights_0:0\n","(245, 128)\n","model4MLP/biases_0:0\n","(128,)\n","model4MLP/weights_1:0\n","(128, 256)\n","model4MLP/biases_1:0\n","(256,)\n","model4MLP/weights_2:0\n","(256, 512)\n","model4MLP/biases_2:0\n","(512,)\n","model4MLP/weights_3:0\n","(512, 512)\n","model4MLP/biases_3:0\n","(512,)\n","model4MLP/weights_4:0\n","(512, 2)\n","model4MLP/biases_4:0\n","(2,)\n","model5MLP/weights_0:0\n","(245, 128)\n","model5MLP/biases_0:0\n","(128,)\n","model5MLP/weights_1:0\n","(128, 256)\n","model5MLP/biases_1:0\n","(256,)\n","model5MLP/weights_2:0\n","(256, 512)\n","model5MLP/biases_2:0\n","(512,)\n","model5MLP/weights_3:0\n","(512, 512)\n","model5MLP/biases_3:0\n","(512,)\n","model5MLP/weights_4:0\n","(512, 2)\n","model5MLP/biases_4:0\n","(2,)\n","model6MLP/weights_0:0\n","(245, 128)\n","model6MLP/biases_0:0\n","(128,)\n","model6MLP/weights_1:0\n","(128, 256)\n","model6MLP/biases_1:0\n","(256,)\n","model6MLP/weights_2:0\n","(256, 512)\n","model6MLP/biases_2:0\n","(512,)\n","model6MLP/weights_3:0\n","(512, 512)\n","model6MLP/biases_3:0\n","(512,)\n","model6MLP/weights_4:0\n","(512, 2)\n","model6MLP/biases_4:0\n","(2,)\n","model7MLP/weights_0:0\n","(245, 128)\n","model7MLP/biases_0:0\n","(128,)\n","model7MLP/weights_1:0\n","(128, 256)\n","model7MLP/biases_1:0\n","(256,)\n","model7MLP/weights_2:0\n","(256, 512)\n","model7MLP/biases_2:0\n","(512,)\n","model7MLP/weights_3:0\n","(512, 512)\n","model7MLP/biases_3:0\n","(512,)\n","model7MLP/weights_4:0\n","(512, 2)\n","model7MLP/biases_4:0\n","(2,)\n","The var_values is: Tensor(\"mul_113:0\", shape=(?, 1), dtype=float32)\n","self.nll_gradients is: Tensor(\"gradients_16/MatMul_80_grad/MatMul:0\", shape=(?, 245), dtype=float32)\n","The var_values is: Tensor(\"mul_120:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(245, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","model1MLP/weights_0:0\n","(245, 128)\n","model1MLP/biases_0:0\n","(128,)\n","model1MLP/weights_1:0\n","(128, 256)\n","model1MLP/biases_1:0\n","(256,)\n","model1MLP/weights_2:0\n","(256, 512)\n","model1MLP/biases_2:0\n","(512,)\n","model1MLP/weights_3:0\n","(512, 512)\n","model1MLP/biases_3:0\n","(512,)\n","model1MLP/weights_4:0\n","(512, 2)\n","model1MLP/biases_4:0\n","(2,)\n","model2MLP/weights_0:0\n","(245, 128)\n","model2MLP/biases_0:0\n","(128,)\n","model2MLP/weights_1:0\n","(128, 256)\n","model2MLP/biases_1:0\n","(256,)\n","model2MLP/weights_2:0\n","(256, 512)\n","model2MLP/biases_2:0\n","(512,)\n","model2MLP/weights_3:0\n","(512, 512)\n","model2MLP/biases_3:0\n","(512,)\n","model2MLP/weights_4:0\n","(512, 2)\n","model2MLP/biases_4:0\n","(2,)\n","model3MLP/weights_0:0\n","(245, 128)\n","model3MLP/biases_0:0\n","(128,)\n","model3MLP/weights_1:0\n","(128, 256)\n","model3MLP/biases_1:0\n","(256,)\n","model3MLP/weights_2:0\n","(256, 512)\n","model3MLP/biases_2:0\n","(512,)\n","model3MLP/weights_3:0\n","(512, 512)\n","model3MLP/biases_3:0\n","(512,)\n","model3MLP/weights_4:0\n","(512, 2)\n","model3MLP/biases_4:0\n","(2,)\n","model4MLP/weights_0:0\n","(245, 128)\n","model4MLP/biases_0:0\n","(128,)\n","model4MLP/weights_1:0\n","(128, 256)\n","model4MLP/biases_1:0\n","(256,)\n","model4MLP/weights_2:0\n","(256, 512)\n","model4MLP/biases_2:0\n","(512,)\n","model4MLP/weights_3:0\n","(512, 512)\n","model4MLP/biases_3:0\n","(512,)\n","model4MLP/weights_4:0\n","(512, 2)\n","model4MLP/biases_4:0\n","(2,)\n","model5MLP/weights_0:0\n","(245, 128)\n","model5MLP/biases_0:0\n","(128,)\n","model5MLP/weights_1:0\n","(128, 256)\n","model5MLP/biases_1:0\n","(256,)\n","model5MLP/weights_2:0\n","(256, 512)\n","model5MLP/biases_2:0\n","(512,)\n","model5MLP/weights_3:0\n","(512, 512)\n","model5MLP/biases_3:0\n","(512,)\n","model5MLP/weights_4:0\n","(512, 2)\n","model5MLP/biases_4:0\n","(2,)\n","model6MLP/weights_0:0\n","(245, 128)\n","model6MLP/biases_0:0\n","(128,)\n","model6MLP/weights_1:0\n","(128, 256)\n","model6MLP/biases_1:0\n","(256,)\n","model6MLP/weights_2:0\n","(256, 512)\n","model6MLP/biases_2:0\n","(512,)\n","model6MLP/weights_3:0\n","(512, 512)\n","model6MLP/biases_3:0\n","(512,)\n","model6MLP/weights_4:0\n","(512, 2)\n","model6MLP/biases_4:0\n","(2,)\n","model7MLP/weights_0:0\n","(245, 128)\n","model7MLP/biases_0:0\n","(128,)\n","model7MLP/weights_1:0\n","(128, 256)\n","model7MLP/biases_1:0\n","(256,)\n","model7MLP/weights_2:0\n","(256, 512)\n","model7MLP/biases_2:0\n","(512,)\n","model7MLP/weights_3:0\n","(512, 512)\n","model7MLP/biases_3:0\n","(512,)\n","model7MLP/weights_4:0\n","(512, 2)\n","model7MLP/biases_4:0\n","(2,)\n","model8MLP/weights_0:0\n","(245, 128)\n","model8MLP/biases_0:0\n","(128,)\n","model8MLP/weights_1:0\n","(128, 256)\n","model8MLP/biases_1:0\n","(256,)\n","model8MLP/weights_2:0\n","(256, 512)\n","model8MLP/biases_2:0\n","(512,)\n","model8MLP/weights_3:0\n","(512, 512)\n","model8MLP/biases_3:0\n","(512,)\n","model8MLP/weights_4:0\n","(512, 2)\n","model8MLP/biases_4:0\n","(2,)\n","The var_values is: Tensor(\"mul_127:0\", shape=(?, 1), dtype=float32)\n","self.nll_gradients is: Tensor(\"gradients_18/MatMul_90_grad/MatMul:0\", shape=(?, 245), dtype=float32)\n","The var_values is: Tensor(\"mul_134:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(245, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","model1MLP/weights_0:0\n","(245, 128)\n","model1MLP/biases_0:0\n","(128,)\n","model1MLP/weights_1:0\n","(128, 256)\n","model1MLP/biases_1:0\n","(256,)\n","model1MLP/weights_2:0\n","(256, 512)\n","model1MLP/biases_2:0\n","(512,)\n","model1MLP/weights_3:0\n","(512, 512)\n","model1MLP/biases_3:0\n","(512,)\n","model1MLP/weights_4:0\n","(512, 2)\n","model1MLP/biases_4:0\n","(2,)\n","model2MLP/weights_0:0\n","(245, 128)\n","model2MLP/biases_0:0\n","(128,)\n","model2MLP/weights_1:0\n","(128, 256)\n","model2MLP/biases_1:0\n","(256,)\n","model2MLP/weights_2:0\n","(256, 512)\n","model2MLP/biases_2:0\n","(512,)\n","model2MLP/weights_3:0\n","(512, 512)\n","model2MLP/biases_3:0\n","(512,)\n","model2MLP/weights_4:0\n","(512, 2)\n","model2MLP/biases_4:0\n","(2,)\n","model3MLP/weights_0:0\n","(245, 128)\n","model3MLP/biases_0:0\n","(128,)\n","model3MLP/weights_1:0\n","(128, 256)\n","model3MLP/biases_1:0\n","(256,)\n","model3MLP/weights_2:0\n","(256, 512)\n","model3MLP/biases_2:0\n","(512,)\n","model3MLP/weights_3:0\n","(512, 512)\n","model3MLP/biases_3:0\n","(512,)\n","model3MLP/weights_4:0\n","(512, 2)\n","model3MLP/biases_4:0\n","(2,)\n","model4MLP/weights_0:0\n","(245, 128)\n","model4MLP/biases_0:0\n","(128,)\n","model4MLP/weights_1:0\n","(128, 256)\n","model4MLP/biases_1:0\n","(256,)\n","model4MLP/weights_2:0\n","(256, 512)\n","model4MLP/biases_2:0\n","(512,)\n","model4MLP/weights_3:0\n","(512, 512)\n","model4MLP/biases_3:0\n","(512,)\n","model4MLP/weights_4:0\n","(512, 2)\n","model4MLP/biases_4:0\n","(2,)\n","model5MLP/weights_0:0\n","(245, 128)\n","model5MLP/biases_0:0\n","(128,)\n","model5MLP/weights_1:0\n","(128, 256)\n","model5MLP/biases_1:0\n","(256,)\n","model5MLP/weights_2:0\n","(256, 512)\n","model5MLP/biases_2:0\n","(512,)\n","model5MLP/weights_3:0\n","(512, 512)\n","model5MLP/biases_3:0\n","(512,)\n","model5MLP/weights_4:0\n","(512, 2)\n","model5MLP/biases_4:0\n","(2,)\n","model6MLP/weights_0:0\n","(245, 128)\n","model6MLP/biases_0:0\n","(128,)\n","model6MLP/weights_1:0\n","(128, 256)\n","model6MLP/biases_1:0\n","(256,)\n","model6MLP/weights_2:0\n","(256, 512)\n","model6MLP/biases_2:0\n","(512,)\n","model6MLP/weights_3:0\n","(512, 512)\n","model6MLP/biases_3:0\n","(512,)\n","model6MLP/weights_4:0\n","(512, 2)\n","model6MLP/biases_4:0\n","(2,)\n","model7MLP/weights_0:0\n","(245, 128)\n","model7MLP/biases_0:0\n","(128,)\n","model7MLP/weights_1:0\n","(128, 256)\n","model7MLP/biases_1:0\n","(256,)\n","model7MLP/weights_2:0\n","(256, 512)\n","model7MLP/biases_2:0\n","(512,)\n","model7MLP/weights_3:0\n","(512, 512)\n","model7MLP/biases_3:0\n","(512,)\n","model7MLP/weights_4:0\n","(512, 2)\n","model7MLP/biases_4:0\n","(2,)\n","model8MLP/weights_0:0\n","(245, 128)\n","model8MLP/biases_0:0\n","(128,)\n","model8MLP/weights_1:0\n","(128, 256)\n","model8MLP/biases_1:0\n","(256,)\n","model8MLP/weights_2:0\n","(256, 512)\n","model8MLP/biases_2:0\n","(512,)\n","model8MLP/weights_3:0\n","(512, 512)\n","model8MLP/biases_3:0\n","(512,)\n","model8MLP/weights_4:0\n","(512, 2)\n","model8MLP/biases_4:0\n","(2,)\n","model9MLP/weights_0:0\n","(245, 128)\n","model9MLP/biases_0:0\n","(128,)\n","model9MLP/weights_1:0\n","(128, 256)\n","model9MLP/biases_1:0\n","(256,)\n","model9MLP/weights_2:0\n","(256, 512)\n","model9MLP/biases_2:0\n","(512,)\n","model9MLP/weights_3:0\n","(512, 512)\n","model9MLP/biases_3:0\n","(512,)\n","model9MLP/weights_4:0\n","(512, 2)\n","model9MLP/biases_4:0\n","(2,)\n","2024-04-25 18:29:47.754428: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n","itr: 0, nll: nan\n","itr: 0, nll: nan\n","itr: 0, nll: nan\n","itr: 0, nll: nan\n","itr: 0, nll: nan\n","itr: 0, nll: nan\n","itr: 0, nll: nan\n","itr: 0, nll: nan\n","itr: 0, nll: nan\n","itr: 0, nll: nan\n","itr: 300, nll: nan\n","itr: 300, nll: nan\n","itr: 300, nll: nan\n","itr: 300, nll: nan\n","itr: 300, nll: nan\n","itr: 300, nll: nan\n","itr: 300, nll: nan\n","itr: 300, nll: nan\n","itr: 300, nll: nan\n","itr: 300, nll: nan\n","itr: 600, nll: nan\n","itr: 600, nll: nan\n","itr: 600, nll: nan\n","itr: 600, nll: nan\n","itr: 600, nll: nan\n","itr: 600, nll: nan\n","itr: 600, nll: nan\n","itr: 600, nll: nan\n","itr: 600, nll: nan\n","itr: 600, nll: nan\n","itr: 900, nll: nan\n","itr: 900, nll: nan\n","itr: 900, nll: nan\n","itr: 900, nll: nan\n","itr: 900, nll: nan\n","itr: 900, nll: nan\n","itr: 900, nll: nan\n","itr: 900, nll: nan\n","itr: 900, nll: nan\n","itr: 900, nll: nan\n","itr: 1200, nll: nan\n","itr: 1200, nll: nan\n","itr: 1200, nll: nan\n","itr: 1200, nll: nan\n","itr: 1200, nll: nan\n","itr: 1200, nll: nan\n","itr: 1200, nll: nan\n","itr: 1200, nll: nan\n","itr: 1200, nll: nan\n","itr: 1200, nll: nan\n","itr: 1500, nll: nan\n","itr: 1500, nll: nan\n","itr: 1500, nll: nan\n","itr: 1500, nll: nan\n","itr: 1500, nll: nan\n","itr: 1500, nll: nan\n","itr: 1500, nll: nan\n","itr: 1500, nll: nan\n","itr: 1500, nll: nan\n","itr: 1500, nll: nan\n","itr: 1800, nll: nan\n","itr: 1800, nll: nan\n","itr: 1800, nll: nan\n","itr: 1800, nll: nan\n","itr: 1800, nll: nan\n","itr: 1800, nll: nan\n","itr: 1800, nll: nan\n","itr: 1800, nll: nan\n","itr: 1800, nll: nan\n","itr: 1800, nll: nan\n","itr: 2100, nll: nan\n","itr: 2100, nll: nan\n","itr: 2100, nll: nan\n","itr: 2100, nll: nan\n","itr: 2100, nll: nan\n","itr: 2100, nll: nan\n","itr: 2100, nll: nan\n","itr: 2100, nll: nan\n","itr: 2100, nll: nan\n","itr: 2100, nll: nan\n","itr: 2400, nll: nan\n","itr: 2400, nll: nan\n","itr: 2400, nll: nan\n","itr: 2400, nll: nan\n","itr: 2400, nll: nan\n","itr: 2400, nll: nan\n","itr: 2400, nll: nan\n","itr: 2400, nll: nan\n","itr: 2400, nll: nan\n","itr: 2400, nll: nan\n","itr: 2700, nll: nan\n","itr: 2700, nll: nan\n","itr: 2700, nll: nan\n","itr: 2700, nll: nan\n","itr: 2700, nll: nan\n","itr: 2700, nll: nan\n","itr: 2700, nll: nan\n","itr: 2700, nll: nan\n","itr: 2700, nll: nan\n","itr: 2700, nll: nan\n","itr: 3000, nll: nan\n","itr: 3000, nll: nan\n","itr: 3000, nll: nan\n","itr: 3000, nll: nan\n","itr: 3000, nll: nan\n","itr: 3000, nll: nan\n","itr: 3000, nll: nan\n","itr: 3000, nll: nan\n","itr: 3000, nll: nan\n","itr: 3000, nll: nan\n","itr: 3300, nll: nan\n","itr: 3300, nll: nan\n","itr: 3300, nll: nan\n","itr: 3300, nll: nan\n","itr: 3300, nll: nan\n","itr: 3300, nll: nan\n","itr: 3300, nll: nan\n","itr: 3300, nll: nan\n","itr: 3300, nll: nan\n","itr: 3300, nll: nan\n","itr: 3600, nll: nan\n","itr: 3600, nll: nan\n","itr: 3600, nll: nan\n","itr: 3600, nll: nan\n","itr: 3600, nll: nan\n","itr: 3600, nll: nan\n","itr: 3600, nll: nan\n","itr: 3600, nll: nan\n","itr: 3600, nll: nan\n","itr: 3600, nll: nan\n","itr: 3900, nll: nan\n","itr: 3900, nll: nan\n","itr: 3900, nll: nan\n","itr: 3900, nll: nan\n","itr: 3900, nll: nan\n","itr: 3900, nll: nan\n","itr: 3900, nll: nan\n","itr: 3900, nll: nan\n","itr: 3900, nll: nan\n","itr: 3900, nll: nan\n","itr: 4200, nll: nan\n","itr: 4200, nll: nan\n","itr: 4200, nll: nan\n","itr: 4200, nll: nan\n","itr: 4200, nll: nan\n","itr: 4200, nll: nan\n","itr: 4200, nll: nan\n","itr: 4200, nll: nan\n","itr: 4200, nll: nan\n","itr: 4200, nll: nan\n","itr: 4500, nll: nan\n","itr: 4500, nll: nan\n","itr: 4500, nll: nan\n","itr: 4500, nll: nan\n","itr: 4500, nll: nan\n","itr: 4500, nll: nan\n","itr: 4500, nll: nan\n","itr: 4500, nll: nan\n","itr: 4500, nll: nan\n","itr: 4500, nll: nan\n","itr: 4800, nll: nan\n","itr: 4800, nll: nan\n","itr: 4800, nll: nan\n","itr: 4800, nll: nan\n","itr: 4800, nll: nan\n","itr: 4800, nll: nan\n","itr: 4800, nll: nan\n","itr: 4800, nll: nan\n","itr: 4800, nll: nan\n","itr: 4800, nll: nan\n","itr: 5100, nll: nan\n","itr: 5100, nll: nan\n","itr: 5100, nll: nan\n","itr: 5100, nll: nan\n","itr: 5100, nll: nan\n","itr: 5100, nll: nan\n","itr: 5100, nll: nan\n","itr: 5100, nll: nan\n","itr: 5100, nll: nan\n","itr: 5100, nll: nan\n","itr: 5400, nll: nan\n","itr: 5400, nll: nan\n","itr: 5400, nll: nan\n","itr: 5400, nll: nan\n","itr: 5400, nll: nan\n","itr: 5400, nll: nan\n","itr: 5400, nll: nan\n","itr: 5400, nll: nan\n","itr: 5400, nll: nan\n","itr: 5400, nll: nan\n","itr: 5700, nll: nan\n","itr: 5700, nll: nan\n","itr: 5700, nll: nan\n","itr: 5700, nll: nan\n","itr: 5700, nll: nan\n","itr: 5700, nll: nan\n","itr: 5700, nll: nan\n","itr: 5700, nll: nan\n","itr: 5700, nll: nan\n","itr: 5700, nll: nan\n","itr: 6000, nll: nan\n","itr: 6000, nll: nan\n","itr: 6000, nll: nan\n","itr: 6000, nll: nan\n","itr: 6000, nll: nan\n","itr: 6000, nll: nan\n","itr: 6000, nll: nan\n","itr: 6000, nll: nan\n","itr: 6000, nll: nan\n","itr: 6000, nll: nan\n","itr: 6300, nll: nan\n","itr: 6300, nll: nan\n","itr: 6300, nll: nan\n","itr: 6300, nll: nan\n","itr: 6300, nll: nan\n","itr: 6300, nll: nan\n","itr: 6300, nll: nan\n","itr: 6300, nll: nan\n","itr: 6300, nll: nan\n","itr: 6300, nll: nan\n","itr: 6600, nll: nan\n","itr: 6600, nll: nan\n","itr: 6600, nll: nan\n","itr: 6600, nll: nan\n","itr: 6600, nll: nan\n","itr: 6600, nll: nan\n","itr: 6600, nll: nan\n","itr: 6600, nll: nan\n","itr: 6600, nll: nan\n","itr: 6600, nll: nan\n","itr: 6900, nll: nan\n","itr: 6900, nll: nan\n","itr: 6900, nll: nan\n","itr: 6900, nll: nan\n","itr: 6900, nll: nan\n","itr: 6900, nll: nan\n","itr: 6900, nll: nan\n","itr: 6900, nll: nan\n","itr: 6900, nll: nan\n","itr: 6900, nll: nan\n","itr: 7200, nll: nan\n","itr: 7200, nll: nan\n","itr: 7200, nll: nan\n","itr: 7200, nll: nan\n","itr: 7200, nll: nan\n","itr: 7200, nll: nan\n","itr: 7200, nll: nan\n","itr: 7200, nll: nan\n","itr: 7200, nll: nan\n","itr: 7200, nll: nan\n","itr: 7500, nll: nan\n","itr: 7500, nll: nan\n","itr: 7500, nll: nan\n","itr: 7500, nll: nan\n","itr: 7500, nll: nan\n","itr: 7500, nll: nan\n","itr: 7500, nll: nan\n","itr: 7500, nll: nan\n","itr: 7500, nll: nan\n","itr: 7500, nll: nan\n","itr: 7800, nll: nan\n","itr: 7800, nll: nan\n","itr: 7800, nll: nan\n","itr: 7800, nll: nan\n","itr: 7800, nll: nan\n","itr: 7800, nll: nan\n","itr: 7800, nll: nan\n","itr: 7800, nll: nan\n","itr: 7800, nll: nan\n","itr: 7800, nll: nan\n","itr: 8100, nll: nan\n","itr: 8100, nll: nan\n","itr: 8100, nll: nan\n","itr: 8100, nll: nan\n","itr: 8100, nll: nan\n","itr: 8100, nll: nan\n","itr: 8100, nll: nan\n","itr: 8100, nll: nan\n","itr: 8100, nll: nan\n","itr: 8100, nll: nan\n","itr: 8400, nll: nan\n","itr: 8400, nll: nan\n","itr: 8400, nll: nan\n","itr: 8400, nll: nan\n","itr: 8400, nll: nan\n","itr: 8400, nll: nan\n","itr: 8400, nll: nan\n","itr: 8400, nll: nan\n","itr: 8400, nll: nan\n","itr: 8400, nll: nan\n","itr: 8700, nll: nan\n","itr: 8700, nll: nan\n","itr: 8700, nll: nan\n","itr: 8700, nll: nan\n","itr: 8700, nll: nan\n","itr: 8700, nll: nan\n","itr: 8700, nll: nan\n","itr: 8700, nll: nan\n","itr: 8700, nll: nan\n","itr: 8700, nll: nan\n","itr: 9000, nll: nan\n","itr: 9000, nll: nan\n","itr: 9000, nll: nan\n","itr: 9000, nll: nan\n","itr: 9000, nll: nan\n","itr: 9000, nll: nan\n","itr: 9000, nll: nan\n","itr: 9000, nll: nan\n","itr: 9000, nll: nan\n","itr: 9000, nll: nan\n","itr: 9300, nll: nan\n","itr: 9300, nll: nan\n","itr: 9300, nll: nan\n","itr: 9300, nll: nan\n","itr: 9300, nll: nan\n","itr: 9300, nll: nan\n","itr: 9300, nll: nan\n","itr: 9300, nll: nan\n","itr: 9300, nll: nan\n","itr: 9300, nll: nan\n","itr: 9600, nll: nan\n","itr: 9600, nll: nan\n","itr: 9600, nll: nan\n","itr: 9600, nll: nan\n","itr: 9600, nll: nan\n","itr: 9600, nll: nan\n","itr: 9600, nll: nan\n","itr: 9600, nll: nan\n","itr: 9600, nll: nan\n","itr: 9600, nll: nan\n","itr: 9900, nll: nan\n","itr: 9900, nll: nan\n","itr: 9900, nll: nan\n","itr: 9900, nll: nan\n","itr: 9900, nll: nan\n","itr: 9900, nll: nan\n","itr: 9900, nll: nan\n","itr: 9900, nll: nan\n","itr: 9900, nll: nan\n","itr: 9900, nll: nan\n","itr: 10200, nll: nan\n","itr: 10200, nll: nan\n","itr: 10200, nll: nan\n","itr: 10200, nll: nan\n","itr: 10200, nll: nan\n","itr: 10200, nll: nan\n","itr: 10200, nll: nan\n","itr: 10200, nll: nan\n","itr: 10200, nll: nan\n","itr: 10200, nll: nan\n","itr: 10500, nll: nan\n","itr: 10500, nll: nan\n","itr: 10500, nll: nan\n","itr: 10500, nll: nan\n","itr: 10500, nll: nan\n","itr: 10500, nll: nan\n","itr: 10500, nll: nan\n","itr: 10500, nll: nan\n","itr: 10500, nll: nan\n","itr: 10500, nll: nan\n","itr: 10800, nll: nan\n","itr: 10800, nll: nan\n","itr: 10800, nll: nan\n","itr: 10800, nll: nan\n","itr: 10800, nll: nan\n","itr: 10800, nll: nan\n","itr: 10800, nll: nan\n","itr: 10800, nll: nan\n","itr: 10800, nll: nan\n","itr: 10800, nll: nan\n","itr: 11100, nll: nan\n","itr: 11100, nll: nan\n","itr: 11100, nll: nan\n","itr: 11100, nll: nan\n","itr: 11100, nll: nan\n","itr: 11100, nll: nan\n","itr: 11100, nll: nan\n","itr: 11100, nll: nan\n","itr: 11100, nll: nan\n","itr: 11100, nll: nan\n","itr: 11400, nll: nan\n","itr: 11400, nll: nan\n","itr: 11400, nll: nan\n","itr: 11400, nll: nan\n","itr: 11400, nll: nan\n","itr: 11400, nll: nan\n","itr: 11400, nll: nan\n","itr: 11400, nll: nan\n","itr: 11400, nll: nan\n","itr: 11400, nll: nan\n","itr: 11700, nll: nan\n","itr: 11700, nll: nan\n","itr: 11700, nll: nan\n","itr: 11700, nll: nan\n","itr: 11700, nll: nan\n","itr: 11700, nll: nan\n","itr: 11700, nll: nan\n","itr: 11700, nll: nan\n","itr: 11700, nll: nan\n","itr: 11700, nll: nan\n","itr: 12000, nll: nan\n","itr: 12000, nll: nan\n","itr: 12000, nll: nan\n","itr: 12000, nll: nan\n","itr: 12000, nll: nan\n","itr: 12000, nll: nan\n","itr: 12000, nll: nan\n","itr: 12000, nll: nan\n","itr: 12000, nll: nan\n","itr: 12000, nll: nan\n","itr: 12300, nll: nan\n","itr: 12300, nll: nan\n","itr: 12300, nll: nan\n","itr: 12300, nll: nan\n","itr: 12300, nll: nan\n","itr: 12300, nll: nan\n","itr: 12300, nll: nan\n","itr: 12300, nll: nan\n","itr: 12300, nll: nan\n","itr: 12300, nll: nan\n","itr: 12600, nll: nan\n","itr: 12600, nll: nan\n","itr: 12600, nll: nan\n","itr: 12600, nll: nan\n","itr: 12600, nll: nan\n","itr: 12600, nll: nan\n","itr: 12600, nll: nan\n","itr: 12600, nll: nan\n","itr: 12600, nll: nan\n","itr: 12600, nll: nan\n","itr: 12900, nll: nan\n","itr: 12900, nll: nan\n","itr: 12900, nll: nan\n","itr: 12900, nll: nan\n","itr: 12900, nll: nan\n","itr: 12900, nll: nan\n","itr: 12900, nll: nan\n","itr: 12900, nll: nan\n","itr: 12900, nll: nan\n","itr: 12900, nll: nan\n","itr: 13200, nll: nan\n","itr: 13200, nll: nan\n","itr: 13200, nll: nan\n","itr: 13200, nll: nan\n","itr: 13200, nll: nan\n","itr: 13200, nll: nan\n","itr: 13200, nll: nan\n","itr: 13200, nll: nan\n","itr: 13200, nll: nan\n","itr: 13200, nll: nan\n","itr: 13500, nll: nan\n","itr: 13500, nll: nan\n","itr: 13500, nll: nan\n","itr: 13500, nll: nan\n","itr: 13500, nll: nan\n","itr: 13500, nll: nan\n","itr: 13500, nll: nan\n","itr: 13500, nll: nan\n","itr: 13500, nll: nan\n","itr: 13500, nll: nan\n","itr: 13800, nll: nan\n","itr: 13800, nll: nan\n","itr: 13800, nll: nan\n","itr: 13800, nll: nan\n","itr: 13800, nll: nan\n","itr: 13800, nll: nan\n","itr: 13800, nll: nan\n","itr: 13800, nll: nan\n","itr: 13800, nll: nan\n","itr: 13800, nll: nan\n","itr: 14100, nll: nan\n","itr: 14100, nll: nan\n","itr: 14100, nll: nan\n","itr: 14100, nll: nan\n","itr: 14100, nll: nan\n","itr: 14100, nll: nan\n","itr: 14100, nll: nan\n","itr: 14100, nll: nan\n","itr: 14100, nll: nan\n","itr: 14100, nll: nan\n","itr: 14400, nll: nan\n","itr: 14400, nll: nan\n","itr: 14400, nll: nan\n","itr: 14400, nll: nan\n","itr: 14400, nll: nan\n","itr: 14400, nll: nan\n","itr: 14400, nll: nan\n","itr: 14400, nll: nan\n","itr: 14400, nll: nan\n","itr: 14400, nll: nan\n","itr: 14700, nll: nan\n","itr: 14700, nll: nan\n","itr: 14700, nll: nan\n","itr: 14700, nll: nan\n","itr: 14700, nll: nan\n","itr: 14700, nll: nan\n","itr: 14700, nll: nan\n","itr: 14700, nll: nan\n","itr: 14700, nll: nan\n","itr: 14700, nll: nan\n","itr: 15000, nll: nan\n","itr: 15000, nll: nan\n","itr: 15000, nll: nan\n","itr: 15000, nll: nan\n","itr: 15000, nll: nan\n","itr: 15000, nll: nan\n","itr: 15000, nll: nan\n","itr: 15000, nll: nan\n","itr: 15000, nll: nan\n","itr: 15000, nll: nan\n","itr: 15300, nll: nan\n","itr: 15300, nll: nan\n","itr: 15300, nll: nan\n","itr: 15300, nll: nan\n","itr: 15300, nll: nan\n","itr: 15300, nll: nan\n","itr: 15300, nll: nan\n","itr: 15300, nll: nan\n","itr: 15300, nll: nan\n","itr: 15300, nll: nan\n","itr: 15600, nll: nan\n","itr: 15600, nll: nan\n","itr: 15600, nll: nan\n","itr: 15600, nll: nan\n","itr: 15600, nll: nan\n","itr: 15600, nll: nan\n","itr: 15600, nll: nan\n","itr: 15600, nll: nan\n","itr: 15600, nll: nan\n","itr: 15600, nll: nan\n","itr: 15900, nll: nan\n","itr: 15900, nll: nan\n","itr: 15900, nll: nan\n","itr: 15900, nll: nan\n","itr: 15900, nll: nan\n","itr: 15900, nll: nan\n","itr: 15900, nll: nan\n","itr: 15900, nll: nan\n","itr: 15900, nll: nan\n","itr: 15900, nll: nan\n","itr: 16200, nll: nan\n","itr: 16200, nll: nan\n","itr: 16200, nll: nan\n","itr: 16200, nll: nan\n","itr: 16200, nll: nan\n","itr: 16200, nll: nan\n","itr: 16200, nll: nan\n","itr: 16200, nll: nan\n","itr: 16200, nll: nan\n","itr: 16200, nll: nan\n","itr: 16500, nll: nan\n","itr: 16500, nll: nan\n","itr: 16500, nll: nan\n","itr: 16500, nll: nan\n","itr: 16500, nll: nan\n","itr: 16500, nll: nan\n","itr: 16500, nll: nan\n","itr: 16500, nll: nan\n","itr: 16500, nll: nan\n","itr: 16500, nll: nan\n","itr: 16800, nll: nan\n","itr: 16800, nll: nan\n","itr: 16800, nll: nan\n","itr: 16800, nll: nan\n","itr: 16800, nll: nan\n","itr: 16800, nll: nan\n","itr: 16800, nll: nan\n","itr: 16800, nll: nan\n","itr: 16800, nll: nan\n","itr: 16800, nll: nan\n","itr: 17100, nll: nan\n","itr: 17100, nll: nan\n","itr: 17100, nll: nan\n","itr: 17100, nll: nan\n","itr: 17100, nll: nan\n","itr: 17100, nll: nan\n","itr: 17100, nll: nan\n","itr: 17100, nll: nan\n","itr: 17100, nll: nan\n","itr: 17100, nll: nan\n","itr: 17400, nll: nan\n","itr: 17400, nll: nan\n","itr: 17400, nll: nan\n","itr: 17400, nll: nan\n","itr: 17400, nll: nan\n","itr: 17400, nll: nan\n","itr: 17400, nll: nan\n","itr: 17400, nll: nan\n","itr: 17400, nll: nan\n","itr: 17400, nll: nan\n","itr: 17700, nll: nan\n","itr: 17700, nll: nan\n","itr: 17700, nll: nan\n","itr: 17700, nll: nan\n","itr: 17700, nll: nan\n","itr: 17700, nll: nan\n","itr: 17700, nll: nan\n","itr: 17700, nll: nan\n","itr: 17700, nll: nan\n","itr: 17700, nll: nan\n","itr: 18000, nll: nan\n","itr: 18000, nll: nan\n","itr: 18000, nll: nan\n","itr: 18000, nll: nan\n","itr: 18000, nll: nan\n","itr: 18000, nll: nan\n","itr: 18000, nll: nan\n","itr: 18000, nll: nan\n","itr: 18000, nll: nan\n","itr: 18000, nll: nan\n","itr: 18300, nll: nan\n","itr: 18300, nll: nan\n","itr: 18300, nll: nan\n","itr: 18300, nll: nan\n","itr: 18300, nll: nan\n","itr: 18300, nll: nan\n","itr: 18300, nll: nan\n","itr: 18300, nll: nan\n","itr: 18300, nll: nan\n","itr: 18300, nll: nan\n","itr: 18600, nll: nan\n","itr: 18600, nll: nan\n","itr: 18600, nll: nan\n","itr: 18600, nll: nan\n","itr: 18600, nll: nan\n","itr: 18600, nll: nan\n","itr: 18600, nll: nan\n","itr: 18600, nll: nan\n","itr: 18600, nll: nan\n","itr: 18600, nll: nan\n","itr: 18900, nll: nan\n","itr: 18900, nll: nan\n","itr: 18900, nll: nan\n","itr: 18900, nll: nan\n","itr: 18900, nll: nan\n","itr: 18900, nll: nan\n","itr: 18900, nll: nan\n","itr: 18900, nll: nan\n","itr: 18900, nll: nan\n","itr: 18900, nll: nan\n","itr: 19200, nll: nan\n","itr: 19200, nll: nan\n","itr: 19200, nll: nan\n","itr: 19200, nll: nan\n","itr: 19200, nll: nan\n","itr: 19200, nll: nan\n","itr: 19200, nll: nan\n","itr: 19200, nll: nan\n","itr: 19200, nll: nan\n","itr: 19200, nll: nan\n","itr: 19500, nll: nan\n","itr: 19500, nll: nan\n","itr: 19500, nll: nan\n","itr: 19500, nll: nan\n","itr: 19500, nll: nan\n","itr: 19500, nll: nan\n","itr: 19500, nll: nan\n","itr: 19500, nll: nan\n","itr: 19500, nll: nan\n","itr: 19500, nll: nan\n","itr: 19800, nll: nan\n","itr: 19800, nll: nan\n","itr: 19800, nll: nan\n","itr: 19800, nll: nan\n","itr: 19800, nll: nan\n","itr: 19800, nll: nan\n","itr: 19800, nll: nan\n","itr: 19800, nll: nan\n","itr: 19800, nll: nan\n","itr: 19800, nll: nan\n","itr: 20100, nll: nan\n","itr: 20100, nll: nan\n","itr: 20100, nll: nan\n","itr: 20100, nll: nan\n","itr: 20100, nll: nan\n","itr: 20100, nll: nan\n","itr: 20100, nll: nan\n","itr: 20100, nll: nan\n","itr: 20100, nll: nan\n","itr: 20100, nll: nan\n","itr: 20400, nll: nan\n","itr: 20400, nll: nan\n","itr: 20400, nll: nan\n","itr: 20400, nll: nan\n","itr: 20400, nll: nan\n","itr: 20400, nll: nan\n","itr: 20400, nll: nan\n","itr: 20400, nll: nan\n","itr: 20400, nll: nan\n","itr: 20400, nll: nan\n","itr: 20700, nll: nan\n","itr: 20700, nll: nan\n","itr: 20700, nll: nan\n","itr: 20700, nll: nan\n","itr: 20700, nll: nan\n","itr: 20700, nll: nan\n","itr: 20700, nll: nan\n","itr: 20700, nll: nan\n","itr: 20700, nll: nan\n","itr: 20700, nll: nan\n","itr: 21000, nll: nan\n","itr: 21000, nll: nan\n","itr: 21000, nll: nan\n","itr: 21000, nll: nan\n","itr: 21000, nll: nan\n","itr: 21000, nll: nan\n","itr: 21000, nll: nan\n","itr: 21000, nll: nan\n","itr: 21000, nll: nan\n","itr: 21000, nll: nan\n","itr: 21300, nll: nan\n","itr: 21300, nll: nan\n","itr: 21300, nll: nan\n","itr: 21300, nll: nan\n","itr: 21300, nll: nan\n","itr: 21300, nll: nan\n","itr: 21300, nll: nan\n","itr: 21300, nll: nan\n","itr: 21300, nll: nan\n","itr: 21300, nll: nan\n","itr: 21600, nll: nan\n","itr: 21600, nll: nan\n","itr: 21600, nll: nan\n","itr: 21600, nll: nan\n","itr: 21600, nll: nan\n","itr: 21600, nll: nan\n","itr: 21600, nll: nan\n","itr: 21600, nll: nan\n","itr: 21600, nll: nan\n","itr: 21600, nll: nan\n","itr: 21900, nll: nan\n","itr: 21900, nll: nan\n","itr: 21900, nll: nan\n","itr: 21900, nll: nan\n","itr: 21900, nll: nan\n","itr: 21900, nll: nan\n","itr: 21900, nll: nan\n","itr: 21900, nll: nan\n","itr: 21900, nll: nan\n","itr: 21900, nll: nan\n","itr: 22200, nll: nan\n","itr: 22200, nll: nan\n","itr: 22200, nll: nan\n","itr: 22200, nll: nan\n","itr: 22200, nll: nan\n","itr: 22200, nll: nan\n","itr: 22200, nll: nan\n","itr: 22200, nll: nan\n","itr: 22200, nll: nan\n","itr: 22200, nll: nan\n","itr: 22500, nll: nan\n","itr: 22500, nll: nan\n","itr: 22500, nll: nan\n","itr: 22500, nll: nan\n","itr: 22500, nll: nan\n","itr: 22500, nll: nan\n","itr: 22500, nll: nan\n","itr: 22500, nll: nan\n","itr: 22500, nll: nan\n","itr: 22500, nll: nan\n","itr: 22800, nll: nan\n","itr: 22800, nll: nan\n","itr: 22800, nll: nan\n","itr: 22800, nll: nan\n","itr: 22800, nll: nan\n","itr: 22800, nll: nan\n","itr: 22800, nll: nan\n","itr: 22800, nll: nan\n","itr: 22800, nll: nan\n","itr: 22800, nll: nan\n","itr: 23100, nll: nan\n","itr: 23100, nll: nan\n","itr: 23100, nll: nan\n","itr: 23100, nll: nan\n","itr: 23100, nll: nan\n","itr: 23100, nll: nan\n","itr: 23100, nll: nan\n","itr: 23100, nll: nan\n","itr: 23100, nll: nan\n","itr: 23100, nll: nan\n","itr: 23400, nll: nan\n","itr: 23400, nll: nan\n","itr: 23400, nll: nan\n","itr: 23400, nll: nan\n","itr: 23400, nll: nan\n","itr: 23400, nll: nan\n","itr: 23400, nll: nan\n","itr: 23400, nll: nan\n","itr: 23400, nll: nan\n","itr: 23400, nll: nan\n","itr: 23700, nll: nan\n","itr: 23700, nll: nan\n","itr: 23700, nll: nan\n","itr: 23700, nll: nan\n","itr: 23700, nll: nan\n","itr: 23700, nll: nan\n","itr: 23700, nll: nan\n","itr: 23700, nll: nan\n","itr: 23700, nll: nan\n","itr: 23700, nll: nan\n","itr: 24000, nll: nan\n","itr: 24000, nll: nan\n","itr: 24000, nll: nan\n","itr: 24000, nll: nan\n","itr: 24000, nll: nan\n","itr: 24000, nll: nan\n","itr: 24000, nll: nan\n","itr: 24000, nll: nan\n","itr: 24000, nll: nan\n","itr: 24000, nll: nan\n","itr: 24300, nll: nan\n","itr: 24300, nll: nan\n","itr: 24300, nll: nan\n","itr: 24300, nll: nan\n","itr: 24300, nll: nan\n","itr: 24300, nll: nan\n","itr: 24300, nll: nan\n","itr: 24300, nll: nan\n","itr: 24300, nll: nan\n","itr: 24300, nll: nan\n","itr: 24600, nll: nan\n","itr: 24600, nll: nan\n","itr: 24600, nll: nan\n","itr: 24600, nll: nan\n","itr: 24600, nll: nan\n","itr: 24600, nll: nan\n","itr: 24600, nll: nan\n","itr: 24600, nll: nan\n","itr: 24600, nll: nan\n","itr: 24600, nll: nan\n","itr: 24900, nll: nan\n","itr: 24900, nll: nan\n","itr: 24900, nll: nan\n","itr: 24900, nll: nan\n","itr: 24900, nll: nan\n","itr: 24900, nll: nan\n","itr: 24900, nll: nan\n","itr: 24900, nll: nan\n","itr: 24900, nll: nan\n","itr: 24900, nll: nan\n","itr: 25200, nll: nan\n","itr: 25200, nll: nan\n","itr: 25200, nll: nan\n","itr: 25200, nll: nan\n","itr: 25200, nll: nan\n","itr: 25200, nll: nan\n","itr: 25200, nll: nan\n","itr: 25200, nll: nan\n","itr: 25200, nll: nan\n","itr: 25200, nll: nan\n","itr: 25500, nll: nan\n","itr: 25500, nll: nan\n","itr: 25500, nll: nan\n","itr: 25500, nll: nan\n","itr: 25500, nll: nan\n","itr: 25500, nll: nan\n","itr: 25500, nll: nan\n","itr: 25500, nll: nan\n","itr: 25500, nll: nan\n","itr: 25500, nll: nan\n","itr: 25800, nll: nan\n","itr: 25800, nll: nan\n","itr: 25800, nll: nan\n","itr: 25800, nll: nan\n","itr: 25800, nll: nan\n","itr: 25800, nll: nan\n","itr: 25800, nll: nan\n","itr: 25800, nll: nan\n","itr: 25800, nll: nan\n","itr: 25800, nll: nan\n","itr: 26100, nll: nan\n","itr: 26100, nll: nan\n","itr: 26100, nll: nan\n","itr: 26100, nll: nan\n","itr: 26100, nll: nan\n","itr: 26100, nll: nan\n","itr: 26100, nll: nan\n","itr: 26100, nll: nan\n","itr: 26100, nll: nan\n","itr: 26100, nll: nan\n","itr: 26400, nll: nan\n","itr: 26400, nll: nan\n","itr: 26400, nll: nan\n","itr: 26400, nll: nan\n","itr: 26400, nll: nan\n","itr: 26400, nll: nan\n","itr: 26400, nll: nan\n","itr: 26400, nll: nan\n","itr: 26400, nll: nan\n","itr: 26400, nll: nan\n","itr: 26700, nll: nan\n","itr: 26700, nll: nan\n","itr: 26700, nll: nan\n","itr: 26700, nll: nan\n","itr: 26700, nll: nan\n","itr: 26700, nll: nan\n","itr: 26700, nll: nan\n","itr: 26700, nll: nan\n","itr: 26700, nll: nan\n","itr: 26700, nll: nan\n","itr: 27000, nll: nan\n","itr: 27000, nll: nan\n","itr: 27000, nll: nan\n","itr: 27000, nll: nan\n","itr: 27000, nll: nan\n","itr: 27000, nll: nan\n","itr: 27000, nll: nan\n","itr: 27000, nll: nan\n","itr: 27000, nll: nan\n","itr: 27000, nll: nan\n","itr: 27300, nll: nan\n","itr: 27300, nll: nan\n","itr: 27300, nll: nan\n","itr: 27300, nll: nan\n","itr: 27300, nll: nan\n","itr: 27300, nll: nan\n","itr: 27300, nll: nan\n","itr: 27300, nll: nan\n","itr: 27300, nll: nan\n","itr: 27300, nll: nan\n","itr: 27600, nll: nan\n","itr: 27600, nll: nan\n","itr: 27600, nll: nan\n","itr: 27600, nll: nan\n","itr: 27600, nll: nan\n","itr: 27600, nll: nan\n","itr: 27600, nll: nan\n","itr: 27600, nll: nan\n","itr: 27600, nll: nan\n","itr: 27600, nll: nan\n","itr: 27900, nll: nan\n","itr: 27900, nll: nan\n","itr: 27900, nll: nan\n","itr: 27900, nll: nan\n","itr: 27900, nll: nan\n","itr: 27900, nll: nan\n","itr: 27900, nll: nan\n","itr: 27900, nll: nan\n","itr: 27900, nll: nan\n","itr: 27900, nll: nan\n","itr: 28200, nll: nan\n","itr: 28200, nll: nan\n","itr: 28200, nll: nan\n","itr: 28200, nll: nan\n","itr: 28200, nll: nan\n","itr: 28200, nll: nan\n","itr: 28200, nll: nan\n","itr: 28200, nll: nan\n","itr: 28200, nll: nan\n","itr: 28200, nll: nan\n","itr: 28500, nll: nan\n","itr: 28500, nll: nan\n","itr: 28500, nll: nan\n","itr: 28500, nll: nan\n","itr: 28500, nll: nan\n","itr: 28500, nll: nan\n","itr: 28500, nll: nan\n","itr: 28500, nll: nan\n","itr: 28500, nll: nan\n","itr: 28500, nll: nan\n","itr: 28800, nll: nan\n","itr: 28800, nll: nan\n","itr: 28800, nll: nan\n","itr: 28800, nll: nan\n","itr: 28800, nll: nan\n","itr: 28800, nll: nan\n","itr: 28800, nll: nan\n","itr: 28800, nll: nan\n","itr: 28800, nll: nan\n","itr: 28800, nll: nan\n","itr: 29100, nll: nan\n","itr: 29100, nll: nan\n","itr: 29100, nll: nan\n","itr: 29100, nll: nan\n","itr: 29100, nll: nan\n","itr: 29100, nll: nan\n","itr: 29100, nll: nan\n","itr: 29100, nll: nan\n","itr: 29100, nll: nan\n","itr: 29100, nll: nan\n","itr: 29400, nll: nan\n","itr: 29400, nll: nan\n","itr: 29400, nll: nan\n","itr: 29400, nll: nan\n","itr: 29400, nll: nan\n","itr: 29400, nll: nan\n","itr: 29400, nll: nan\n","itr: 29400, nll: nan\n","itr: 29400, nll: nan\n","itr: 29400, nll: nan\n","itr: 29700, nll: nan\n","itr: 29700, nll: nan\n","itr: 29700, nll: nan\n","itr: 29700, nll: nan\n","itr: 29700, nll: nan\n","itr: 29700, nll: nan\n","itr: 29700, nll: nan\n","itr: 29700, nll: nan\n","itr: 29700, nll: nan\n","itr: 29700, nll: nan\n","min_val is : 0.0\n","max_val is : 25.91068069062784\n","Traceback (most recent call last):\n","  File \"/content/vldb2021_fauce/models_training/train.py\", line 256, in <module>\n","    main()\n","  File \"/content/vldb2021_fauce/models_training/train.py\", line 69, in main\n","    train_ensemble(args)\n","  File \"/content/vldb2021_fauce/models_training/train.py\", line 157, in train_ensemble\n","    test_ensemble(args, ensemble, sess, dataLoader)\n","  File \"/content/vldb2021_fauce/models_training/train.py\", line 166, in test_ensemble\n","    mean, var = ensemble_mean_var(ensemble, test_xs, sess)\n","  File \"/content/vldb2021_fauce/models_training/train.py\", line 93, in ensemble_mean_var\n","    en_mean /= total\n","ZeroDivisionError: division by zero\n"]}]},{"cell_type":"code","source":["!cd vldb2021_fauce/models_training && pipenv run python train.py --dataset /content/normalized_combined_df.csv --output /content/output_normalized_combined_df.csv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5jeaRxmtf2SB","executionInfo":{"status":"ok","timestamp":1714113582831,"user_tz":240,"elapsed":9272942,"user":{"displayName":"Divyanshu Verma","userId":"14078105191069123729"}},"outputId":"d1df58e9-b230-43c9-b43d-cf0de862409b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-04-26 04:05:11.224150: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-26 04:05:11.224221: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-26 04:05:11.225821: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-04-26 04:05:11.234563: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-04-26 04:05:12.380813: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","WARNING:tensorflow:From /root/.local/share/virtualenvs/vldb2021_fauce-rT6f7ILy/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:108: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n","2208\n","246\n","The shape of the data_x is: (2208, 245)\n","[[ 1.09861229]\n"," [13.54837071]\n"," [ 3.40119738]\n"," ...\n"," [21.72199204]\n"," [19.37286807]\n"," [17.90493902]]\n","self.min_val is: 0.0\n","self.max_val is: 25.91068069062784\n","/content/vldb2021_fauce/models_training/utils.py:45: RuntimeWarning: invalid value encountered in divide\n","  self.data_x_standardized = (self.data_x - self.input_mean)/self.input_std\n","The shape of the train_data_x is: (1987, 245)\n","The var_values is: Tensor(\"mul_1:0\", shape=(?, 1), dtype=float32)\n","WARNING:tensorflow:From /root/.local/share/virtualenvs/vldb2021_fauce-rT6f7ILy/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1260: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n","self.nll_gradients is: Tensor(\"gradients/MatMul_grad/MatMul:0\", shape=(?, 245), dtype=float32)\n","The var_values is: Tensor(\"mul_8:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(245, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","WARNING:tensorflow:From /root/.local/share/virtualenvs/vldb2021_fauce-rT6f7ILy/lib/python3.10/site-packages/tensorflow/python/training/rmsprop.py:188: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","The var_values is: Tensor(\"mul_15:0\", shape=(?, 1), dtype=float32)\n","self.nll_gradients is: Tensor(\"gradients_2/MatMul_10_grad/MatMul:0\", shape=(?, 245), dtype=float32)\n","The var_values is: Tensor(\"mul_22:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(245, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","model1MLP/weights_0:0\n","(245, 128)\n","model1MLP/biases_0:0\n","(128,)\n","model1MLP/weights_1:0\n","(128, 256)\n","model1MLP/biases_1:0\n","(256,)\n","model1MLP/weights_2:0\n","(256, 512)\n","model1MLP/biases_2:0\n","(512,)\n","model1MLP/weights_3:0\n","(512, 512)\n","model1MLP/biases_3:0\n","(512,)\n","model1MLP/weights_4:0\n","(512, 2)\n","model1MLP/biases_4:0\n","(2,)\n","The var_values is: Tensor(\"mul_29:0\", shape=(?, 1), dtype=float32)\n","self.nll_gradients is: Tensor(\"gradients_4/MatMul_20_grad/MatMul:0\", shape=(?, 245), dtype=float32)\n","The var_values is: Tensor(\"mul_36:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(245, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","model1MLP/weights_0:0\n","(245, 128)\n","model1MLP/biases_0:0\n","(128,)\n","model1MLP/weights_1:0\n","(128, 256)\n","model1MLP/biases_1:0\n","(256,)\n","model1MLP/weights_2:0\n","(256, 512)\n","model1MLP/biases_2:0\n","(512,)\n","model1MLP/weights_3:0\n","(512, 512)\n","model1MLP/biases_3:0\n","(512,)\n","model1MLP/weights_4:0\n","(512, 2)\n","model1MLP/biases_4:0\n","(2,)\n","model2MLP/weights_0:0\n","(245, 128)\n","model2MLP/biases_0:0\n","(128,)\n","model2MLP/weights_1:0\n","(128, 256)\n","model2MLP/biases_1:0\n","(256,)\n","model2MLP/weights_2:0\n","(256, 512)\n","model2MLP/biases_2:0\n","(512,)\n","model2MLP/weights_3:0\n","(512, 512)\n","model2MLP/biases_3:0\n","(512,)\n","model2MLP/weights_4:0\n","(512, 2)\n","model2MLP/biases_4:0\n","(2,)\n","The var_values is: Tensor(\"mul_43:0\", shape=(?, 1), dtype=float32)\n","self.nll_gradients is: Tensor(\"gradients_6/MatMul_30_grad/MatMul:0\", shape=(?, 245), dtype=float32)\n","The var_values is: Tensor(\"mul_50:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(245, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","model1MLP/weights_0:0\n","(245, 128)\n","model1MLP/biases_0:0\n","(128,)\n","model1MLP/weights_1:0\n","(128, 256)\n","model1MLP/biases_1:0\n","(256,)\n","model1MLP/weights_2:0\n","(256, 512)\n","model1MLP/biases_2:0\n","(512,)\n","model1MLP/weights_3:0\n","(512, 512)\n","model1MLP/biases_3:0\n","(512,)\n","model1MLP/weights_4:0\n","(512, 2)\n","model1MLP/biases_4:0\n","(2,)\n","model2MLP/weights_0:0\n","(245, 128)\n","model2MLP/biases_0:0\n","(128,)\n","model2MLP/weights_1:0\n","(128, 256)\n","model2MLP/biases_1:0\n","(256,)\n","model2MLP/weights_2:0\n","(256, 512)\n","model2MLP/biases_2:0\n","(512,)\n","model2MLP/weights_3:0\n","(512, 512)\n","model2MLP/biases_3:0\n","(512,)\n","model2MLP/weights_4:0\n","(512, 2)\n","model2MLP/biases_4:0\n","(2,)\n","model3MLP/weights_0:0\n","(245, 128)\n","model3MLP/biases_0:0\n","(128,)\n","model3MLP/weights_1:0\n","(128, 256)\n","model3MLP/biases_1:0\n","(256,)\n","model3MLP/weights_2:0\n","(256, 512)\n","model3MLP/biases_2:0\n","(512,)\n","model3MLP/weights_3:0\n","(512, 512)\n","model3MLP/biases_3:0\n","(512,)\n","model3MLP/weights_4:0\n","(512, 2)\n","model3MLP/biases_4:0\n","(2,)\n","The var_values is: Tensor(\"mul_57:0\", shape=(?, 1), dtype=float32)\n","self.nll_gradients is: Tensor(\"gradients_8/MatMul_40_grad/MatMul:0\", shape=(?, 245), dtype=float32)\n","The var_values is: Tensor(\"mul_64:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(245, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","model1MLP/weights_0:0\n","(245, 128)\n","model1MLP/biases_0:0\n","(128,)\n","model1MLP/weights_1:0\n","(128, 256)\n","model1MLP/biases_1:0\n","(256,)\n","model1MLP/weights_2:0\n","(256, 512)\n","model1MLP/biases_2:0\n","(512,)\n","model1MLP/weights_3:0\n","(512, 512)\n","model1MLP/biases_3:0\n","(512,)\n","model1MLP/weights_4:0\n","(512, 2)\n","model1MLP/biases_4:0\n","(2,)\n","model2MLP/weights_0:0\n","(245, 128)\n","model2MLP/biases_0:0\n","(128,)\n","model2MLP/weights_1:0\n","(128, 256)\n","model2MLP/biases_1:0\n","(256,)\n","model2MLP/weights_2:0\n","(256, 512)\n","model2MLP/biases_2:0\n","(512,)\n","model2MLP/weights_3:0\n","(512, 512)\n","model2MLP/biases_3:0\n","(512,)\n","model2MLP/weights_4:0\n","(512, 2)\n","model2MLP/biases_4:0\n","(2,)\n","model3MLP/weights_0:0\n","(245, 128)\n","model3MLP/biases_0:0\n","(128,)\n","model3MLP/weights_1:0\n","(128, 256)\n","model3MLP/biases_1:0\n","(256,)\n","model3MLP/weights_2:0\n","(256, 512)\n","model3MLP/biases_2:0\n","(512,)\n","model3MLP/weights_3:0\n","(512, 512)\n","model3MLP/biases_3:0\n","(512,)\n","model3MLP/weights_4:0\n","(512, 2)\n","model3MLP/biases_4:0\n","(2,)\n","model4MLP/weights_0:0\n","(245, 128)\n","model4MLP/biases_0:0\n","(128,)\n","model4MLP/weights_1:0\n","(128, 256)\n","model4MLP/biases_1:0\n","(256,)\n","model4MLP/weights_2:0\n","(256, 512)\n","model4MLP/biases_2:0\n","(512,)\n","model4MLP/weights_3:0\n","(512, 512)\n","model4MLP/biases_3:0\n","(512,)\n","model4MLP/weights_4:0\n","(512, 2)\n","model4MLP/biases_4:0\n","(2,)\n","The var_values is: Tensor(\"mul_71:0\", shape=(?, 1), dtype=float32)\n","self.nll_gradients is: Tensor(\"gradients_10/MatMul_50_grad/MatMul:0\", shape=(?, 245), dtype=float32)\n","The var_values is: Tensor(\"mul_78:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(245, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","model1MLP/weights_0:0\n","(245, 128)\n","model1MLP/biases_0:0\n","(128,)\n","model1MLP/weights_1:0\n","(128, 256)\n","model1MLP/biases_1:0\n","(256,)\n","model1MLP/weights_2:0\n","(256, 512)\n","model1MLP/biases_2:0\n","(512,)\n","model1MLP/weights_3:0\n","(512, 512)\n","model1MLP/biases_3:0\n","(512,)\n","model1MLP/weights_4:0\n","(512, 2)\n","model1MLP/biases_4:0\n","(2,)\n","model2MLP/weights_0:0\n","(245, 128)\n","model2MLP/biases_0:0\n","(128,)\n","model2MLP/weights_1:0\n","(128, 256)\n","model2MLP/biases_1:0\n","(256,)\n","model2MLP/weights_2:0\n","(256, 512)\n","model2MLP/biases_2:0\n","(512,)\n","model2MLP/weights_3:0\n","(512, 512)\n","model2MLP/biases_3:0\n","(512,)\n","model2MLP/weights_4:0\n","(512, 2)\n","model2MLP/biases_4:0\n","(2,)\n","model3MLP/weights_0:0\n","(245, 128)\n","model3MLP/biases_0:0\n","(128,)\n","model3MLP/weights_1:0\n","(128, 256)\n","model3MLP/biases_1:0\n","(256,)\n","model3MLP/weights_2:0\n","(256, 512)\n","model3MLP/biases_2:0\n","(512,)\n","model3MLP/weights_3:0\n","(512, 512)\n","model3MLP/biases_3:0\n","(512,)\n","model3MLP/weights_4:0\n","(512, 2)\n","model3MLP/biases_4:0\n","(2,)\n","model4MLP/weights_0:0\n","(245, 128)\n","model4MLP/biases_0:0\n","(128,)\n","model4MLP/weights_1:0\n","(128, 256)\n","model4MLP/biases_1:0\n","(256,)\n","model4MLP/weights_2:0\n","(256, 512)\n","model4MLP/biases_2:0\n","(512,)\n","model4MLP/weights_3:0\n","(512, 512)\n","model4MLP/biases_3:0\n","(512,)\n","model4MLP/weights_4:0\n","(512, 2)\n","model4MLP/biases_4:0\n","(2,)\n","model5MLP/weights_0:0\n","(245, 128)\n","model5MLP/biases_0:0\n","(128,)\n","model5MLP/weights_1:0\n","(128, 256)\n","model5MLP/biases_1:0\n","(256,)\n","model5MLP/weights_2:0\n","(256, 512)\n","model5MLP/biases_2:0\n","(512,)\n","model5MLP/weights_3:0\n","(512, 512)\n","model5MLP/biases_3:0\n","(512,)\n","model5MLP/weights_4:0\n","(512, 2)\n","model5MLP/biases_4:0\n","(2,)\n","The var_values is: Tensor(\"mul_85:0\", shape=(?, 1), dtype=float32)\n","self.nll_gradients is: Tensor(\"gradients_12/MatMul_60_grad/MatMul:0\", shape=(?, 245), dtype=float32)\n","The var_values is: Tensor(\"mul_92:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(245, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","model1MLP/weights_0:0\n","(245, 128)\n","model1MLP/biases_0:0\n","(128,)\n","model1MLP/weights_1:0\n","(128, 256)\n","model1MLP/biases_1:0\n","(256,)\n","model1MLP/weights_2:0\n","(256, 512)\n","model1MLP/biases_2:0\n","(512,)\n","model1MLP/weights_3:0\n","(512, 512)\n","model1MLP/biases_3:0\n","(512,)\n","model1MLP/weights_4:0\n","(512, 2)\n","model1MLP/biases_4:0\n","(2,)\n","model2MLP/weights_0:0\n","(245, 128)\n","model2MLP/biases_0:0\n","(128,)\n","model2MLP/weights_1:0\n","(128, 256)\n","model2MLP/biases_1:0\n","(256,)\n","model2MLP/weights_2:0\n","(256, 512)\n","model2MLP/biases_2:0\n","(512,)\n","model2MLP/weights_3:0\n","(512, 512)\n","model2MLP/biases_3:0\n","(512,)\n","model2MLP/weights_4:0\n","(512, 2)\n","model2MLP/biases_4:0\n","(2,)\n","model3MLP/weights_0:0\n","(245, 128)\n","model3MLP/biases_0:0\n","(128,)\n","model3MLP/weights_1:0\n","(128, 256)\n","model3MLP/biases_1:0\n","(256,)\n","model3MLP/weights_2:0\n","(256, 512)\n","model3MLP/biases_2:0\n","(512,)\n","model3MLP/weights_3:0\n","(512, 512)\n","model3MLP/biases_3:0\n","(512,)\n","model3MLP/weights_4:0\n","(512, 2)\n","model3MLP/biases_4:0\n","(2,)\n","model4MLP/weights_0:0\n","(245, 128)\n","model4MLP/biases_0:0\n","(128,)\n","model4MLP/weights_1:0\n","(128, 256)\n","model4MLP/biases_1:0\n","(256,)\n","model4MLP/weights_2:0\n","(256, 512)\n","model4MLP/biases_2:0\n","(512,)\n","model4MLP/weights_3:0\n","(512, 512)\n","model4MLP/biases_3:0\n","(512,)\n","model4MLP/weights_4:0\n","(512, 2)\n","model4MLP/biases_4:0\n","(2,)\n","model5MLP/weights_0:0\n","(245, 128)\n","model5MLP/biases_0:0\n","(128,)\n","model5MLP/weights_1:0\n","(128, 256)\n","model5MLP/biases_1:0\n","(256,)\n","model5MLP/weights_2:0\n","(256, 512)\n","model5MLP/biases_2:0\n","(512,)\n","model5MLP/weights_3:0\n","(512, 512)\n","model5MLP/biases_3:0\n","(512,)\n","model5MLP/weights_4:0\n","(512, 2)\n","model5MLP/biases_4:0\n","(2,)\n","model6MLP/weights_0:0\n","(245, 128)\n","model6MLP/biases_0:0\n","(128,)\n","model6MLP/weights_1:0\n","(128, 256)\n","model6MLP/biases_1:0\n","(256,)\n","model6MLP/weights_2:0\n","(256, 512)\n","model6MLP/biases_2:0\n","(512,)\n","model6MLP/weights_3:0\n","(512, 512)\n","model6MLP/biases_3:0\n","(512,)\n","model6MLP/weights_4:0\n","(512, 2)\n","model6MLP/biases_4:0\n","(2,)\n","The var_values is: Tensor(\"mul_99:0\", shape=(?, 1), dtype=float32)\n","self.nll_gradients is: Tensor(\"gradients_14/MatMul_70_grad/MatMul:0\", shape=(?, 245), dtype=float32)\n","The var_values is: Tensor(\"mul_106:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(245, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","model1MLP/weights_0:0\n","(245, 128)\n","model1MLP/biases_0:0\n","(128,)\n","model1MLP/weights_1:0\n","(128, 256)\n","model1MLP/biases_1:0\n","(256,)\n","model1MLP/weights_2:0\n","(256, 512)\n","model1MLP/biases_2:0\n","(512,)\n","model1MLP/weights_3:0\n","(512, 512)\n","model1MLP/biases_3:0\n","(512,)\n","model1MLP/weights_4:0\n","(512, 2)\n","model1MLP/biases_4:0\n","(2,)\n","model2MLP/weights_0:0\n","(245, 128)\n","model2MLP/biases_0:0\n","(128,)\n","model2MLP/weights_1:0\n","(128, 256)\n","model2MLP/biases_1:0\n","(256,)\n","model2MLP/weights_2:0\n","(256, 512)\n","model2MLP/biases_2:0\n","(512,)\n","model2MLP/weights_3:0\n","(512, 512)\n","model2MLP/biases_3:0\n","(512,)\n","model2MLP/weights_4:0\n","(512, 2)\n","model2MLP/biases_4:0\n","(2,)\n","model3MLP/weights_0:0\n","(245, 128)\n","model3MLP/biases_0:0\n","(128,)\n","model3MLP/weights_1:0\n","(128, 256)\n","model3MLP/biases_1:0\n","(256,)\n","model3MLP/weights_2:0\n","(256, 512)\n","model3MLP/biases_2:0\n","(512,)\n","model3MLP/weights_3:0\n","(512, 512)\n","model3MLP/biases_3:0\n","(512,)\n","model3MLP/weights_4:0\n","(512, 2)\n","model3MLP/biases_4:0\n","(2,)\n","model4MLP/weights_0:0\n","(245, 128)\n","model4MLP/biases_0:0\n","(128,)\n","model4MLP/weights_1:0\n","(128, 256)\n","model4MLP/biases_1:0\n","(256,)\n","model4MLP/weights_2:0\n","(256, 512)\n","model4MLP/biases_2:0\n","(512,)\n","model4MLP/weights_3:0\n","(512, 512)\n","model4MLP/biases_3:0\n","(512,)\n","model4MLP/weights_4:0\n","(512, 2)\n","model4MLP/biases_4:0\n","(2,)\n","model5MLP/weights_0:0\n","(245, 128)\n","model5MLP/biases_0:0\n","(128,)\n","model5MLP/weights_1:0\n","(128, 256)\n","model5MLP/biases_1:0\n","(256,)\n","model5MLP/weights_2:0\n","(256, 512)\n","model5MLP/biases_2:0\n","(512,)\n","model5MLP/weights_3:0\n","(512, 512)\n","model5MLP/biases_3:0\n","(512,)\n","model5MLP/weights_4:0\n","(512, 2)\n","model5MLP/biases_4:0\n","(2,)\n","model6MLP/weights_0:0\n","(245, 128)\n","model6MLP/biases_0:0\n","(128,)\n","model6MLP/weights_1:0\n","(128, 256)\n","model6MLP/biases_1:0\n","(256,)\n","model6MLP/weights_2:0\n","(256, 512)\n","model6MLP/biases_2:0\n","(512,)\n","model6MLP/weights_3:0\n","(512, 512)\n","model6MLP/biases_3:0\n","(512,)\n","model6MLP/weights_4:0\n","(512, 2)\n","model6MLP/biases_4:0\n","(2,)\n","model7MLP/weights_0:0\n","(245, 128)\n","model7MLP/biases_0:0\n","(128,)\n","model7MLP/weights_1:0\n","(128, 256)\n","model7MLP/biases_1:0\n","(256,)\n","model7MLP/weights_2:0\n","(256, 512)\n","model7MLP/biases_2:0\n","(512,)\n","model7MLP/weights_3:0\n","(512, 512)\n","model7MLP/biases_3:0\n","(512,)\n","model7MLP/weights_4:0\n","(512, 2)\n","model7MLP/biases_4:0\n","(2,)\n","The var_values is: Tensor(\"mul_113:0\", shape=(?, 1), dtype=float32)\n","self.nll_gradients is: Tensor(\"gradients_16/MatMul_80_grad/MatMul:0\", shape=(?, 245), dtype=float32)\n","The var_values is: Tensor(\"mul_120:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(245, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","model1MLP/weights_0:0\n","(245, 128)\n","model1MLP/biases_0:0\n","(128,)\n","model1MLP/weights_1:0\n","(128, 256)\n","model1MLP/biases_1:0\n","(256,)\n","model1MLP/weights_2:0\n","(256, 512)\n","model1MLP/biases_2:0\n","(512,)\n","model1MLP/weights_3:0\n","(512, 512)\n","model1MLP/biases_3:0\n","(512,)\n","model1MLP/weights_4:0\n","(512, 2)\n","model1MLP/biases_4:0\n","(2,)\n","model2MLP/weights_0:0\n","(245, 128)\n","model2MLP/biases_0:0\n","(128,)\n","model2MLP/weights_1:0\n","(128, 256)\n","model2MLP/biases_1:0\n","(256,)\n","model2MLP/weights_2:0\n","(256, 512)\n","model2MLP/biases_2:0\n","(512,)\n","model2MLP/weights_3:0\n","(512, 512)\n","model2MLP/biases_3:0\n","(512,)\n","model2MLP/weights_4:0\n","(512, 2)\n","model2MLP/biases_4:0\n","(2,)\n","model3MLP/weights_0:0\n","(245, 128)\n","model3MLP/biases_0:0\n","(128,)\n","model3MLP/weights_1:0\n","(128, 256)\n","model3MLP/biases_1:0\n","(256,)\n","model3MLP/weights_2:0\n","(256, 512)\n","model3MLP/biases_2:0\n","(512,)\n","model3MLP/weights_3:0\n","(512, 512)\n","model3MLP/biases_3:0\n","(512,)\n","model3MLP/weights_4:0\n","(512, 2)\n","model3MLP/biases_4:0\n","(2,)\n","model4MLP/weights_0:0\n","(245, 128)\n","model4MLP/biases_0:0\n","(128,)\n","model4MLP/weights_1:0\n","(128, 256)\n","model4MLP/biases_1:0\n","(256,)\n","model4MLP/weights_2:0\n","(256, 512)\n","model4MLP/biases_2:0\n","(512,)\n","model4MLP/weights_3:0\n","(512, 512)\n","model4MLP/biases_3:0\n","(512,)\n","model4MLP/weights_4:0\n","(512, 2)\n","model4MLP/biases_4:0\n","(2,)\n","model5MLP/weights_0:0\n","(245, 128)\n","model5MLP/biases_0:0\n","(128,)\n","model5MLP/weights_1:0\n","(128, 256)\n","model5MLP/biases_1:0\n","(256,)\n","model5MLP/weights_2:0\n","(256, 512)\n","model5MLP/biases_2:0\n","(512,)\n","model5MLP/weights_3:0\n","(512, 512)\n","model5MLP/biases_3:0\n","(512,)\n","model5MLP/weights_4:0\n","(512, 2)\n","model5MLP/biases_4:0\n","(2,)\n","model6MLP/weights_0:0\n","(245, 128)\n","model6MLP/biases_0:0\n","(128,)\n","model6MLP/weights_1:0\n","(128, 256)\n","model6MLP/biases_1:0\n","(256,)\n","model6MLP/weights_2:0\n","(256, 512)\n","model6MLP/biases_2:0\n","(512,)\n","model6MLP/weights_3:0\n","(512, 512)\n","model6MLP/biases_3:0\n","(512,)\n","model6MLP/weights_4:0\n","(512, 2)\n","model6MLP/biases_4:0\n","(2,)\n","model7MLP/weights_0:0\n","(245, 128)\n","model7MLP/biases_0:0\n","(128,)\n","model7MLP/weights_1:0\n","(128, 256)\n","model7MLP/biases_1:0\n","(256,)\n","model7MLP/weights_2:0\n","(256, 512)\n","model7MLP/biases_2:0\n","(512,)\n","model7MLP/weights_3:0\n","(512, 512)\n","model7MLP/biases_3:0\n","(512,)\n","model7MLP/weights_4:0\n","(512, 2)\n","model7MLP/biases_4:0\n","(2,)\n","model8MLP/weights_0:0\n","(245, 128)\n","model8MLP/biases_0:0\n","(128,)\n","model8MLP/weights_1:0\n","(128, 256)\n","model8MLP/biases_1:0\n","(256,)\n","model8MLP/weights_2:0\n","(256, 512)\n","model8MLP/biases_2:0\n","(512,)\n","model8MLP/weights_3:0\n","(512, 512)\n","model8MLP/biases_3:0\n","(512,)\n","model8MLP/weights_4:0\n","(512, 2)\n","model8MLP/biases_4:0\n","(2,)\n","The var_values is: Tensor(\"mul_127:0\", shape=(?, 1), dtype=float32)\n","self.nll_gradients is: Tensor(\"gradients_18/MatMul_90_grad/MatMul:0\", shape=(?, 245), dtype=float32)\n","The var_values is: Tensor(\"mul_134:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(245, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","model1MLP/weights_0:0\n","(245, 128)\n","model1MLP/biases_0:0\n","(128,)\n","model1MLP/weights_1:0\n","(128, 256)\n","model1MLP/biases_1:0\n","(256,)\n","model1MLP/weights_2:0\n","(256, 512)\n","model1MLP/biases_2:0\n","(512,)\n","model1MLP/weights_3:0\n","(512, 512)\n","model1MLP/biases_3:0\n","(512,)\n","model1MLP/weights_4:0\n","(512, 2)\n","model1MLP/biases_4:0\n","(2,)\n","model2MLP/weights_0:0\n","(245, 128)\n","model2MLP/biases_0:0\n","(128,)\n","model2MLP/weights_1:0\n","(128, 256)\n","model2MLP/biases_1:0\n","(256,)\n","model2MLP/weights_2:0\n","(256, 512)\n","model2MLP/biases_2:0\n","(512,)\n","model2MLP/weights_3:0\n","(512, 512)\n","model2MLP/biases_3:0\n","(512,)\n","model2MLP/weights_4:0\n","(512, 2)\n","model2MLP/biases_4:0\n","(2,)\n","model3MLP/weights_0:0\n","(245, 128)\n","model3MLP/biases_0:0\n","(128,)\n","model3MLP/weights_1:0\n","(128, 256)\n","model3MLP/biases_1:0\n","(256,)\n","model3MLP/weights_2:0\n","(256, 512)\n","model3MLP/biases_2:0\n","(512,)\n","model3MLP/weights_3:0\n","(512, 512)\n","model3MLP/biases_3:0\n","(512,)\n","model3MLP/weights_4:0\n","(512, 2)\n","model3MLP/biases_4:0\n","(2,)\n","model4MLP/weights_0:0\n","(245, 128)\n","model4MLP/biases_0:0\n","(128,)\n","model4MLP/weights_1:0\n","(128, 256)\n","model4MLP/biases_1:0\n","(256,)\n","model4MLP/weights_2:0\n","(256, 512)\n","model4MLP/biases_2:0\n","(512,)\n","model4MLP/weights_3:0\n","(512, 512)\n","model4MLP/biases_3:0\n","(512,)\n","model4MLP/weights_4:0\n","(512, 2)\n","model4MLP/biases_4:0\n","(2,)\n","model5MLP/weights_0:0\n","(245, 128)\n","model5MLP/biases_0:0\n","(128,)\n","model5MLP/weights_1:0\n","(128, 256)\n","model5MLP/biases_1:0\n","(256,)\n","model5MLP/weights_2:0\n","(256, 512)\n","model5MLP/biases_2:0\n","(512,)\n","model5MLP/weights_3:0\n","(512, 512)\n","model5MLP/biases_3:0\n","(512,)\n","model5MLP/weights_4:0\n","(512, 2)\n","model5MLP/biases_4:0\n","(2,)\n","model6MLP/weights_0:0\n","(245, 128)\n","model6MLP/biases_0:0\n","(128,)\n","model6MLP/weights_1:0\n","(128, 256)\n","model6MLP/biases_1:0\n","(256,)\n","model6MLP/weights_2:0\n","(256, 512)\n","model6MLP/biases_2:0\n","(512,)\n","model6MLP/weights_3:0\n","(512, 512)\n","model6MLP/biases_3:0\n","(512,)\n","model6MLP/weights_4:0\n","(512, 2)\n","model6MLP/biases_4:0\n","(2,)\n","model7MLP/weights_0:0\n","(245, 128)\n","model7MLP/biases_0:0\n","(128,)\n","model7MLP/weights_1:0\n","(128, 256)\n","model7MLP/biases_1:0\n","(256,)\n","model7MLP/weights_2:0\n","(256, 512)\n","model7MLP/biases_2:0\n","(512,)\n","model7MLP/weights_3:0\n","(512, 512)\n","model7MLP/biases_3:0\n","(512,)\n","model7MLP/weights_4:0\n","(512, 2)\n","model7MLP/biases_4:0\n","(2,)\n","model8MLP/weights_0:0\n","(245, 128)\n","model8MLP/biases_0:0\n","(128,)\n","model8MLP/weights_1:0\n","(128, 256)\n","model8MLP/biases_1:0\n","(256,)\n","model8MLP/weights_2:0\n","(256, 512)\n","model8MLP/biases_2:0\n","(512,)\n","model8MLP/weights_3:0\n","(512, 512)\n","model8MLP/biases_3:0\n","(512,)\n","model8MLP/weights_4:0\n","(512, 2)\n","model8MLP/biases_4:0\n","(2,)\n","model9MLP/weights_0:0\n","(245, 128)\n","model9MLP/biases_0:0\n","(128,)\n","model9MLP/weights_1:0\n","(128, 256)\n","model9MLP/biases_1:0\n","(256,)\n","model9MLP/weights_2:0\n","(256, 512)\n","model9MLP/biases_2:0\n","(512,)\n","model9MLP/weights_3:0\n","(512, 512)\n","model9MLP/biases_3:0\n","(512,)\n","model9MLP/weights_4:0\n","(512, 2)\n","model9MLP/biases_4:0\n","(2,)\n","2024-04-26 04:05:24.371723: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n","itr: 0, nll: nan\n","itr: 0, nll: nan\n","itr: 0, nll: nan\n","itr: 0, nll: nan\n","itr: 0, nll: nan\n","itr: 0, nll: nan\n","itr: 0, nll: nan\n","itr: 0, nll: nan\n","itr: 0, nll: nan\n","itr: 0, nll: nan\n","itr: 300, nll: nan\n","itr: 300, nll: nan\n","itr: 300, nll: nan\n","itr: 300, nll: nan\n","itr: 300, nll: nan\n","itr: 300, nll: nan\n","itr: 300, nll: nan\n","itr: 300, nll: nan\n","itr: 300, nll: nan\n","itr: 300, nll: nan\n","itr: 600, nll: nan\n","itr: 600, nll: nan\n","itr: 600, nll: nan\n","itr: 600, nll: nan\n","itr: 600, nll: nan\n","itr: 600, nll: nan\n","itr: 600, nll: nan\n","itr: 600, nll: nan\n","itr: 600, nll: nan\n","itr: 600, nll: nan\n","itr: 900, nll: nan\n","itr: 900, nll: nan\n","itr: 900, nll: nan\n","itr: 900, nll: nan\n","itr: 900, nll: nan\n","itr: 900, nll: nan\n","itr: 900, nll: nan\n","itr: 900, nll: nan\n","itr: 900, nll: nan\n","itr: 900, nll: nan\n","itr: 1200, nll: nan\n","itr: 1200, nll: nan\n","itr: 1200, nll: nan\n","itr: 1200, nll: nan\n","itr: 1200, nll: nan\n","itr: 1200, nll: nan\n","itr: 1200, nll: nan\n","itr: 1200, nll: nan\n","itr: 1200, nll: nan\n","itr: 1200, nll: nan\n","itr: 1500, nll: nan\n","itr: 1500, nll: nan\n","itr: 1500, nll: nan\n","itr: 1500, nll: nan\n","itr: 1500, nll: nan\n","itr: 1500, nll: nan\n","itr: 1500, nll: nan\n","itr: 1500, nll: nan\n","itr: 1500, nll: nan\n","itr: 1500, nll: nan\n","itr: 1800, nll: nan\n","itr: 1800, nll: nan\n","itr: 1800, nll: nan\n","itr: 1800, nll: nan\n","itr: 1800, nll: nan\n","itr: 1800, nll: nan\n","itr: 1800, nll: nan\n","itr: 1800, nll: nan\n","itr: 1800, nll: nan\n","itr: 1800, nll: nan\n","itr: 2100, nll: nan\n","itr: 2100, nll: nan\n","itr: 2100, nll: nan\n","itr: 2100, nll: nan\n","itr: 2100, nll: nan\n","itr: 2100, nll: nan\n","itr: 2100, nll: nan\n","itr: 2100, nll: nan\n","itr: 2100, nll: nan\n","itr: 2100, nll: nan\n","itr: 2400, nll: nan\n","itr: 2400, nll: nan\n","itr: 2400, nll: nan\n","itr: 2400, nll: nan\n","itr: 2400, nll: nan\n","itr: 2400, nll: nan\n","itr: 2400, nll: nan\n","itr: 2400, nll: nan\n","itr: 2400, nll: nan\n","itr: 2400, nll: nan\n","itr: 2700, nll: nan\n","itr: 2700, nll: nan\n","itr: 2700, nll: nan\n","itr: 2700, nll: nan\n","itr: 2700, nll: nan\n","itr: 2700, nll: nan\n","itr: 2700, nll: nan\n","itr: 2700, nll: nan\n","itr: 2700, nll: nan\n","itr: 2700, nll: nan\n","itr: 3000, nll: nan\n","itr: 3000, nll: nan\n","itr: 3000, nll: nan\n","itr: 3000, nll: nan\n","itr: 3000, nll: nan\n","itr: 3000, nll: nan\n","itr: 3000, nll: nan\n","itr: 3000, nll: nan\n","itr: 3000, nll: nan\n","itr: 3000, nll: nan\n","itr: 3300, nll: nan\n","itr: 3300, nll: nan\n","itr: 3300, nll: nan\n","itr: 3300, nll: nan\n","itr: 3300, nll: nan\n","itr: 3300, nll: nan\n","itr: 3300, nll: nan\n","itr: 3300, nll: nan\n","itr: 3300, nll: nan\n","itr: 3300, nll: nan\n","itr: 3600, nll: nan\n","itr: 3600, nll: nan\n","itr: 3600, nll: nan\n","itr: 3600, nll: nan\n","itr: 3600, nll: nan\n","itr: 3600, nll: nan\n","itr: 3600, nll: nan\n","itr: 3600, nll: nan\n","itr: 3600, nll: nan\n","itr: 3600, nll: nan\n","itr: 3900, nll: nan\n","itr: 3900, nll: nan\n","itr: 3900, nll: nan\n","itr: 3900, nll: nan\n","itr: 3900, nll: nan\n","itr: 3900, nll: nan\n","itr: 3900, nll: nan\n","itr: 3900, nll: nan\n","itr: 3900, nll: nan\n","itr: 3900, nll: nan\n","itr: 4200, nll: nan\n","itr: 4200, nll: nan\n","itr: 4200, nll: nan\n","itr: 4200, nll: nan\n","itr: 4200, nll: nan\n","itr: 4200, nll: nan\n","itr: 4200, nll: nan\n","itr: 4200, nll: nan\n","itr: 4200, nll: nan\n","itr: 4200, nll: nan\n","itr: 4500, nll: nan\n","itr: 4500, nll: nan\n","itr: 4500, nll: nan\n","itr: 4500, nll: nan\n","itr: 4500, nll: nan\n","itr: 4500, nll: nan\n","itr: 4500, nll: nan\n","itr: 4500, nll: nan\n","itr: 4500, nll: nan\n","itr: 4500, nll: nan\n","itr: 4800, nll: nan\n","itr: 4800, nll: nan\n","itr: 4800, nll: nan\n","itr: 4800, nll: nan\n","itr: 4800, nll: nan\n","itr: 4800, nll: nan\n","itr: 4800, nll: nan\n","itr: 4800, nll: nan\n","itr: 4800, nll: nan\n","itr: 4800, nll: nan\n","itr: 5100, nll: nan\n","itr: 5100, nll: nan\n","itr: 5100, nll: nan\n","itr: 5100, nll: nan\n","itr: 5100, nll: nan\n","itr: 5100, nll: nan\n","itr: 5100, nll: nan\n","itr: 5100, nll: nan\n","itr: 5100, nll: nan\n","itr: 5100, nll: nan\n","itr: 5400, nll: nan\n","itr: 5400, nll: nan\n","itr: 5400, nll: nan\n","itr: 5400, nll: nan\n","itr: 5400, nll: nan\n","itr: 5400, nll: nan\n","itr: 5400, nll: nan\n","itr: 5400, nll: nan\n","itr: 5400, nll: nan\n","itr: 5400, nll: nan\n","itr: 5700, nll: nan\n","itr: 5700, nll: nan\n","itr: 5700, nll: nan\n","itr: 5700, nll: nan\n","itr: 5700, nll: nan\n","itr: 5700, nll: nan\n","itr: 5700, nll: nan\n","itr: 5700, nll: nan\n","itr: 5700, nll: nan\n","itr: 5700, nll: nan\n","itr: 6000, nll: nan\n","itr: 6000, nll: nan\n","itr: 6000, nll: nan\n","itr: 6000, nll: nan\n","itr: 6000, nll: nan\n","itr: 6000, nll: nan\n","itr: 6000, nll: nan\n","itr: 6000, nll: nan\n","itr: 6000, nll: nan\n","itr: 6000, nll: nan\n","itr: 6300, nll: nan\n","itr: 6300, nll: nan\n","itr: 6300, nll: nan\n","itr: 6300, nll: nan\n","itr: 6300, nll: nan\n","itr: 6300, nll: nan\n","itr: 6300, nll: nan\n","itr: 6300, nll: nan\n","itr: 6300, nll: nan\n","itr: 6300, nll: nan\n","itr: 6600, nll: nan\n","itr: 6600, nll: nan\n","itr: 6600, nll: nan\n","itr: 6600, nll: nan\n","itr: 6600, nll: nan\n","itr: 6600, nll: nan\n","itr: 6600, nll: nan\n","itr: 6600, nll: nan\n","itr: 6600, nll: nan\n","itr: 6600, nll: nan\n","itr: 6900, nll: nan\n","itr: 6900, nll: nan\n","itr: 6900, nll: nan\n","itr: 6900, nll: nan\n","itr: 6900, nll: nan\n","itr: 6900, nll: nan\n","itr: 6900, nll: nan\n","itr: 6900, nll: nan\n","itr: 6900, nll: nan\n","itr: 6900, nll: nan\n","itr: 7200, nll: nan\n","itr: 7200, nll: nan\n","itr: 7200, nll: nan\n","itr: 7200, nll: nan\n","itr: 7200, nll: nan\n","itr: 7200, nll: nan\n","itr: 7200, nll: nan\n","itr: 7200, nll: nan\n","itr: 7200, nll: nan\n","itr: 7200, nll: nan\n","itr: 7500, nll: nan\n","itr: 7500, nll: nan\n","itr: 7500, nll: nan\n","itr: 7500, nll: nan\n","itr: 7500, nll: nan\n","itr: 7500, nll: nan\n","itr: 7500, nll: nan\n","itr: 7500, nll: nan\n","itr: 7500, nll: nan\n","itr: 7500, nll: nan\n","itr: 7800, nll: nan\n","itr: 7800, nll: nan\n","itr: 7800, nll: nan\n","itr: 7800, nll: nan\n","itr: 7800, nll: nan\n","itr: 7800, nll: nan\n","itr: 7800, nll: nan\n","itr: 7800, nll: nan\n","itr: 7800, nll: nan\n","itr: 7800, nll: nan\n","itr: 8100, nll: nan\n","itr: 8100, nll: nan\n","itr: 8100, nll: nan\n","itr: 8100, nll: nan\n","itr: 8100, nll: nan\n","itr: 8100, nll: nan\n","itr: 8100, nll: nan\n","itr: 8100, nll: nan\n","itr: 8100, nll: nan\n","itr: 8100, nll: nan\n","itr: 8400, nll: nan\n","itr: 8400, nll: nan\n","itr: 8400, nll: nan\n","itr: 8400, nll: nan\n","itr: 8400, nll: nan\n","itr: 8400, nll: nan\n","itr: 8400, nll: nan\n","itr: 8400, nll: nan\n","itr: 8400, nll: nan\n","itr: 8400, nll: nan\n","itr: 8700, nll: nan\n","itr: 8700, nll: nan\n","itr: 8700, nll: nan\n","itr: 8700, nll: nan\n","itr: 8700, nll: nan\n","itr: 8700, nll: nan\n","itr: 8700, nll: nan\n","itr: 8700, nll: nan\n","itr: 8700, nll: nan\n","itr: 8700, nll: nan\n","itr: 9000, nll: nan\n","itr: 9000, nll: nan\n","itr: 9000, nll: nan\n","itr: 9000, nll: nan\n","itr: 9000, nll: nan\n","itr: 9000, nll: nan\n","itr: 9000, nll: nan\n","itr: 9000, nll: nan\n","itr: 9000, nll: nan\n","itr: 9000, nll: nan\n","itr: 9300, nll: nan\n","itr: 9300, nll: nan\n","itr: 9300, nll: nan\n","itr: 9300, nll: nan\n","itr: 9300, nll: nan\n","itr: 9300, nll: nan\n","itr: 9300, nll: nan\n","itr: 9300, nll: nan\n","itr: 9300, nll: nan\n","itr: 9300, nll: nan\n","itr: 9600, nll: nan\n","itr: 9600, nll: nan\n","itr: 9600, nll: nan\n","itr: 9600, nll: nan\n","itr: 9600, nll: nan\n","itr: 9600, nll: nan\n","itr: 9600, nll: nan\n","itr: 9600, nll: nan\n","itr: 9600, nll: nan\n","itr: 9600, nll: nan\n","itr: 9900, nll: nan\n","itr: 9900, nll: nan\n","itr: 9900, nll: nan\n","itr: 9900, nll: nan\n","itr: 9900, nll: nan\n","itr: 9900, nll: nan\n","itr: 9900, nll: nan\n","itr: 9900, nll: nan\n","itr: 9900, nll: nan\n","itr: 9900, nll: nan\n","itr: 10200, nll: nan\n","itr: 10200, nll: nan\n","itr: 10200, nll: nan\n","itr: 10200, nll: nan\n","itr: 10200, nll: nan\n","itr: 10200, nll: nan\n","itr: 10200, nll: nan\n","itr: 10200, nll: nan\n","itr: 10200, nll: nan\n","itr: 10200, nll: nan\n","itr: 10500, nll: nan\n","itr: 10500, nll: nan\n","itr: 10500, nll: nan\n","itr: 10500, nll: nan\n","itr: 10500, nll: nan\n","itr: 10500, nll: nan\n","itr: 10500, nll: nan\n","itr: 10500, nll: nan\n","itr: 10500, nll: nan\n","itr: 10500, nll: nan\n","itr: 10800, nll: nan\n","itr: 10800, nll: nan\n","itr: 10800, nll: nan\n","itr: 10800, nll: nan\n","itr: 10800, nll: nan\n","itr: 10800, nll: nan\n","itr: 10800, nll: nan\n","itr: 10800, nll: nan\n","itr: 10800, nll: nan\n","itr: 10800, nll: nan\n","itr: 11100, nll: nan\n","itr: 11100, nll: nan\n","itr: 11100, nll: nan\n","itr: 11100, nll: nan\n","itr: 11100, nll: nan\n","itr: 11100, nll: nan\n","itr: 11100, nll: nan\n","itr: 11100, nll: nan\n","itr: 11100, nll: nan\n","itr: 11100, nll: nan\n","itr: 11400, nll: nan\n","itr: 11400, nll: nan\n","itr: 11400, nll: nan\n","itr: 11400, nll: nan\n","itr: 11400, nll: nan\n","itr: 11400, nll: nan\n","itr: 11400, nll: nan\n","itr: 11400, nll: nan\n","itr: 11400, nll: nan\n","itr: 11400, nll: nan\n","itr: 11700, nll: nan\n","itr: 11700, nll: nan\n","itr: 11700, nll: nan\n","itr: 11700, nll: nan\n","itr: 11700, nll: nan\n","itr: 11700, nll: nan\n","itr: 11700, nll: nan\n","itr: 11700, nll: nan\n","itr: 11700, nll: nan\n","itr: 11700, nll: nan\n","itr: 12000, nll: nan\n","itr: 12000, nll: nan\n","itr: 12000, nll: nan\n","itr: 12000, nll: nan\n","itr: 12000, nll: nan\n","itr: 12000, nll: nan\n","itr: 12000, nll: nan\n","itr: 12000, nll: nan\n","itr: 12000, nll: nan\n","itr: 12000, nll: nan\n","itr: 12300, nll: nan\n","itr: 12300, nll: nan\n","itr: 12300, nll: nan\n","itr: 12300, nll: nan\n","itr: 12300, nll: nan\n","itr: 12300, nll: nan\n","itr: 12300, nll: nan\n","itr: 12300, nll: nan\n","itr: 12300, nll: nan\n","itr: 12300, nll: nan\n","itr: 12600, nll: nan\n","itr: 12600, nll: nan\n","itr: 12600, nll: nan\n","itr: 12600, nll: nan\n","itr: 12600, nll: nan\n","itr: 12600, nll: nan\n","itr: 12600, nll: nan\n","itr: 12600, nll: nan\n","itr: 12600, nll: nan\n","itr: 12600, nll: nan\n","itr: 12900, nll: nan\n","itr: 12900, nll: nan\n","itr: 12900, nll: nan\n","itr: 12900, nll: nan\n","itr: 12900, nll: nan\n","itr: 12900, nll: nan\n","itr: 12900, nll: nan\n","itr: 12900, nll: nan\n","itr: 12900, nll: nan\n","itr: 12900, nll: nan\n","itr: 13200, nll: nan\n","itr: 13200, nll: nan\n","itr: 13200, nll: nan\n","itr: 13200, nll: nan\n","itr: 13200, nll: nan\n","itr: 13200, nll: nan\n","itr: 13200, nll: nan\n","itr: 13200, nll: nan\n","itr: 13200, nll: nan\n","itr: 13200, nll: nan\n","itr: 13500, nll: nan\n","itr: 13500, nll: nan\n","itr: 13500, nll: nan\n","itr: 13500, nll: nan\n","itr: 13500, nll: nan\n","itr: 13500, nll: nan\n","itr: 13500, nll: nan\n","itr: 13500, nll: nan\n","itr: 13500, nll: nan\n","itr: 13500, nll: nan\n","itr: 13800, nll: nan\n","itr: 13800, nll: nan\n","itr: 13800, nll: nan\n","itr: 13800, nll: nan\n","itr: 13800, nll: nan\n","itr: 13800, nll: nan\n","itr: 13800, nll: nan\n","itr: 13800, nll: nan\n","itr: 13800, nll: nan\n","itr: 13800, nll: nan\n","itr: 14100, nll: nan\n","itr: 14100, nll: nan\n","itr: 14100, nll: nan\n","itr: 14100, nll: nan\n","itr: 14100, nll: nan\n","itr: 14100, nll: nan\n","itr: 14100, nll: nan\n","itr: 14100, nll: nan\n","itr: 14100, nll: nan\n","itr: 14100, nll: nan\n","itr: 14400, nll: nan\n","itr: 14400, nll: nan\n","itr: 14400, nll: nan\n","itr: 14400, nll: nan\n","itr: 14400, nll: nan\n","itr: 14400, nll: nan\n","itr: 14400, nll: nan\n","itr: 14400, nll: nan\n","itr: 14400, nll: nan\n","itr: 14400, nll: nan\n","itr: 14700, nll: nan\n","itr: 14700, nll: nan\n","itr: 14700, nll: nan\n","itr: 14700, nll: nan\n","itr: 14700, nll: nan\n","itr: 14700, nll: nan\n","itr: 14700, nll: nan\n","itr: 14700, nll: nan\n","itr: 14700, nll: nan\n","itr: 14700, nll: nan\n","itr: 15000, nll: nan\n","itr: 15000, nll: nan\n","itr: 15000, nll: nan\n","itr: 15000, nll: nan\n","itr: 15000, nll: nan\n","itr: 15000, nll: nan\n","itr: 15000, nll: nan\n","itr: 15000, nll: nan\n","itr: 15000, nll: nan\n","itr: 15000, nll: nan\n","itr: 15300, nll: nan\n","itr: 15300, nll: nan\n","itr: 15300, nll: nan\n","itr: 15300, nll: nan\n","itr: 15300, nll: nan\n","itr: 15300, nll: nan\n","itr: 15300, nll: nan\n","itr: 15300, nll: nan\n","itr: 15300, nll: nan\n","itr: 15300, nll: nan\n","itr: 15600, nll: nan\n","itr: 15600, nll: nan\n","itr: 15600, nll: nan\n","itr: 15600, nll: nan\n","itr: 15600, nll: nan\n","itr: 15600, nll: nan\n","itr: 15600, nll: nan\n","itr: 15600, nll: nan\n","itr: 15600, nll: nan\n","itr: 15600, nll: nan\n","itr: 15900, nll: nan\n","itr: 15900, nll: nan\n","itr: 15900, nll: nan\n","itr: 15900, nll: nan\n","itr: 15900, nll: nan\n","itr: 15900, nll: nan\n","itr: 15900, nll: nan\n","itr: 15900, nll: nan\n","itr: 15900, nll: nan\n","itr: 15900, nll: nan\n","itr: 16200, nll: nan\n","itr: 16200, nll: nan\n","itr: 16200, nll: nan\n","itr: 16200, nll: nan\n","itr: 16200, nll: nan\n","itr: 16200, nll: nan\n","itr: 16200, nll: nan\n","itr: 16200, nll: nan\n","itr: 16200, nll: nan\n","itr: 16200, nll: nan\n","itr: 16500, nll: nan\n","itr: 16500, nll: nan\n","itr: 16500, nll: nan\n","itr: 16500, nll: nan\n","itr: 16500, nll: nan\n","itr: 16500, nll: nan\n","itr: 16500, nll: nan\n","itr: 16500, nll: nan\n","itr: 16500, nll: nan\n","itr: 16500, nll: nan\n","itr: 16800, nll: nan\n","itr: 16800, nll: nan\n","itr: 16800, nll: nan\n","itr: 16800, nll: nan\n","itr: 16800, nll: nan\n","itr: 16800, nll: nan\n","itr: 16800, nll: nan\n","itr: 16800, nll: nan\n","itr: 16800, nll: nan\n","itr: 16800, nll: nan\n","itr: 17100, nll: nan\n","itr: 17100, nll: nan\n","itr: 17100, nll: nan\n","itr: 17100, nll: nan\n","itr: 17100, nll: nan\n","itr: 17100, nll: nan\n","itr: 17100, nll: nan\n","itr: 17100, nll: nan\n","itr: 17100, nll: nan\n","itr: 17100, nll: nan\n","itr: 17400, nll: nan\n","itr: 17400, nll: nan\n","itr: 17400, nll: nan\n","itr: 17400, nll: nan\n","itr: 17400, nll: nan\n","itr: 17400, nll: nan\n","itr: 17400, nll: nan\n","itr: 17400, nll: nan\n","itr: 17400, nll: nan\n","itr: 17400, nll: nan\n","itr: 17700, nll: nan\n","itr: 17700, nll: nan\n","itr: 17700, nll: nan\n","itr: 17700, nll: nan\n","itr: 17700, nll: nan\n","itr: 17700, nll: nan\n","itr: 17700, nll: nan\n","itr: 17700, nll: nan\n","itr: 17700, nll: nan\n","itr: 17700, nll: nan\n","itr: 18000, nll: nan\n","itr: 18000, nll: nan\n","itr: 18000, nll: nan\n","itr: 18000, nll: nan\n","itr: 18000, nll: nan\n","itr: 18000, nll: nan\n","itr: 18000, nll: nan\n","itr: 18000, nll: nan\n","itr: 18000, nll: nan\n","itr: 18000, nll: nan\n","itr: 18300, nll: nan\n","itr: 18300, nll: nan\n","itr: 18300, nll: nan\n","itr: 18300, nll: nan\n","itr: 18300, nll: nan\n","itr: 18300, nll: nan\n","itr: 18300, nll: nan\n","itr: 18300, nll: nan\n","itr: 18300, nll: nan\n","itr: 18300, nll: nan\n","itr: 18600, nll: nan\n","itr: 18600, nll: nan\n","itr: 18600, nll: nan\n","itr: 18600, nll: nan\n","itr: 18600, nll: nan\n","itr: 18600, nll: nan\n","itr: 18600, nll: nan\n","itr: 18600, nll: nan\n","itr: 18600, nll: nan\n","itr: 18600, nll: nan\n","itr: 18900, nll: nan\n","itr: 18900, nll: nan\n","itr: 18900, nll: nan\n","itr: 18900, nll: nan\n","itr: 18900, nll: nan\n","itr: 18900, nll: nan\n","itr: 18900, nll: nan\n","itr: 18900, nll: nan\n","itr: 18900, nll: nan\n","itr: 18900, nll: nan\n","itr: 19200, nll: nan\n","itr: 19200, nll: nan\n","itr: 19200, nll: nan\n","itr: 19200, nll: nan\n","itr: 19200, nll: nan\n","itr: 19200, nll: nan\n","itr: 19200, nll: nan\n","itr: 19200, nll: nan\n","itr: 19200, nll: nan\n","itr: 19200, nll: nan\n","itr: 19500, nll: nan\n","itr: 19500, nll: nan\n","itr: 19500, nll: nan\n","itr: 19500, nll: nan\n","itr: 19500, nll: nan\n","itr: 19500, nll: nan\n","itr: 19500, nll: nan\n","itr: 19500, nll: nan\n","itr: 19500, nll: nan\n","itr: 19500, nll: nan\n","itr: 19800, nll: nan\n","itr: 19800, nll: nan\n","itr: 19800, nll: nan\n","itr: 19800, nll: nan\n","itr: 19800, nll: nan\n","itr: 19800, nll: nan\n","itr: 19800, nll: nan\n","itr: 19800, nll: nan\n","itr: 19800, nll: nan\n","itr: 19800, nll: nan\n","itr: 20100, nll: nan\n","itr: 20100, nll: nan\n","itr: 20100, nll: nan\n","itr: 20100, nll: nan\n","itr: 20100, nll: nan\n","itr: 20100, nll: nan\n","itr: 20100, nll: nan\n","itr: 20100, nll: nan\n","itr: 20100, nll: nan\n","itr: 20100, nll: nan\n","itr: 20400, nll: nan\n","itr: 20400, nll: nan\n","itr: 20400, nll: nan\n","itr: 20400, nll: nan\n","itr: 20400, nll: nan\n","itr: 20400, nll: nan\n","itr: 20400, nll: nan\n","itr: 20400, nll: nan\n","itr: 20400, nll: nan\n","itr: 20400, nll: nan\n","itr: 20700, nll: nan\n","itr: 20700, nll: nan\n","itr: 20700, nll: nan\n","itr: 20700, nll: nan\n","itr: 20700, nll: nan\n","itr: 20700, nll: nan\n","itr: 20700, nll: nan\n","itr: 20700, nll: nan\n","itr: 20700, nll: nan\n","itr: 20700, nll: nan\n","itr: 21000, nll: nan\n","itr: 21000, nll: nan\n","itr: 21000, nll: nan\n","itr: 21000, nll: nan\n","itr: 21000, nll: nan\n","itr: 21000, nll: nan\n","itr: 21000, nll: nan\n","itr: 21000, nll: nan\n","itr: 21000, nll: nan\n","itr: 21000, nll: nan\n","itr: 21300, nll: nan\n","itr: 21300, nll: nan\n","itr: 21300, nll: nan\n","itr: 21300, nll: nan\n","itr: 21300, nll: nan\n","itr: 21300, nll: nan\n","itr: 21300, nll: nan\n","itr: 21300, nll: nan\n","itr: 21300, nll: nan\n","itr: 21300, nll: nan\n","itr: 21600, nll: nan\n","itr: 21600, nll: nan\n","itr: 21600, nll: nan\n","itr: 21600, nll: nan\n","itr: 21600, nll: nan\n","itr: 21600, nll: nan\n","itr: 21600, nll: nan\n","itr: 21600, nll: nan\n","itr: 21600, nll: nan\n","itr: 21600, nll: nan\n","itr: 21900, nll: nan\n","itr: 21900, nll: nan\n","itr: 21900, nll: nan\n","itr: 21900, nll: nan\n","itr: 21900, nll: nan\n","itr: 21900, nll: nan\n","itr: 21900, nll: nan\n","itr: 21900, nll: nan\n","itr: 21900, nll: nan\n","itr: 21900, nll: nan\n","itr: 22200, nll: nan\n","itr: 22200, nll: nan\n","itr: 22200, nll: nan\n","itr: 22200, nll: nan\n","itr: 22200, nll: nan\n","itr: 22200, nll: nan\n","itr: 22200, nll: nan\n","itr: 22200, nll: nan\n","itr: 22200, nll: nan\n","itr: 22200, nll: nan\n","itr: 22500, nll: nan\n","itr: 22500, nll: nan\n","itr: 22500, nll: nan\n","itr: 22500, nll: nan\n","itr: 22500, nll: nan\n","itr: 22500, nll: nan\n","itr: 22500, nll: nan\n","itr: 22500, nll: nan\n","itr: 22500, nll: nan\n","itr: 22500, nll: nan\n","itr: 22800, nll: nan\n","itr: 22800, nll: nan\n","itr: 22800, nll: nan\n","itr: 22800, nll: nan\n","itr: 22800, nll: nan\n","itr: 22800, nll: nan\n","itr: 22800, nll: nan\n","itr: 22800, nll: nan\n","itr: 22800, nll: nan\n","itr: 22800, nll: nan\n","itr: 23100, nll: nan\n","itr: 23100, nll: nan\n","itr: 23100, nll: nan\n","itr: 23100, nll: nan\n","itr: 23100, nll: nan\n","itr: 23100, nll: nan\n","itr: 23100, nll: nan\n","itr: 23100, nll: nan\n","itr: 23100, nll: nan\n","itr: 23100, nll: nan\n","itr: 23400, nll: nan\n","itr: 23400, nll: nan\n","itr: 23400, nll: nan\n","itr: 23400, nll: nan\n","itr: 23400, nll: nan\n","itr: 23400, nll: nan\n","itr: 23400, nll: nan\n","itr: 23400, nll: nan\n","itr: 23400, nll: nan\n","itr: 23400, nll: nan\n","itr: 23700, nll: nan\n","itr: 23700, nll: nan\n","itr: 23700, nll: nan\n","itr: 23700, nll: nan\n","itr: 23700, nll: nan\n","itr: 23700, nll: nan\n","itr: 23700, nll: nan\n","itr: 23700, nll: nan\n","itr: 23700, nll: nan\n","itr: 23700, nll: nan\n","itr: 24000, nll: nan\n","itr: 24000, nll: nan\n","itr: 24000, nll: nan\n","itr: 24000, nll: nan\n","itr: 24000, nll: nan\n","itr: 24000, nll: nan\n","itr: 24000, nll: nan\n","itr: 24000, nll: nan\n","itr: 24000, nll: nan\n","itr: 24000, nll: nan\n","itr: 24300, nll: nan\n","itr: 24300, nll: nan\n","itr: 24300, nll: nan\n","itr: 24300, nll: nan\n","itr: 24300, nll: nan\n","itr: 24300, nll: nan\n","itr: 24300, nll: nan\n","itr: 24300, nll: nan\n","itr: 24300, nll: nan\n","itr: 24300, nll: nan\n","itr: 24600, nll: nan\n","itr: 24600, nll: nan\n","itr: 24600, nll: nan\n","itr: 24600, nll: nan\n","itr: 24600, nll: nan\n","itr: 24600, nll: nan\n","itr: 24600, nll: nan\n","itr: 24600, nll: nan\n","itr: 24600, nll: nan\n","itr: 24600, nll: nan\n","itr: 24900, nll: nan\n","itr: 24900, nll: nan\n","itr: 24900, nll: nan\n","itr: 24900, nll: nan\n","itr: 24900, nll: nan\n","itr: 24900, nll: nan\n","itr: 24900, nll: nan\n","itr: 24900, nll: nan\n","itr: 24900, nll: nan\n","itr: 24900, nll: nan\n","itr: 25200, nll: nan\n","itr: 25200, nll: nan\n","itr: 25200, nll: nan\n","itr: 25200, nll: nan\n","itr: 25200, nll: nan\n","itr: 25200, nll: nan\n","itr: 25200, nll: nan\n","itr: 25200, nll: nan\n","itr: 25200, nll: nan\n","itr: 25200, nll: nan\n","itr: 25500, nll: nan\n","itr: 25500, nll: nan\n","itr: 25500, nll: nan\n","itr: 25500, nll: nan\n","itr: 25500, nll: nan\n","itr: 25500, nll: nan\n","itr: 25500, nll: nan\n","itr: 25500, nll: nan\n","itr: 25500, nll: nan\n","itr: 25500, nll: nan\n","itr: 25800, nll: nan\n","itr: 25800, nll: nan\n","itr: 25800, nll: nan\n","itr: 25800, nll: nan\n","itr: 25800, nll: nan\n","itr: 25800, nll: nan\n","itr: 25800, nll: nan\n","itr: 25800, nll: nan\n","itr: 25800, nll: nan\n","itr: 25800, nll: nan\n","itr: 26100, nll: nan\n","itr: 26100, nll: nan\n","itr: 26100, nll: nan\n","itr: 26100, nll: nan\n","itr: 26100, nll: nan\n","itr: 26100, nll: nan\n","itr: 26100, nll: nan\n","itr: 26100, nll: nan\n","itr: 26100, nll: nan\n","itr: 26100, nll: nan\n","itr: 26400, nll: nan\n","itr: 26400, nll: nan\n","itr: 26400, nll: nan\n","itr: 26400, nll: nan\n","itr: 26400, nll: nan\n","itr: 26400, nll: nan\n","itr: 26400, nll: nan\n","itr: 26400, nll: nan\n","itr: 26400, nll: nan\n","itr: 26400, nll: nan\n","itr: 26700, nll: nan\n","itr: 26700, nll: nan\n","itr: 26700, nll: nan\n","itr: 26700, nll: nan\n","itr: 26700, nll: nan\n","itr: 26700, nll: nan\n","itr: 26700, nll: nan\n","itr: 26700, nll: nan\n","itr: 26700, nll: nan\n","itr: 26700, nll: nan\n","itr: 27000, nll: nan\n","itr: 27000, nll: nan\n","itr: 27000, nll: nan\n","itr: 27000, nll: nan\n","itr: 27000, nll: nan\n","itr: 27000, nll: nan\n","itr: 27000, nll: nan\n","itr: 27000, nll: nan\n","itr: 27000, nll: nan\n","itr: 27000, nll: nan\n","itr: 27300, nll: nan\n","itr: 27300, nll: nan\n","itr: 27300, nll: nan\n","itr: 27300, nll: nan\n","itr: 27300, nll: nan\n","itr: 27300, nll: nan\n","itr: 27300, nll: nan\n","itr: 27300, nll: nan\n","itr: 27300, nll: nan\n","itr: 27300, nll: nan\n","itr: 27600, nll: nan\n","itr: 27600, nll: nan\n","itr: 27600, nll: nan\n","itr: 27600, nll: nan\n","itr: 27600, nll: nan\n","itr: 27600, nll: nan\n","itr: 27600, nll: nan\n","itr: 27600, nll: nan\n","itr: 27600, nll: nan\n","itr: 27600, nll: nan\n","itr: 27900, nll: nan\n","itr: 27900, nll: nan\n","itr: 27900, nll: nan\n","itr: 27900, nll: nan\n","itr: 27900, nll: nan\n","itr: 27900, nll: nan\n","itr: 27900, nll: nan\n","itr: 27900, nll: nan\n","itr: 27900, nll: nan\n","itr: 27900, nll: nan\n","itr: 28200, nll: nan\n","itr: 28200, nll: nan\n","itr: 28200, nll: nan\n","itr: 28200, nll: nan\n","itr: 28200, nll: nan\n","itr: 28200, nll: nan\n","itr: 28200, nll: nan\n","itr: 28200, nll: nan\n","itr: 28200, nll: nan\n","itr: 28200, nll: nan\n","itr: 28500, nll: nan\n","itr: 28500, nll: nan\n","itr: 28500, nll: nan\n","itr: 28500, nll: nan\n","itr: 28500, nll: nan\n","itr: 28500, nll: nan\n","itr: 28500, nll: nan\n","itr: 28500, nll: nan\n","itr: 28500, nll: nan\n","itr: 28500, nll: nan\n","itr: 28800, nll: nan\n","itr: 28800, nll: nan\n","itr: 28800, nll: nan\n","itr: 28800, nll: nan\n","itr: 28800, nll: nan\n","itr: 28800, nll: nan\n","itr: 28800, nll: nan\n","itr: 28800, nll: nan\n","itr: 28800, nll: nan\n","itr: 28800, nll: nan\n","itr: 29100, nll: nan\n","itr: 29100, nll: nan\n","itr: 29100, nll: nan\n","itr: 29100, nll: nan\n","itr: 29100, nll: nan\n","itr: 29100, nll: nan\n","itr: 29100, nll: nan\n","itr: 29100, nll: nan\n","itr: 29100, nll: nan\n","itr: 29100, nll: nan\n","itr: 29400, nll: nan\n","itr: 29400, nll: nan\n","itr: 29400, nll: nan\n","itr: 29400, nll: nan\n","itr: 29400, nll: nan\n","itr: 29400, nll: nan\n","itr: 29400, nll: nan\n","itr: 29400, nll: nan\n","itr: 29400, nll: nan\n","itr: 29400, nll: nan\n","itr: 29700, nll: nan\n","itr: 29700, nll: nan\n","itr: 29700, nll: nan\n","itr: 29700, nll: nan\n","itr: 29700, nll: nan\n","itr: 29700, nll: nan\n","itr: 29700, nll: nan\n","itr: 29700, nll: nan\n","itr: 29700, nll: nan\n","itr: 29700, nll: nan\n","min_val is : 0.0\n","max_val is : 25.91068069062784\n","Traceback (most recent call last):\n","  File \"/content/vldb2021_fauce/models_training/train.py\", line 256, in <module>\n","    main()\n","  File \"/content/vldb2021_fauce/models_training/train.py\", line 69, in main\n","    train_ensemble(args)\n","  File \"/content/vldb2021_fauce/models_training/train.py\", line 157, in train_ensemble\n","    test_ensemble(args, ensemble, sess, dataLoader)\n","  File \"/content/vldb2021_fauce/models_training/train.py\", line 166, in test_ensemble\n","    mean, var = ensemble_mean_var(ensemble, test_xs, sess)\n","  File \"/content/vldb2021_fauce/models_training/train.py\", line 93, in ensemble_mean_var\n","    en_mean /= total\n","ZeroDivisionError: division by zero\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"3uTAIf3WLc4_","outputId":"1dd3ac76-ed2b-45b3-f5b4-2de1afd1dcd8"},"outputs":[{"name":"stdout","output_type":"stream","text":["2024-04-19 19:38:01.555090: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-19 19:38:01.555158: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-19 19:38:01.556657: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-04-19 19:38:01.566579: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-04-19 19:38:02.679773: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","WARNING:tensorflow:From /root/.local/share/virtualenvs/vldb2021_fauce-rT6f7ILy/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:108: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n","15309\n","30\n","The shape of the data_x is: (15309, 29)\n","[[0.51191605]\n"," [1.68930066]\n"," [1.40568387]\n"," ...\n"," [1.10623218]\n"," [0.6616419 ]\n"," [1.06732577]]\n","self.min_val is: 0.0\n","self.max_val is: 5.115495726\n","The shape of the train_data_x is: (13778, 29)\n","The var_values is: Tensor(\"mul_1:0\", shape=(?, 1), dtype=float32)\n","WARNING:tensorflow:From /root/.local/share/virtualenvs/vldb2021_fauce-rT6f7ILy/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1260: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n","self.nll_gradients is: Tensor(\"gradients/MatMul_grad/MatMul:0\", shape=(?, 29), dtype=float32)\n","The var_values is: Tensor(\"mul_8:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(29, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","WARNING:tensorflow:From /root/.local/share/virtualenvs/vldb2021_fauce-rT6f7ILy/lib/python3.10/site-packages/tensorflow/python/training/rmsprop.py:188: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","The var_values is: Tensor(\"mul_15:0\", shape=(?, 1), dtype=float32)\n","self.nll_gradients is: Tensor(\"gradients_2/MatMul_10_grad/MatMul:0\", shape=(?, 29), dtype=float32)\n","The var_values is: Tensor(\"mul_22:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(29, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","model1MLP/weights_0:0\n","(29, 128)\n","model1MLP/biases_0:0\n","(128,)\n","model1MLP/weights_1:0\n","(128, 256)\n","model1MLP/biases_1:0\n","(256,)\n","model1MLP/weights_2:0\n","(256, 512)\n","model1MLP/biases_2:0\n","(512,)\n","model1MLP/weights_3:0\n","(512, 512)\n","model1MLP/biases_3:0\n","(512,)\n","model1MLP/weights_4:0\n","(512, 2)\n","model1MLP/biases_4:0\n","(2,)\n","The var_values is: Tensor(\"mul_29:0\", shape=(?, 1), dtype=float32)\n","self.nll_gradients is: Tensor(\"gradients_4/MatMul_20_grad/MatMul:0\", shape=(?, 29), dtype=float32)\n","The var_values is: Tensor(\"mul_36:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(29, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","model1MLP/weights_0:0\n","(29, 128)\n","model1MLP/biases_0:0\n","(128,)\n","model1MLP/weights_1:0\n","(128, 256)\n","model1MLP/biases_1:0\n","(256,)\n","model1MLP/weights_2:0\n","(256, 512)\n","model1MLP/biases_2:0\n","(512,)\n","model1MLP/weights_3:0\n","(512, 512)\n","model1MLP/biases_3:0\n","(512,)\n","model1MLP/weights_4:0\n","(512, 2)\n","model1MLP/biases_4:0\n","(2,)\n","model2MLP/weights_0:0\n","(29, 128)\n","model2MLP/biases_0:0\n","(128,)\n","model2MLP/weights_1:0\n","(128, 256)\n","model2MLP/biases_1:0\n","(256,)\n","model2MLP/weights_2:0\n","(256, 512)\n","model2MLP/biases_2:0\n","(512,)\n","model2MLP/weights_3:0\n","(512, 512)\n","model2MLP/biases_3:0\n","(512,)\n","model2MLP/weights_4:0\n","(512, 2)\n","model2MLP/biases_4:0\n","(2,)\n","The var_values is: Tensor(\"mul_43:0\", shape=(?, 1), dtype=float32)\n","self.nll_gradients is: Tensor(\"gradients_6/MatMul_30_grad/MatMul:0\", shape=(?, 29), dtype=float32)\n","The var_values is: Tensor(\"mul_50:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(29, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","model1MLP/weights_0:0\n","(29, 128)\n","model1MLP/biases_0:0\n","(128,)\n","model1MLP/weights_1:0\n","(128, 256)\n","model1MLP/biases_1:0\n","(256,)\n","model1MLP/weights_2:0\n","(256, 512)\n","model1MLP/biases_2:0\n","(512,)\n","model1MLP/weights_3:0\n","(512, 512)\n","model1MLP/biases_3:0\n","(512,)\n","model1MLP/weights_4:0\n","(512, 2)\n","model1MLP/biases_4:0\n","(2,)\n","model2MLP/weights_0:0\n","(29, 128)\n","model2MLP/biases_0:0\n","(128,)\n","model2MLP/weights_1:0\n","(128, 256)\n","model2MLP/biases_1:0\n","(256,)\n","model2MLP/weights_2:0\n","(256, 512)\n","model2MLP/biases_2:0\n","(512,)\n","model2MLP/weights_3:0\n","(512, 512)\n","model2MLP/biases_3:0\n","(512,)\n","model2MLP/weights_4:0\n","(512, 2)\n","model2MLP/biases_4:0\n","(2,)\n","model3MLP/weights_0:0\n","(29, 128)\n","model3MLP/biases_0:0\n","(128,)\n","model3MLP/weights_1:0\n","(128, 256)\n","model3MLP/biases_1:0\n","(256,)\n","model3MLP/weights_2:0\n","(256, 512)\n","model3MLP/biases_2:0\n","(512,)\n","model3MLP/weights_3:0\n","(512, 512)\n","model3MLP/biases_3:0\n","(512,)\n","model3MLP/weights_4:0\n","(512, 2)\n","model3MLP/biases_4:0\n","(2,)\n","The var_values is: Tensor(\"mul_57:0\", shape=(?, 1), dtype=float32)\n","self.nll_gradients is: Tensor(\"gradients_8/MatMul_40_grad/MatMul:0\", shape=(?, 29), dtype=float32)\n","The var_values is: Tensor(\"mul_64:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(29, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","model1MLP/weights_0:0\n","(29, 128)\n","model1MLP/biases_0:0\n","(128,)\n","model1MLP/weights_1:0\n","(128, 256)\n","model1MLP/biases_1:0\n","(256,)\n","model1MLP/weights_2:0\n","(256, 512)\n","model1MLP/biases_2:0\n","(512,)\n","model1MLP/weights_3:0\n","(512, 512)\n","model1MLP/biases_3:0\n","(512,)\n","model1MLP/weights_4:0\n","(512, 2)\n","model1MLP/biases_4:0\n","(2,)\n","model2MLP/weights_0:0\n","(29, 128)\n","model2MLP/biases_0:0\n","(128,)\n","model2MLP/weights_1:0\n","(128, 256)\n","model2MLP/biases_1:0\n","(256,)\n","model2MLP/weights_2:0\n","(256, 512)\n","model2MLP/biases_2:0\n","(512,)\n","model2MLP/weights_3:0\n","(512, 512)\n","model2MLP/biases_3:0\n","(512,)\n","model2MLP/weights_4:0\n","(512, 2)\n","model2MLP/biases_4:0\n","(2,)\n","model3MLP/weights_0:0\n","(29, 128)\n","model3MLP/biases_0:0\n","(128,)\n","model3MLP/weights_1:0\n","(128, 256)\n","model3MLP/biases_1:0\n","(256,)\n","model3MLP/weights_2:0\n","(256, 512)\n","model3MLP/biases_2:0\n","(512,)\n","model3MLP/weights_3:0\n","(512, 512)\n","model3MLP/biases_3:0\n","(512,)\n","model3MLP/weights_4:0\n","(512, 2)\n","model3MLP/biases_4:0\n","(2,)\n","model4MLP/weights_0:0\n","(29, 128)\n","model4MLP/biases_0:0\n","(128,)\n","model4MLP/weights_1:0\n","(128, 256)\n","model4MLP/biases_1:0\n","(256,)\n","model4MLP/weights_2:0\n","(256, 512)\n","model4MLP/biases_2:0\n","(512,)\n","model4MLP/weights_3:0\n","(512, 512)\n","model4MLP/biases_3:0\n","(512,)\n","model4MLP/weights_4:0\n","(512, 2)\n","model4MLP/biases_4:0\n","(2,)\n","The var_values is: Tensor(\"mul_71:0\", shape=(?, 1), dtype=float32)\n","self.nll_gradients is: Tensor(\"gradients_10/MatMul_50_grad/MatMul:0\", shape=(?, 29), dtype=float32)\n","The var_values is: Tensor(\"mul_78:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(29, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","model1MLP/weights_0:0\n","(29, 128)\n","model1MLP/biases_0:0\n","(128,)\n","model1MLP/weights_1:0\n","(128, 256)\n","model1MLP/biases_1:0\n","(256,)\n","model1MLP/weights_2:0\n","(256, 512)\n","model1MLP/biases_2:0\n","(512,)\n","model1MLP/weights_3:0\n","(512, 512)\n","model1MLP/biases_3:0\n","(512,)\n","model1MLP/weights_4:0\n","(512, 2)\n","model1MLP/biases_4:0\n","(2,)\n","model2MLP/weights_0:0\n","(29, 128)\n","model2MLP/biases_0:0\n","(128,)\n","model2MLP/weights_1:0\n","(128, 256)\n","model2MLP/biases_1:0\n","(256,)\n","model2MLP/weights_2:0\n","(256, 512)\n","model2MLP/biases_2:0\n","(512,)\n","model2MLP/weights_3:0\n","(512, 512)\n","model2MLP/biases_3:0\n","(512,)\n","model2MLP/weights_4:0\n","(512, 2)\n","model2MLP/biases_4:0\n","(2,)\n","model3MLP/weights_0:0\n","(29, 128)\n","model3MLP/biases_0:0\n","(128,)\n","model3MLP/weights_1:0\n","(128, 256)\n","model3MLP/biases_1:0\n","(256,)\n","model3MLP/weights_2:0\n","(256, 512)\n","model3MLP/biases_2:0\n","(512,)\n","model3MLP/weights_3:0\n","(512, 512)\n","model3MLP/biases_3:0\n","(512,)\n","model3MLP/weights_4:0\n","(512, 2)\n","model3MLP/biases_4:0\n","(2,)\n","model4MLP/weights_0:0\n","(29, 128)\n","model4MLP/biases_0:0\n","(128,)\n","model4MLP/weights_1:0\n","(128, 256)\n","model4MLP/biases_1:0\n","(256,)\n","model4MLP/weights_2:0\n","(256, 512)\n","model4MLP/biases_2:0\n","(512,)\n","model4MLP/weights_3:0\n","(512, 512)\n","model4MLP/biases_3:0\n","(512,)\n","model4MLP/weights_4:0\n","(512, 2)\n","model4MLP/biases_4:0\n","(2,)\n","model5MLP/weights_0:0\n","(29, 128)\n","model5MLP/biases_0:0\n","(128,)\n","model5MLP/weights_1:0\n","(128, 256)\n","model5MLP/biases_1:0\n","(256,)\n","model5MLP/weights_2:0\n","(256, 512)\n","model5MLP/biases_2:0\n","(512,)\n","model5MLP/weights_3:0\n","(512, 512)\n","model5MLP/biases_3:0\n","(512,)\n","model5MLP/weights_4:0\n","(512, 2)\n","model5MLP/biases_4:0\n","(2,)\n","The var_values is: Tensor(\"mul_85:0\", shape=(?, 1), dtype=float32)\n","self.nll_gradients is: Tensor(\"gradients_12/MatMul_60_grad/MatMul:0\", shape=(?, 29), dtype=float32)\n","The var_values is: Tensor(\"mul_92:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(29, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","model1MLP/weights_0:0\n","(29, 128)\n","model1MLP/biases_0:0\n","(128,)\n","model1MLP/weights_1:0\n","(128, 256)\n","model1MLP/biases_1:0\n","(256,)\n","model1MLP/weights_2:0\n","(256, 512)\n","model1MLP/biases_2:0\n","(512,)\n","model1MLP/weights_3:0\n","(512, 512)\n","model1MLP/biases_3:0\n","(512,)\n","model1MLP/weights_4:0\n","(512, 2)\n","model1MLP/biases_4:0\n","(2,)\n","model2MLP/weights_0:0\n","(29, 128)\n","model2MLP/biases_0:0\n","(128,)\n","model2MLP/weights_1:0\n","(128, 256)\n","model2MLP/biases_1:0\n","(256,)\n","model2MLP/weights_2:0\n","(256, 512)\n","model2MLP/biases_2:0\n","(512,)\n","model2MLP/weights_3:0\n","(512, 512)\n","model2MLP/biases_3:0\n","(512,)\n","model2MLP/weights_4:0\n","(512, 2)\n","model2MLP/biases_4:0\n","(2,)\n","model3MLP/weights_0:0\n","(29, 128)\n","model3MLP/biases_0:0\n","(128,)\n","model3MLP/weights_1:0\n","(128, 256)\n","model3MLP/biases_1:0\n","(256,)\n","model3MLP/weights_2:0\n","(256, 512)\n","model3MLP/biases_2:0\n","(512,)\n","model3MLP/weights_3:0\n","(512, 512)\n","model3MLP/biases_3:0\n","(512,)\n","model3MLP/weights_4:0\n","(512, 2)\n","model3MLP/biases_4:0\n","(2,)\n","model4MLP/weights_0:0\n","(29, 128)\n","model4MLP/biases_0:0\n","(128,)\n","model4MLP/weights_1:0\n","(128, 256)\n","model4MLP/biases_1:0\n","(256,)\n","model4MLP/weights_2:0\n","(256, 512)\n","model4MLP/biases_2:0\n","(512,)\n","model4MLP/weights_3:0\n","(512, 512)\n","model4MLP/biases_3:0\n","(512,)\n","model4MLP/weights_4:0\n","(512, 2)\n","model4MLP/biases_4:0\n","(2,)\n","model5MLP/weights_0:0\n","(29, 128)\n","model5MLP/biases_0:0\n","(128,)\n","model5MLP/weights_1:0\n","(128, 256)\n","model5MLP/biases_1:0\n","(256,)\n","model5MLP/weights_2:0\n","(256, 512)\n","model5MLP/biases_2:0\n","(512,)\n","model5MLP/weights_3:0\n","(512, 512)\n","model5MLP/biases_3:0\n","(512,)\n","model5MLP/weights_4:0\n","(512, 2)\n","model5MLP/biases_4:0\n","(2,)\n","model6MLP/weights_0:0\n","(29, 128)\n","model6MLP/biases_0:0\n","(128,)\n","model6MLP/weights_1:0\n","(128, 256)\n","model6MLP/biases_1:0\n","(256,)\n","model6MLP/weights_2:0\n","(256, 512)\n","model6MLP/biases_2:0\n","(512,)\n","model6MLP/weights_3:0\n","(512, 512)\n","model6MLP/biases_3:0\n","(512,)\n","model6MLP/weights_4:0\n","(512, 2)\n","model6MLP/biases_4:0\n","(2,)\n","The var_values is: Tensor(\"mul_99:0\", shape=(?, 1), dtype=float32)\n","self.nll_gradients is: Tensor(\"gradients_14/MatMul_70_grad/MatMul:0\", shape=(?, 29), dtype=float32)\n","The var_values is: Tensor(\"mul_106:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(29, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","model1MLP/weights_0:0\n","(29, 128)\n","model1MLP/biases_0:0\n","(128,)\n","model1MLP/weights_1:0\n","(128, 256)\n","model1MLP/biases_1:0\n","(256,)\n","model1MLP/weights_2:0\n","(256, 512)\n","model1MLP/biases_2:0\n","(512,)\n","model1MLP/weights_3:0\n","(512, 512)\n","model1MLP/biases_3:0\n","(512,)\n","model1MLP/weights_4:0\n","(512, 2)\n","model1MLP/biases_4:0\n","(2,)\n","model2MLP/weights_0:0\n","(29, 128)\n","model2MLP/biases_0:0\n","(128,)\n","model2MLP/weights_1:0\n","(128, 256)\n","model2MLP/biases_1:0\n","(256,)\n","model2MLP/weights_2:0\n","(256, 512)\n","model2MLP/biases_2:0\n","(512,)\n","model2MLP/weights_3:0\n","(512, 512)\n","model2MLP/biases_3:0\n","(512,)\n","model2MLP/weights_4:0\n","(512, 2)\n","model2MLP/biases_4:0\n","(2,)\n","model3MLP/weights_0:0\n","(29, 128)\n","model3MLP/biases_0:0\n","(128,)\n","model3MLP/weights_1:0\n","(128, 256)\n","model3MLP/biases_1:0\n","(256,)\n","model3MLP/weights_2:0\n","(256, 512)\n","model3MLP/biases_2:0\n","(512,)\n","model3MLP/weights_3:0\n","(512, 512)\n","model3MLP/biases_3:0\n","(512,)\n","model3MLP/weights_4:0\n","(512, 2)\n","model3MLP/biases_4:0\n","(2,)\n","model4MLP/weights_0:0\n","(29, 128)\n","model4MLP/biases_0:0\n","(128,)\n","model4MLP/weights_1:0\n","(128, 256)\n","model4MLP/biases_1:0\n","(256,)\n","model4MLP/weights_2:0\n","(256, 512)\n","model4MLP/biases_2:0\n","(512,)\n","model4MLP/weights_3:0\n","(512, 512)\n","model4MLP/biases_3:0\n","(512,)\n","model4MLP/weights_4:0\n","(512, 2)\n","model4MLP/biases_4:0\n","(2,)\n","model5MLP/weights_0:0\n","(29, 128)\n","model5MLP/biases_0:0\n","(128,)\n","model5MLP/weights_1:0\n","(128, 256)\n","model5MLP/biases_1:0\n","(256,)\n","model5MLP/weights_2:0\n","(256, 512)\n","model5MLP/biases_2:0\n","(512,)\n","model5MLP/weights_3:0\n","(512, 512)\n","model5MLP/biases_3:0\n","(512,)\n","model5MLP/weights_4:0\n","(512, 2)\n","model5MLP/biases_4:0\n","(2,)\n","model6MLP/weights_0:0\n","(29, 128)\n","model6MLP/biases_0:0\n","(128,)\n","model6MLP/weights_1:0\n","(128, 256)\n","model6MLP/biases_1:0\n","(256,)\n","model6MLP/weights_2:0\n","(256, 512)\n","model6MLP/biases_2:0\n","(512,)\n","model6MLP/weights_3:0\n","(512, 512)\n","model6MLP/biases_3:0\n","(512,)\n","model6MLP/weights_4:0\n","(512, 2)\n","model6MLP/biases_4:0\n","(2,)\n","model7MLP/weights_0:0\n","(29, 128)\n","model7MLP/biases_0:0\n","(128,)\n","model7MLP/weights_1:0\n","(128, 256)\n","model7MLP/biases_1:0\n","(256,)\n","model7MLP/weights_2:0\n","(256, 512)\n","model7MLP/biases_2:0\n","(512,)\n","model7MLP/weights_3:0\n","(512, 512)\n","model7MLP/biases_3:0\n","(512,)\n","model7MLP/weights_4:0\n","(512, 2)\n","model7MLP/biases_4:0\n","(2,)\n","The var_values is: Tensor(\"mul_113:0\", shape=(?, 1), dtype=float32)\n","self.nll_gradients is: Tensor(\"gradients_16/MatMul_80_grad/MatMul:0\", shape=(?, 29), dtype=float32)\n","The var_values is: Tensor(\"mul_120:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(29, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","model1MLP/weights_0:0\n","(29, 128)\n","model1MLP/biases_0:0\n","(128,)\n","model1MLP/weights_1:0\n","(128, 256)\n","model1MLP/biases_1:0\n","(256,)\n","model1MLP/weights_2:0\n","(256, 512)\n","model1MLP/biases_2:0\n","(512,)\n","model1MLP/weights_3:0\n","(512, 512)\n","model1MLP/biases_3:0\n","(512,)\n","model1MLP/weights_4:0\n","(512, 2)\n","model1MLP/biases_4:0\n","(2,)\n","model2MLP/weights_0:0\n","(29, 128)\n","model2MLP/biases_0:0\n","(128,)\n","model2MLP/weights_1:0\n","(128, 256)\n","model2MLP/biases_1:0\n","(256,)\n","model2MLP/weights_2:0\n","(256, 512)\n","model2MLP/biases_2:0\n","(512,)\n","model2MLP/weights_3:0\n","(512, 512)\n","model2MLP/biases_3:0\n","(512,)\n","model2MLP/weights_4:0\n","(512, 2)\n","model2MLP/biases_4:0\n","(2,)\n","model3MLP/weights_0:0\n","(29, 128)\n","model3MLP/biases_0:0\n","(128,)\n","model3MLP/weights_1:0\n","(128, 256)\n","model3MLP/biases_1:0\n","(256,)\n","model3MLP/weights_2:0\n","(256, 512)\n","model3MLP/biases_2:0\n","(512,)\n","model3MLP/weights_3:0\n","(512, 512)\n","model3MLP/biases_3:0\n","(512,)\n","model3MLP/weights_4:0\n","(512, 2)\n","model3MLP/biases_4:0\n","(2,)\n","model4MLP/weights_0:0\n","(29, 128)\n","model4MLP/biases_0:0\n","(128,)\n","model4MLP/weights_1:0\n","(128, 256)\n","model4MLP/biases_1:0\n","(256,)\n","model4MLP/weights_2:0\n","(256, 512)\n","model4MLP/biases_2:0\n","(512,)\n","model4MLP/weights_3:0\n","(512, 512)\n","model4MLP/biases_3:0\n","(512,)\n","model4MLP/weights_4:0\n","(512, 2)\n","model4MLP/biases_4:0\n","(2,)\n","model5MLP/weights_0:0\n","(29, 128)\n","model5MLP/biases_0:0\n","(128,)\n","model5MLP/weights_1:0\n","(128, 256)\n","model5MLP/biases_1:0\n","(256,)\n","model5MLP/weights_2:0\n","(256, 512)\n","model5MLP/biases_2:0\n","(512,)\n","model5MLP/weights_3:0\n","(512, 512)\n","model5MLP/biases_3:0\n","(512,)\n","model5MLP/weights_4:0\n","(512, 2)\n","model5MLP/biases_4:0\n","(2,)\n","model6MLP/weights_0:0\n","(29, 128)\n","model6MLP/biases_0:0\n","(128,)\n","model6MLP/weights_1:0\n","(128, 256)\n","model6MLP/biases_1:0\n","(256,)\n","model6MLP/weights_2:0\n","(256, 512)\n","model6MLP/biases_2:0\n","(512,)\n","model6MLP/weights_3:0\n","(512, 512)\n","model6MLP/biases_3:0\n","(512,)\n","model6MLP/weights_4:0\n","(512, 2)\n","model6MLP/biases_4:0\n","(2,)\n","model7MLP/weights_0:0\n","(29, 128)\n","model7MLP/biases_0:0\n","(128,)\n","model7MLP/weights_1:0\n","(128, 256)\n","model7MLP/biases_1:0\n","(256,)\n","model7MLP/weights_2:0\n","(256, 512)\n","model7MLP/biases_2:0\n","(512,)\n","model7MLP/weights_3:0\n","(512, 512)\n","model7MLP/biases_3:0\n","(512,)\n","model7MLP/weights_4:0\n","(512, 2)\n","model7MLP/biases_4:0\n","(2,)\n","model8MLP/weights_0:0\n","(29, 128)\n","model8MLP/biases_0:0\n","(128,)\n","model8MLP/weights_1:0\n","(128, 256)\n","model8MLP/biases_1:0\n","(256,)\n","model8MLP/weights_2:0\n","(256, 512)\n","model8MLP/biases_2:0\n","(512,)\n","model8MLP/weights_3:0\n","(512, 512)\n","model8MLP/biases_3:0\n","(512,)\n","model8MLP/weights_4:0\n","(512, 2)\n","model8MLP/biases_4:0\n","(2,)\n","The var_values is: Tensor(\"mul_127:0\", shape=(?, 1), dtype=float32)\n","self.nll_gradients is: Tensor(\"gradients_18/MatMul_90_grad/MatMul:0\", shape=(?, 29), dtype=float32)\n","The var_values is: Tensor(\"mul_134:0\", shape=(?, 1), dtype=float32)\n","model0MLP/weights_0:0\n","(29, 128)\n","model0MLP/biases_0:0\n","(128,)\n","model0MLP/weights_1:0\n","(128, 256)\n","model0MLP/biases_1:0\n","(256,)\n","model0MLP/weights_2:0\n","(256, 512)\n","model0MLP/biases_2:0\n","(512,)\n","model0MLP/weights_3:0\n","(512, 512)\n","model0MLP/biases_3:0\n","(512,)\n","model0MLP/weights_4:0\n","(512, 2)\n","model0MLP/biases_4:0\n","(2,)\n","model1MLP/weights_0:0\n","(29, 128)\n","model1MLP/biases_0:0\n","(128,)\n","model1MLP/weights_1:0\n","(128, 256)\n","model1MLP/biases_1:0\n","(256,)\n","model1MLP/weights_2:0\n","(256, 512)\n","model1MLP/biases_2:0\n","(512,)\n","model1MLP/weights_3:0\n","(512, 512)\n","model1MLP/biases_3:0\n","(512,)\n","model1MLP/weights_4:0\n","(512, 2)\n","model1MLP/biases_4:0\n","(2,)\n","model2MLP/weights_0:0\n","(29, 128)\n","model2MLP/biases_0:0\n","(128,)\n","model2MLP/weights_1:0\n","(128, 256)\n","model2MLP/biases_1:0\n","(256,)\n","model2MLP/weights_2:0\n","(256, 512)\n","model2MLP/biases_2:0\n","(512,)\n","model2MLP/weights_3:0\n","(512, 512)\n","model2MLP/biases_3:0\n","(512,)\n","model2MLP/weights_4:0\n","(512, 2)\n","model2MLP/biases_4:0\n","(2,)\n","model3MLP/weights_0:0\n","(29, 128)\n","model3MLP/biases_0:0\n","(128,)\n","model3MLP/weights_1:0\n","(128, 256)\n","model3MLP/biases_1:0\n","(256,)\n","model3MLP/weights_2:0\n","(256, 512)\n","model3MLP/biases_2:0\n","(512,)\n","model3MLP/weights_3:0\n","(512, 512)\n","model3MLP/biases_3:0\n","(512,)\n","model3MLP/weights_4:0\n","(512, 2)\n","model3MLP/biases_4:0\n","(2,)\n","model4MLP/weights_0:0\n","(29, 128)\n","model4MLP/biases_0:0\n","(128,)\n","model4MLP/weights_1:0\n","(128, 256)\n","model4MLP/biases_1:0\n","(256,)\n","model4MLP/weights_2:0\n","(256, 512)\n","model4MLP/biases_2:0\n","(512,)\n","model4MLP/weights_3:0\n","(512, 512)\n","model4MLP/biases_3:0\n","(512,)\n","model4MLP/weights_4:0\n","(512, 2)\n","model4MLP/biases_4:0\n","(2,)\n","model5MLP/weights_0:0\n","(29, 128)\n","model5MLP/biases_0:0\n","(128,)\n","model5MLP/weights_1:0\n","(128, 256)\n","model5MLP/biases_1:0\n","(256,)\n","model5MLP/weights_2:0\n","(256, 512)\n","model5MLP/biases_2:0\n","(512,)\n","model5MLP/weights_3:0\n","(512, 512)\n","model5MLP/biases_3:0\n","(512,)\n","model5MLP/weights_4:0\n","(512, 2)\n","model5MLP/biases_4:0\n","(2,)\n","model6MLP/weights_0:0\n","(29, 128)\n","model6MLP/biases_0:0\n","(128,)\n","model6MLP/weights_1:0\n","(128, 256)\n","model6MLP/biases_1:0\n","(256,)\n","model6MLP/weights_2:0\n","(256, 512)\n","model6MLP/biases_2:0\n","(512,)\n","model6MLP/weights_3:0\n","(512, 512)\n","model6MLP/biases_3:0\n","(512,)\n","model6MLP/weights_4:0\n","(512, 2)\n","model6MLP/biases_4:0\n","(2,)\n","model7MLP/weights_0:0\n","(29, 128)\n","model7MLP/biases_0:0\n","(128,)\n","model7MLP/weights_1:0\n","(128, 256)\n","model7MLP/biases_1:0\n","(256,)\n","model7MLP/weights_2:0\n","(256, 512)\n","model7MLP/biases_2:0\n","(512,)\n","model7MLP/weights_3:0\n","(512, 512)\n","model7MLP/biases_3:0\n","(512,)\n","model7MLP/weights_4:0\n","(512, 2)\n","model7MLP/biases_4:0\n","(2,)\n","model8MLP/weights_0:0\n","(29, 128)\n","model8MLP/biases_0:0\n","(128,)\n","model8MLP/weights_1:0\n","(128, 256)\n","model8MLP/biases_1:0\n","(256,)\n","model8MLP/weights_2:0\n","(256, 512)\n","model8MLP/biases_2:0\n","(512,)\n","model8MLP/weights_3:0\n","(512, 512)\n","model8MLP/biases_3:0\n","(512,)\n","model8MLP/weights_4:0\n","(512, 2)\n","model8MLP/biases_4:0\n","(2,)\n","model9MLP/weights_0:0\n","(29, 128)\n","model9MLP/biases_0:0\n","(128,)\n","model9MLP/weights_1:0\n","(128, 256)\n","model9MLP/biases_1:0\n","(256,)\n","model9MLP/weights_2:0\n","(256, 512)\n","model9MLP/biases_2:0\n","(512,)\n","model9MLP/weights_3:0\n","(512, 512)\n","model9MLP/biases_3:0\n","(512,)\n","model9MLP/weights_4:0\n","(512, 2)\n","model9MLP/biases_4:0\n","(2,)\n","2024-04-19 19:38:13.689181: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n","itr: 0, nll: 0.12554383277893066\n","itr: 0, nll: 0.9003423452377319\n","itr: 0, nll: 0.10870647430419922\n","itr: 0, nll: -0.017589211463928223\n","itr: 0, nll: 0.12841343879699707\n","itr: 0, nll: 0.10896778106689453\n","itr: 0, nll: -0.0968397855758667\n","itr: 0, nll: 0.08348321914672852\n","itr: 0, nll: 2.084991931915283\n","itr: 0, nll: 3.3056769371032715\n","itr: 300, nll: -0.31964361667633057\n","itr: 300, nll: -0.2324974536895752\n","itr: 300, nll: -0.4874659776687622\n","itr: 300, nll: -0.17476534843444824\n","itr: 300, nll: -0.30287933349609375\n","itr: 300, nll: -0.4432373046875\n","itr: 300, nll: -0.15806353092193604\n","itr: 300, nll: -0.20549464225769043\n","itr: 300, nll: -0.18808186054229736\n","itr: 300, nll: -0.334452748298645\n","itr: 600, nll: -0.27855420112609863\n","itr: 600, nll: -0.4415987730026245\n","itr: 600, nll: -0.3551098108291626\n","itr: 600, nll: -0.2184128761291504\n","itr: 600, nll: -0.28712546825408936\n","itr: 600, nll: -0.3274601697921753\n","itr: 600, nll: -0.4138084650039673\n","itr: 600, nll: -0.35898590087890625\n","itr: 600, nll: -0.3144240379333496\n","itr: 600, nll: -0.46599018573760986\n","itr: 900, nll: -0.4408247470855713\n","itr: 900, nll: -0.4868103265762329\n","itr: 900, nll: -0.48604655265808105\n","itr: 900, nll: -0.4819427728652954\n","itr: 900, nll: -0.299452543258667\n","itr: 900, nll: -0.31780409812927246\n","itr: 900, nll: -0.31665921211242676\n","itr: 900, nll: -0.3225287199020386\n","itr: 900, nll: -0.5366357564926147\n","itr: 900, nll: -0.31301093101501465\n","itr: 1200, nll: -0.4606664180755615\n","itr: 1200, nll: -0.3453962802886963\n","itr: 1200, nll: -0.5165964365005493\n","itr: 1200, nll: -0.5716134309768677\n","itr: 1200, nll: -0.28930044174194336\n","itr: 1200, nll: -0.4820723533630371\n","itr: 1200, nll: -0.3290022611618042\n","itr: 1200, nll: -0.4294179677963257\n","itr: 1200, nll: -0.27126479148864746\n","itr: 1200, nll: -0.44640278816223145\n","itr: 1500, nll: -0.6250122785568237\n","itr: 1500, nll: -0.4723787307739258\n","itr: 1500, nll: -0.471451997756958\n","itr: 1500, nll: -0.5876448154449463\n","itr: 1500, nll: -0.38979828357696533\n","itr: 1500, nll: -0.46033740043640137\n","itr: 1500, nll: -0.4362790584564209\n","itr: 1500, nll: -0.5091022253036499\n","itr: 1500, nll: -0.4969257116317749\n","itr: 1500, nll: -0.5669879913330078\n","itr: 1800, nll: -0.5076953172683716\n","itr: 1800, nll: -0.5406168699264526\n","itr: 1800, nll: -0.6539443731307983\n","itr: 1800, nll: -0.4642425775527954\n","itr: 1800, nll: -0.3679473400115967\n","itr: 1800, nll: -0.5758235454559326\n","itr: 1800, nll: -0.57196044921875\n","itr: 1800, nll: -0.5784318447113037\n","itr: 1800, nll: -0.5080549716949463\n","itr: 1800, nll: -0.5424447059631348\n","itr: 2100, nll: -0.699155330657959\n","itr: 2100, nll: -0.5613529682159424\n","itr: 2100, nll: -0.4269191026687622\n","itr: 2100, nll: -0.547677755355835\n","itr: 2100, nll: -0.5450773239135742\n","itr: 2100, nll: -0.3689291477203369\n","itr: 2100, nll: -0.60160231590271\n","itr: 2100, nll: -0.41598784923553467\n","itr: 2100, nll: -0.5657453536987305\n","itr: 2100, nll: -0.6100938320159912\n","itr: 2400, nll: -0.5300791263580322\n","itr: 2400, nll: -0.44364118576049805\n","itr: 2400, nll: -0.5773711204528809\n","itr: 2400, nll: -0.576657772064209\n","itr: 2400, nll: -0.3744276762008667\n","itr: 2400, nll: -0.5624574422836304\n","itr: 2400, nll: -0.5138407945632935\n","itr: 2400, nll: -0.44760918617248535\n","itr: 2400, nll: -0.5508946180343628\n","itr: 2400, nll: -0.5423458814620972\n","itr: 2700, nll: -0.6958719491958618\n","itr: 2700, nll: -0.5984303951263428\n","itr: 2700, nll: -0.5534582138061523\n","itr: 2700, nll: -0.5372694730758667\n","itr: 2700, nll: -0.6087441444396973\n","itr: 2700, nll: -0.5929886102676392\n","itr: 2700, nll: -0.6370812654495239\n","itr: 2700, nll: -0.5922342538833618\n","itr: 2700, nll: -0.6030004024505615\n","itr: 2700, nll: -0.6092565059661865\n","itr: 3000, nll: -0.566780686378479\n","itr: 3000, nll: -0.4869115352630615\n","itr: 3000, nll: -0.7328979969024658\n","itr: 3000, nll: -0.5351476669311523\n","itr: 3000, nll: -0.614104151725769\n","itr: 3000, nll: -0.6717268228530884\n","itr: 3000, nll: -0.7707548141479492\n","itr: 3000, nll: -0.5145766735076904\n","itr: 3000, nll: -0.7095627784729004\n","itr: 3000, nll: -0.678534984588623\n","itr: 3300, nll: -0.5964281558990479\n","itr: 3300, nll: -0.7729556560516357\n","itr: 3300, nll: -0.8319833278656006\n","itr: 3300, nll: -0.5834684371948242\n","itr: 3300, nll: -0.7506756782531738\n","itr: 3300, nll: -0.6046566963195801\n","itr: 3300, nll: -0.6106796264648438\n","itr: 3300, nll: -0.6273163557052612\n","itr: 3300, nll: -0.5901246070861816\n","itr: 3300, nll: -0.7470333576202393\n","itr: 3600, nll: -0.6621291637420654\n","itr: 3600, nll: -0.5748668909072876\n","itr: 3600, nll: -0.48013126850128174\n","itr: 3600, nll: -0.7021836042404175\n","itr: 3600, nll: -0.8866899013519287\n","itr: 3600, nll: -0.6000691652297974\n","itr: 3600, nll: -0.5946545600891113\n","itr: 3600, nll: -0.8614180088043213\n","itr: 3600, nll: -0.7660033702850342\n","itr: 3600, nll: -0.7507243156433105\n","itr: 3900, nll: -0.6690897941589355\n","itr: 3900, nll: -0.7129702568054199\n","itr: 3900, nll: -0.6095318794250488\n","itr: 3900, nll: -0.8776476383209229\n","itr: 3900, nll: -0.7806661128997803\n","itr: 3900, nll: -0.6476123332977295\n","itr: 3900, nll: -0.6976187229156494\n","itr: 3900, nll: -0.8542079925537109\n","itr: 3900, nll: -0.7279369831085205\n","itr: 3900, nll: -0.7600665092468262\n","itr: 4200, nll: -0.6773444414138794\n","itr: 4200, nll: -0.7548401355743408\n","itr: 4200, nll: -0.752531886100769\n","itr: 4200, nll: -0.7737913131713867\n","itr: 4200, nll: -0.8753912448883057\n","itr: 4200, nll: -0.697442889213562\n","itr: 4200, nll: -0.7296935319900513\n","itr: 4200, nll: -0.5865310430526733\n","itr: 4200, nll: -0.6783695220947266\n","itr: 4200, nll: -0.6826385259628296\n","itr: 4500, nll: -0.6871472597122192\n","itr: 4500, nll: -0.8443920612335205\n","itr: 4500, nll: -0.9360013008117676\n","itr: 4500, nll: -0.8173019886016846\n","itr: 4500, nll: -0.7259467840194702\n","itr: 4500, nll: -0.7384852170944214\n","itr: 4500, nll: -0.9006654024124146\n","itr: 4500, nll: -0.8145135641098022\n","itr: 4500, nll: -0.7813940048217773\n","itr: 4500, nll: -0.6644672155380249\n","itr: 4800, nll: -0.6993415355682373\n","itr: 4800, nll: -0.7732619047164917\n","itr: 4800, nll: -0.6385624408721924\n","itr: 4800, nll: -0.7738533020019531\n","itr: 4800, nll: -0.6728736162185669\n","itr: 4800, nll: -0.9634838104248047\n","itr: 4800, nll: -0.9326984882354736\n","itr: 4800, nll: -0.9585700035095215\n","itr: 4800, nll: -0.8274495601654053\n","itr: 4800, nll: -0.7545599937438965\n","itr: 5100, nll: -0.9550811052322388\n","itr: 5100, nll: -0.7920870780944824\n","itr: 5100, nll: -0.9706798791885376\n","itr: 5100, nll: -0.7820186614990234\n","itr: 5100, nll: -0.9079649448394775\n","itr: 5100, nll: -0.8436450958251953\n","itr: 5100, nll: -0.9594814777374268\n","itr: 5100, nll: -0.8202959299087524\n","itr: 5100, nll: -0.9544371366500854\n","itr: 5100, nll: -0.8060804605484009\n","itr: 5400, nll: -0.8962297439575195\n","itr: 5400, nll: -0.8466217517852783\n","itr: 5400, nll: -0.8374340534210205\n","itr: 5400, nll: -0.6776978969573975\n","itr: 5400, nll: -0.9339604377746582\n","itr: 5400, nll: -0.8633593320846558\n","itr: 5400, nll: -0.9475871324539185\n","itr: 5400, nll: -0.9564157724380493\n","itr: 5400, nll: -0.7163082361221313\n","itr: 5400, nll: -0.7634633779525757\n","itr: 5700, nll: -0.8767209053039551\n","itr: 5700, nll: -0.7066383361816406\n","itr: 5700, nll: -0.9669482707977295\n","itr: 5700, nll: -0.7435274124145508\n","itr: 5700, nll: -0.9152578115463257\n","itr: 5700, nll: -0.8359593152999878\n","itr: 5700, nll: -0.9993107318878174\n","itr: 5700, nll: -0.8551549911499023\n","itr: 5700, nll: -0.7731517553329468\n","itr: 5700, nll: -0.7890384197235107\n","itr: 6000, nll: -0.9444048404693604\n","itr: 6000, nll: -0.972252368927002\n","itr: 6000, nll: -0.9788131713867188\n","itr: 6000, nll: -0.9925644397735596\n","itr: 6000, nll: -0.8808531761169434\n","itr: 6000, nll: -1.0960561037063599\n","itr: 6000, nll: -1.0265800952911377\n","itr: 6000, nll: -1.1485512256622314\n","itr: 6000, nll: -0.7523205280303955\n","itr: 6000, nll: -1.1141364574432373\n","itr: 6300, nll: -0.9525508880615234\n","itr: 6300, nll: -1.0033668279647827\n","itr: 6300, nll: -1.077531337738037\n","itr: 6300, nll: -0.8988666534423828\n","itr: 6300, nll: -0.9648230075836182\n","itr: 6300, nll: -1.0945477485656738\n","itr: 6300, nll: -1.1334784030914307\n","itr: 6300, nll: -0.8761866092681885\n","itr: 6300, nll: -0.9513274431228638\n","itr: 6300, nll: -0.9110705852508545\n","itr: 6600, nll: -0.9291024208068848\n","itr: 6600, nll: -0.8072857856750488\n","itr: 6600, nll: -0.8199061155319214\n","itr: 6600, nll: -0.9726173877716064\n","itr: 6600, nll: -0.9763562679290771\n","itr: 6600, nll: -0.8391919136047363\n","itr: 6600, nll: -1.030139684677124\n","itr: 6600, nll: -1.0562189817428589\n","itr: 6600, nll: -0.676156759262085\n","itr: 6600, nll: -1.0368403196334839\n","itr: 6900, nll: -0.9438306093215942\n","itr: 6900, nll: -0.866553544998169\n","itr: 6900, nll: -0.8426291942596436\n","itr: 6900, nll: -0.873151421546936\n","itr: 6900, nll: -1.0459742546081543\n","itr: 6900, nll: -0.9712200164794922\n","itr: 6900, nll: -0.9394634962081909\n","itr: 6900, nll: -0.8975298404693604\n","itr: 6900, nll: -0.8240475654602051\n","itr: 6900, nll: -0.9995691776275635\n","itr: 7200, nll: -1.0193356275558472\n","itr: 7200, nll: -0.9790657758712769\n","itr: 7200, nll: -1.1549839973449707\n","itr: 7200, nll: -0.9485170841217041\n","itr: 7200, nll: -0.9984117746353149\n","itr: 7200, nll: -0.7200441360473633\n","itr: 7200, nll: -0.7274295091629028\n","itr: 7200, nll: -0.9579201936721802\n","itr: 7200, nll: -0.9924967288970947\n","itr: 7200, nll: -0.8626686334609985\n","itr: 7500, nll: -1.1943817138671875\n","itr: 7500, nll: -0.5882828235626221\n","itr: 7500, nll: -0.9306809902191162\n","itr: 7500, nll: -0.8221848011016846\n","itr: 7500, nll: -0.9655053615570068\n","itr: 7500, nll: -0.9913711547851562\n","itr: 7500, nll: -0.8946479558944702\n","itr: 7500, nll: -1.0503650903701782\n","itr: 7500, nll: -0.9768295288085938\n","itr: 7500, nll: -1.053056001663208\n","itr: 7800, nll: -1.0360958576202393\n","itr: 7800, nll: -0.8943911790847778\n","itr: 7800, nll: -1.0859978199005127\n","itr: 7800, nll: -0.8474053144454956\n","itr: 7800, nll: -0.8164911270141602\n","itr: 7800, nll: -1.1096081733703613\n","itr: 7800, nll: -1.087410807609558\n","itr: 7800, nll: -0.9637207984924316\n","itr: 7800, nll: -1.0295848846435547\n","itr: 7800, nll: -0.94451904296875\n","itr: 8100, nll: -1.030820608139038\n","itr: 8100, nll: -1.0996026992797852\n","itr: 8100, nll: -0.9692420959472656\n","itr: 8100, nll: -0.8951104879379272\n","itr: 8100, nll: -1.0319321155548096\n","itr: 8100, nll: -0.9073889255523682\n","itr: 8100, nll: -0.9923604726791382\n","itr: 8100, nll: -1.0647940635681152\n","itr: 8100, nll: -0.9119057655334473\n","itr: 8100, nll: -0.9372165203094482\n","itr: 8400, nll: -1.1326093673706055\n","itr: 8400, nll: -0.9248448610305786\n","itr: 8400, nll: -0.8770287036895752\n","itr: 8400, nll: -0.9972302913665771\n","itr: 8400, nll: -0.6523936986923218\n","itr: 8400, nll: -1.0955309867858887\n","itr: 8400, nll: -1.1747349500656128\n","itr: 8400, nll: -1.0849486589431763\n","itr: 8400, nll: -1.2114994525909424\n","itr: 8400, nll: -0.9977149963378906\n","itr: 8700, nll: -0.9220359325408936\n","itr: 8700, nll: -0.9592045545578003\n","itr: 8700, nll: -1.1632962226867676\n","itr: 8700, nll: -1.208591103553772\n","itr: 8700, nll: -1.1380698680877686\n","itr: 8700, nll: -1.0232329368591309\n","itr: 8700, nll: -1.1474286317825317\n","itr: 8700, nll: -1.1183205842971802\n","itr: 8700, nll: -1.1149588823318481\n","itr: 8700, nll: -1.10819673538208\n","itr: 9000, nll: -0.9531303644180298\n","itr: 9000, nll: -1.1418042182922363\n","itr: 9000, nll: -0.99280846118927\n","itr: 9000, nll: -1.0563883781433105\n","itr: 9000, nll: -0.8614556789398193\n","itr: 9000, nll: -1.2993348836898804\n","itr: 9000, nll: -0.8140683174133301\n","itr: 9000, nll: -0.8486182689666748\n","itr: 9000, nll: -1.052009105682373\n","itr: 9000, nll: -1.0978881120681763\n","itr: 9300, nll: -1.1372166872024536\n","itr: 9300, nll: -1.182347297668457\n","itr: 9300, nll: -0.9673629999160767\n","itr: 9300, nll: -1.141723871231079\n","itr: 9300, nll: -1.2272040843963623\n","itr: 9300, nll: -1.0435643196105957\n","itr: 9300, nll: -1.1600431203842163\n","itr: 9300, nll: -0.89814293384552\n","itr: 9300, nll: -0.9073814153671265\n","itr: 9300, nll: -0.9883027076721191\n","itr: 9600, nll: -1.1666351556777954\n","itr: 9600, nll: -1.174961805343628\n","itr: 9600, nll: -1.173137903213501\n","itr: 9600, nll: -1.0219290256500244\n","itr: 9600, nll: -1.0843437910079956\n","itr: 9600, nll: -1.0510447025299072\n","itr: 9600, nll: -1.0849440097808838\n","itr: 9600, nll: -1.2108302116394043\n","itr: 9600, nll: -1.1381868124008179\n","itr: 9600, nll: -1.1286358833312988\n","itr: 9900, nll: -1.2662487030029297\n","itr: 9900, nll: -1.2083746194839478\n","itr: 9900, nll: -1.2931140661239624\n","itr: 9900, nll: -1.0992399454116821\n","itr: 9900, nll: -1.221524715423584\n","itr: 9900, nll: -1.3316349983215332\n","itr: 9900, nll: -1.1532670259475708\n","itr: 9900, nll: -0.9258514642715454\n","itr: 9900, nll: -1.0527299642562866\n","itr: 9900, nll: -1.1670031547546387\n","itr: 10200, nll: -1.1650445461273193\n","itr: 10200, nll: -1.0921567678451538\n","itr: 10200, nll: -1.251076340675354\n","itr: 10200, nll: -1.118788480758667\n","itr: 10200, nll: -1.131628155708313\n","itr: 10200, nll: -0.9329493045806885\n","itr: 10200, nll: -1.3693639039993286\n","itr: 10200, nll: -1.0562992095947266\n","itr: 10200, nll: -1.1633951663970947\n","itr: 10200, nll: -1.1514861583709717\n","itr: 10500, nll: -1.1500608921051025\n","itr: 10500, nll: -1.0279093980789185\n","itr: 10500, nll: -0.9019837379455566\n","itr: 10500, nll: -1.0786941051483154\n","itr: 10500, nll: -1.1150882244110107\n","itr: 10500, nll: -1.2833123207092285\n","itr: 10500, nll: -1.2356171607971191\n","itr: 10500, nll: -0.9864439964294434\n","itr: 10500, nll: -0.8402771949768066\n","itr: 10500, nll: -1.2160792350769043\n","itr: 10800, nll: -1.138971209526062\n","itr: 10800, nll: -1.173213005065918\n","itr: 10800, nll: -1.1886966228485107\n","itr: 10800, nll: -0.8556976318359375\n","itr: 10800, nll: -1.2671284675598145\n","itr: 10800, nll: -1.2098801136016846\n","itr: 10800, nll: -1.010430097579956\n","itr: 10800, nll: -1.2200990915298462\n","itr: 10800, nll: -0.9737982749938965\n","itr: 10800, nll: -1.1171784400939941\n","itr: 11100, nll: -0.6783416271209717\n","itr: 11100, nll: -1.3298981189727783\n","itr: 11100, nll: -1.2188831567764282\n","itr: 11100, nll: -0.8515150547027588\n","itr: 11100, nll: -1.2405966520309448\n","itr: 11100, nll: -1.335359811782837\n","itr: 11100, nll: -1.1740379333496094\n","itr: 11100, nll: -1.1983561515808105\n","itr: 11100, nll: -0.971908688545227\n","itr: 11100, nll: -1.2191808223724365\n","itr: 11400, nll: -1.3680269718170166\n","itr: 11400, nll: -1.2784324884414673\n","itr: 11400, nll: -1.2471115589141846\n","itr: 11400, nll: -1.367916464805603\n","itr: 11400, nll: -1.3177987337112427\n","itr: 11400, nll: -1.1435602903366089\n","itr: 11400, nll: -1.0706796646118164\n","itr: 11400, nll: -1.2453937530517578\n","itr: 11400, nll: -1.1251752376556396\n","itr: 11400, nll: -1.0335111618041992\n","itr: 11700, nll: -0.6782293319702148\n","itr: 11700, nll: -1.198173999786377\n","itr: 11700, nll: -1.1248633861541748\n","itr: 11700, nll: -1.2094727754592896\n","itr: 11700, nll: -1.119966983795166\n","itr: 11700, nll: -1.2517704963684082\n","itr: 11700, nll: -1.0837514400482178\n","itr: 11700, nll: -1.234621524810791\n","itr: 11700, nll: -1.2995158433914185\n","itr: 11700, nll: -1.2233904600143433\n","itr: 12000, nll: -0.9929168224334717\n","itr: 12000, nll: -1.1531659364700317\n","itr: 12000, nll: -1.0826802253723145\n","itr: 12000, nll: -1.2173799276351929\n","itr: 12000, nll: -1.2591063976287842\n","itr: 12000, nll: -1.1294710636138916\n","itr: 12000, nll: -1.0603760480880737\n","itr: 12000, nll: -1.103790044784546\n","itr: 12000, nll: -0.9700558185577393\n","itr: 12000, nll: -1.1261029243469238\n","itr: 12300, nll: -1.1839637756347656\n","itr: 12300, nll: -1.3553493022918701\n","itr: 12300, nll: -1.2289607524871826\n","itr: 12300, nll: -1.1438524723052979\n","itr: 12300, nll: -1.3399099111557007\n","itr: 12300, nll: -1.1894183158874512\n","itr: 12300, nll: -1.2700064182281494\n","itr: 12300, nll: -1.096980333328247\n","itr: 12300, nll: -0.8781287670135498\n","itr: 12300, nll: -1.2720215320587158\n","itr: 12600, nll: -1.2373394966125488\n","itr: 12600, nll: -1.0650798082351685\n","itr: 12600, nll: -1.1501977443695068\n","itr: 12600, nll: -1.2405484914779663\n","itr: 12600, nll: -1.3626337051391602\n","itr: 12600, nll: -0.8465619087219238\n","itr: 12600, nll: -1.2050514221191406\n","itr: 12600, nll: -1.1798875331878662\n","itr: 12600, nll: -1.2947354316711426\n","itr: 12600, nll: -1.2337020635604858\n","itr: 12900, nll: -1.275132179260254\n","itr: 12900, nll: -1.2798035144805908\n","itr: 12900, nll: -0.9693408012390137\n","itr: 12900, nll: -1.111933946609497\n","itr: 12900, nll: -1.2214387655258179\n","itr: 12900, nll: -1.2526435852050781\n","itr: 12900, nll: -1.3129143714904785\n","itr: 12900, nll: -1.2698075771331787\n","itr: 12900, nll: -1.1181414127349854\n","itr: 12900, nll: -1.1317543983459473\n","itr: 13200, nll: -1.1912777423858643\n","itr: 13200, nll: -1.1406784057617188\n","itr: 13200, nll: -1.114501953125\n","itr: 13200, nll: -1.3191401958465576\n","itr: 13200, nll: -1.4128094911575317\n","itr: 13200, nll: -1.3399806022644043\n","itr: 13200, nll: -1.2996456623077393\n","itr: 13200, nll: -1.191838264465332\n","itr: 13200, nll: -1.1339346170425415\n","itr: 13200, nll: -1.237227439880371\n","itr: 13500, nll: -1.1256831884384155\n","itr: 13500, nll: -1.1271110773086548\n","itr: 13500, nll: -1.316206932067871\n","itr: 13500, nll: -1.2238833904266357\n","itr: 13500, nll: -0.9268093109130859\n","itr: 13500, nll: -1.049720048904419\n","itr: 13500, nll: -1.3948721885681152\n","itr: 13500, nll: -1.0231693983078003\n","itr: 13500, nll: 1.736945629119873\n","itr: 13500, nll: -1.2981557846069336\n","itr: 13800, nll: -1.1676969528198242\n","itr: 13800, nll: -1.4065629243850708\n","itr: 13800, nll: -1.4141449928283691\n","itr: 13800, nll: -1.0706127882003784\n","itr: 13800, nll: -1.3421881198883057\n","itr: 13800, nll: -0.9192007780075073\n","itr: 13800, nll: -1.3808386325836182\n","itr: 13800, nll: -1.0371623039245605\n","itr: 13800, nll: -1.1508862972259521\n","itr: 13800, nll: -1.2047076225280762\n","itr: 14100, nll: -1.2826309204101562\n","itr: 14100, nll: -1.22101628780365\n","itr: 14100, nll: -1.3714860677719116\n","itr: 14100, nll: -1.167144536972046\n","itr: 14100, nll: -1.1754839420318604\n","itr: 14100, nll: -1.2750883102416992\n","itr: 14100, nll: -1.459636926651001\n","itr: 14100, nll: -1.1806941032409668\n","itr: 14100, nll: -1.3738510608673096\n","itr: 14100, nll: -1.2672994136810303\n","itr: 14400, nll: -1.3348348140716553\n","itr: 14400, nll: -1.3100426197052002\n","itr: 14400, nll: -1.1734280586242676\n","itr: 14400, nll: -1.201548457145691\n","itr: 14400, nll: -1.2294577360153198\n","itr: 14400, nll: -1.3012070655822754\n","itr: 14400, nll: -1.2675280570983887\n","itr: 14400, nll: -1.3815325498580933\n","itr: 14400, nll: -1.126476764678955\n","itr: 14400, nll: -1.0202689170837402\n","itr: 14700, nll: -1.4195685386657715\n","itr: 14700, nll: -1.0266724824905396\n","itr: 14700, nll: -1.2557456493377686\n","itr: 14700, nll: -1.126488208770752\n","itr: 14700, nll: -1.350553035736084\n","itr: 14700, nll: -1.280023455619812\n","itr: 14700, nll: -1.3642923831939697\n","itr: 14700, nll: -1.224394679069519\n","itr: 14700, nll: -1.2303829193115234\n","itr: 14700, nll: -1.1971384286880493\n","itr: 15000, nll: -1.240136981010437\n","itr: 15000, nll: -1.3349754810333252\n","itr: 15000, nll: -1.2296030521392822\n","itr: 15000, nll: -1.1376359462738037\n","itr: 15000, nll: -1.3210113048553467\n","itr: 15000, nll: -1.0691006183624268\n","itr: 15000, nll: -1.2540892362594604\n","itr: 15000, nll: -0.6836800575256348\n","itr: 15000, nll: -1.1849497556686401\n","itr: 15000, nll: -1.3132851123809814\n","itr: 15300, nll: -1.3155587911605835\n","itr: 15300, nll: -1.150071382522583\n","itr: 15300, nll: -1.0932979583740234\n","itr: 15300, nll: -1.2752280235290527\n","itr: 15300, nll: -1.022651195526123\n","itr: 15300, nll: -1.1904332637786865\n","itr: 15300, nll: -1.2425651550292969\n","itr: 15300, nll: -1.3874483108520508\n","itr: 15300, nll: -1.3634732961654663\n","itr: 15300, nll: -1.2370550632476807\n","itr: 15600, nll: -1.2703098058700562\n","itr: 15600, nll: -1.175779104232788\n","itr: 15600, nll: -1.315208911895752\n","itr: 15600, nll: -1.3702762126922607\n","itr: 15600, nll: -1.1871623992919922\n","itr: 15600, nll: -1.244194507598877\n","itr: 15600, nll: -1.2912299633026123\n","itr: 15600, nll: -1.2408612966537476\n","itr: 15600, nll: -1.3255586624145508\n","itr: 15600, nll: -1.357790470123291\n","itr: 15900, nll: -1.2511131763458252\n","itr: 15900, nll: -1.4395133256912231\n","itr: 15900, nll: -1.3899681568145752\n","itr: 15900, nll: -1.1658263206481934\n","itr: 15900, nll: -1.1138124465942383\n","itr: 15900, nll: -1.2700871229171753\n","itr: 15900, nll: -1.3868159055709839\n","itr: 15900, nll: -1.399773359298706\n","itr: 15900, nll: -1.1370794773101807\n","itr: 15900, nll: -1.3757343292236328\n","itr: 16200, nll: -1.3991851806640625\n","itr: 16200, nll: -1.4605408906936646\n","itr: 16200, nll: -1.308809757232666\n","itr: 16200, nll: -1.4254063367843628\n","itr: 16200, nll: -1.3602222204208374\n","itr: 16200, nll: -1.329962968826294\n","itr: 16200, nll: -1.2819592952728271\n","itr: 16200, nll: -1.382918119430542\n","itr: 16200, nll: -1.2972887754440308\n","itr: 16200, nll: -1.2524133920669556\n","itr: 16500, nll: -1.4427536725997925\n","itr: 16500, nll: -1.2812800407409668\n","itr: 16500, nll: -1.4630110263824463\n","itr: 16500, nll: -1.1898670196533203\n","itr: 16500, nll: -1.367555022239685\n","itr: 16500, nll: -1.3165395259857178\n","itr: 16500, nll: -1.422579050064087\n","itr: 16500, nll: -1.198685884475708\n","itr: 16500, nll: -1.466199517250061\n","itr: 16500, nll: -1.1562707424163818\n","itr: 16800, nll: -1.1888186931610107\n","itr: 16800, nll: -1.5448029041290283\n","itr: 16800, nll: -1.2998261451721191\n","itr: 16800, nll: -1.243935465812683\n","itr: 16800, nll: -1.360502004623413\n","itr: 16800, nll: -1.1470643281936646\n","itr: 16800, nll: -1.3903403282165527\n","itr: 16800, nll: -1.4857690334320068\n","itr: 16800, nll: -1.3083784580230713\n","itr: 16800, nll: -1.3894888162612915\n","itr: 17100, nll: -1.3132612705230713\n","itr: 17100, nll: -1.2631750106811523\n","itr: 17100, nll: -1.3497838973999023\n","itr: 17100, nll: -1.3610246181488037\n","itr: 17100, nll: -1.0249993801116943\n","itr: 17100, nll: -1.386601209640503\n","itr: 17100, nll: -1.2940138578414917\n","itr: 17100, nll: -1.3157744407653809\n","itr: 17100, nll: -1.3339190483093262\n","itr: 17100, nll: -1.2103694677352905\n","itr: 17400, nll: -1.2248828411102295\n","itr: 17400, nll: -1.4977344274520874\n","itr: 17400, nll: -1.4298920631408691\n","itr: 17400, nll: -1.3908439874649048\n","itr: 17400, nll: -1.2134069204330444\n","itr: 17400, nll: -1.3365293741226196\n","itr: 17400, nll: -1.5769461393356323\n","itr: 17400, nll: -1.1611971855163574\n","itr: 17400, nll: -1.3212755918502808\n","itr: 17400, nll: -1.1579444408416748\n","itr: 17700, nll: -1.4463046789169312\n","itr: 17700, nll: -1.3758273124694824\n","itr: 17700, nll: -1.0986990928649902\n","itr: 17700, nll: -1.36356520652771\n","itr: 17700, nll: -1.3279681205749512\n","itr: 17700, nll: -1.2812824249267578\n","itr: 17700, nll: -1.301638126373291\n","itr: 17700, nll: -1.354261875152588\n","itr: 17700, nll: -1.3878040313720703\n","itr: 17700, nll: -1.4217993021011353\n","itr: 18000, nll: -1.207411527633667\n","itr: 18000, nll: -1.1925640106201172\n","itr: 18000, nll: -1.3960392475128174\n","itr: 18000, nll: -1.2014704942703247\n","itr: 18000, nll: -1.3628087043762207\n","itr: 18000, nll: -1.344238042831421\n","itr: 18000, nll: -1.294387698173523\n","itr: 18000, nll: -1.2181518077850342\n","itr: 18000, nll: -1.1629173755645752\n","itr: 18000, nll: -1.2026307582855225\n","itr: 18300, nll: -1.1445074081420898\n","itr: 18300, nll: -1.4755730628967285\n","itr: 18300, nll: -1.168249249458313\n","itr: 18300, nll: -1.3364131450653076\n","itr: 18300, nll: -1.3860599994659424\n","itr: 18300, nll: -1.4907268285751343\n","itr: 18300, nll: -1.3457883596420288\n","itr: 18300, nll: -0.9669533967971802\n","itr: 18300, nll: -1.5215184688568115\n","itr: 18300, nll: -1.373863697052002\n","itr: 18600, nll: -1.286237120628357\n","itr: 18600, nll: -1.4773976802825928\n","itr: 18600, nll: -1.433085560798645\n","itr: 18600, nll: -1.3426603078842163\n","itr: 18600, nll: -1.4873727560043335\n","itr: 18600, nll: -1.2521119117736816\n","itr: 18600, nll: -1.2664885520935059\n","itr: 18600, nll: -1.5600063800811768\n","itr: 18600, nll: -1.4632511138916016\n","itr: 18600, nll: -1.2736769914627075\n","itr: 18900, nll: -1.244626522064209\n","itr: 18900, nll: -1.4149706363677979\n","itr: 18900, nll: -1.4579635858535767\n","itr: 18900, nll: -1.2817462682724\n","itr: 18900, nll: -1.1178255081176758\n","itr: 18900, nll: -1.3964276313781738\n","itr: 18900, nll: -1.4084229469299316\n","itr: 18900, nll: -1.4136252403259277\n","itr: 18900, nll: -1.3601782321929932\n","itr: 18900, nll: -1.3119463920593262\n","itr: 19200, nll: -1.5332651138305664\n","itr: 19200, nll: -1.3573834896087646\n","itr: 19200, nll: -1.1744710206985474\n","itr: 19200, nll: -1.3715142011642456\n","itr: 19200, nll: -1.6462078094482422\n","itr: 19200, nll: -1.457972526550293\n","itr: 19200, nll: -1.3554425239562988\n","itr: 19200, nll: -1.2548390626907349\n","itr: 19200, nll: -1.4246246814727783\n","itr: 19200, nll: -1.0687956809997559\n","itr: 19500, nll: -1.4997445344924927\n","itr: 19500, nll: -1.4267446994781494\n","itr: 19500, nll: -1.4212071895599365\n","itr: 19500, nll: -1.2675325870513916\n","itr: 19500, nll: -1.412194013595581\n","itr: 19500, nll: -1.4976084232330322\n","itr: 19500, nll: -1.29331636428833\n","itr: 19500, nll: -1.410654067993164\n","itr: 19500, nll: -1.4356882572174072\n","itr: 19500, nll: -1.4655284881591797\n","itr: 19800, nll: -1.3877568244934082\n","itr: 19800, nll: -1.2514129877090454\n","itr: 19800, nll: -1.5404093265533447\n","itr: 19800, nll: -1.41446852684021\n","itr: 19800, nll: -1.4654520750045776\n","itr: 19800, nll: -1.3774291276931763\n","itr: 19800, nll: -1.2731754779815674\n","itr: 19800, nll: -1.316175103187561\n","itr: 19800, nll: -1.3938432931900024\n","itr: 19800, nll: -1.4867699146270752\n","itr: 20100, nll: -1.4520399570465088\n","itr: 20100, nll: -1.1794917583465576\n","itr: 20100, nll: -1.5100547075271606\n","itr: 20100, nll: -1.4360616207122803\n","itr: 20100, nll: -1.510809063911438\n","itr: 20100, nll: -1.3315975666046143\n","itr: 20100, nll: -1.4992091655731201\n","itr: 20100, nll: -1.4380645751953125\n","itr: 20100, nll: -1.3413628339767456\n","itr: 20100, nll: -1.245607614517212\n","itr: 20400, nll: -1.4985452890396118\n","itr: 20400, nll: -1.525965929031372\n","itr: 20400, nll: -1.351143717765808\n","itr: 20400, nll: -1.2928558588027954\n","itr: 20400, nll: -1.4793964624404907\n","itr: 20400, nll: -1.4741811752319336\n","itr: 20400, nll: -1.4475948810577393\n","itr: 20400, nll: -1.586793065071106\n","itr: 20400, nll: -1.3305461406707764\n","itr: 20400, nll: -1.400567889213562\n","itr: 20700, nll: -1.315049171447754\n","itr: 20700, nll: -1.5170270204544067\n","itr: 20700, nll: -1.5273863077163696\n","itr: 20700, nll: -1.5124881267547607\n","itr: 20700, nll: -1.5060595273971558\n","itr: 20700, nll: -1.4387799501419067\n","itr: 20700, nll: -1.4708731174468994\n","itr: 20700, nll: -1.5390273332595825\n","itr: 20700, nll: -1.5044019222259521\n","itr: 20700, nll: -1.436556100845337\n","itr: 21000, nll: -1.3323100805282593\n","itr: 21000, nll: -1.3848445415496826\n","itr: 21000, nll: -1.367600917816162\n","itr: 21000, nll: -1.4107586145401\n","itr: 21000, nll: -1.4556293487548828\n","itr: 21000, nll: -1.7183942794799805\n","itr: 21000, nll: -1.4935072660446167\n","itr: 21000, nll: -1.206680417060852\n","itr: 21000, nll: -1.3800790309906006\n","itr: 21000, nll: -1.50318443775177\n","itr: 21300, nll: -1.6456317901611328\n","itr: 21300, nll: -1.3932157754898071\n","itr: 21300, nll: -1.3978502750396729\n","itr: 21300, nll: -1.3614003658294678\n","itr: 21300, nll: -1.4770139455795288\n","itr: 21300, nll: -1.5060045719146729\n","itr: 21300, nll: -1.599779486656189\n","itr: 21300, nll: -1.3270187377929688\n","itr: 21300, nll: -1.317444086074829\n","itr: 21300, nll: -1.4371426105499268\n","itr: 21600, nll: -1.4196304082870483\n","itr: 21600, nll: -1.364922046661377\n","itr: 21600, nll: -1.3331482410430908\n","itr: 21600, nll: -1.3584709167480469\n","itr: 21600, nll: -1.3145266771316528\n","itr: 21600, nll: -1.2699189186096191\n","itr: 21600, nll: -1.2510536909103394\n","itr: 21600, nll: -1.3570480346679688\n","itr: 21600, nll: -1.4899755716323853\n","itr: 21600, nll: -1.063790202140808\n","itr: 21900, nll: -1.6037871837615967\n","itr: 21900, nll: -1.247211217880249\n","itr: 21900, nll: -1.5939347743988037\n","itr: 21900, nll: -1.465186595916748\n","itr: 21900, nll: -1.3118373155593872\n","itr: 21900, nll: -1.3568806648254395\n","itr: 21900, nll: -1.3112589120864868\n","itr: 21900, nll: -1.2921303510665894\n","itr: 21900, nll: -1.583876609802246\n","itr: 21900, nll: -1.627992868423462\n","itr: 22200, nll: -1.4716798067092896\n","itr: 22200, nll: -1.6330910921096802\n","itr: 22200, nll: -1.5728507041931152\n","itr: 22200, nll: -1.302059292793274\n","itr: 22200, nll: -1.6230714321136475\n","itr: 22200, nll: -1.4353523254394531\n","itr: 22200, nll: -1.3696367740631104\n","itr: 22200, nll: -1.2697495222091675\n","itr: 22200, nll: -1.480009913444519\n","itr: 22200, nll: -1.404921293258667\n","itr: 22500, nll: -1.42384934425354\n","itr: 22500, nll: -1.3879457712173462\n","itr: 22500, nll: -1.3547991514205933\n","itr: 22500, nll: -1.4503995180130005\n","itr: 22500, nll: -1.4156205654144287\n","itr: 22500, nll: -1.5313849449157715\n","itr: 22500, nll: -1.3209630250930786\n","itr: 22500, nll: -1.460651159286499\n","itr: 22500, nll: -1.4325025081634521\n","itr: 22500, nll: -1.3587747812271118\n","itr: 22800, nll: -1.5035889148712158\n","itr: 22800, nll: -1.4714158773422241\n","itr: 22800, nll: -1.5648627281188965\n","itr: 22800, nll: -1.6192190647125244\n","itr: 22800, nll: -1.599565863609314\n","itr: 22800, nll: -1.4922871589660645\n","itr: 22800, nll: -1.4701765775680542\n","itr: 22800, nll: -1.364309549331665\n","itr: 22800, nll: -1.4313801527023315\n","itr: 22800, nll: -1.499546766281128\n","itr: 23100, nll: -1.595990777015686\n","itr: 23100, nll: -1.4373910427093506\n","itr: 23100, nll: -1.5055749416351318\n","itr: 23100, nll: -1.4925405979156494\n","itr: 23100, nll: -1.4287610054016113\n","itr: 23100, nll: -1.3959654569625854\n","itr: 23100, nll: -1.3198285102844238\n","itr: 23100, nll: -1.4670383930206299\n","itr: 23100, nll: -1.4257011413574219\n","itr: 23100, nll: -1.416710615158081\n","itr: 23400, nll: -1.3467520475387573\n","itr: 23400, nll: -1.4546349048614502\n","itr: 23400, nll: -1.4207870960235596\n","itr: 23400, nll: -1.2853071689605713\n","itr: 23400, nll: -1.5242342948913574\n","itr: 23400, nll: -1.40440833568573\n","itr: 23400, nll: -1.5863066911697388\n","itr: 23400, nll: -1.3816092014312744\n","itr: 23400, nll: -1.4892446994781494\n","itr: 23400, nll: -1.483911395072937\n","itr: 23700, nll: -1.2721266746520996\n","itr: 23700, nll: -1.550873041152954\n","itr: 23700, nll: -1.5782287120819092\n","itr: 23700, nll: -1.4328157901763916\n","itr: 23700, nll: -1.3568432331085205\n","itr: 23700, nll: -1.3351593017578125\n","itr: 23700, nll: -1.6029226779937744\n","itr: 23700, nll: -1.3209354877471924\n","itr: 23700, nll: -1.5391325950622559\n","itr: 23700, nll: -1.4482448101043701\n","itr: 24000, nll: -1.431250810623169\n","itr: 24000, nll: -1.405545949935913\n","itr: 24000, nll: -1.4457061290740967\n","itr: 24000, nll: -1.3616008758544922\n","itr: 24000, nll: -1.3375704288482666\n","itr: 24000, nll: -1.5632727146148682\n","itr: 24000, nll: -1.42084538936615\n","itr: 24000, nll: -1.3772271871566772\n","itr: 24000, nll: -1.458163857460022\n","itr: 24000, nll: -1.3351191282272339\n","itr: 24300, nll: -1.4426331520080566\n","itr: 24300, nll: -1.4688079357147217\n","itr: 24300, nll: -1.3813244104385376\n","itr: 24300, nll: -1.4532322883605957\n","itr: 24300, nll: -1.6112468242645264\n","itr: 24300, nll: -1.4885506629943848\n","itr: 24300, nll: -1.5617064237594604\n","itr: 24300, nll: -1.493077278137207\n","itr: 24300, nll: -1.4410181045532227\n","itr: 24300, nll: -1.3849462270736694\n","itr: 24600, nll: -1.5210354328155518\n","itr: 24600, nll: -1.556370735168457\n","itr: 24600, nll: -1.5810811519622803\n","itr: 24600, nll: -1.409873127937317\n","itr: 24600, nll: -1.5721213817596436\n","itr: 24600, nll: -1.6115915775299072\n","itr: 24600, nll: -1.528437614440918\n","itr: 24600, nll: -1.5190656185150146\n","itr: 24600, nll: -1.5170384645462036\n","itr: 24600, nll: -1.363144874572754\n","itr: 24900, nll: -1.4973676204681396\n","itr: 24900, nll: -1.4977418184280396\n","itr: 24900, nll: -1.4028371572494507\n","itr: 24900, nll: -1.2840207815170288\n","itr: 24900, nll: -1.5755382776260376\n","itr: 24900, nll: -1.2469024658203125\n","itr: 24900, nll: -1.6307299137115479\n","itr: 24900, nll: -1.3806143999099731\n","itr: 24900, nll: -1.6442590951919556\n","itr: 24900, nll: -1.5498528480529785\n","itr: 25200, nll: -1.4908769130706787\n","itr: 25200, nll: -1.4274077415466309\n","itr: 25200, nll: -1.5588821172714233\n","itr: 25200, nll: -1.4879720211029053\n","itr: 25200, nll: -1.7278144359588623\n","itr: 25200, nll: -1.5488932132720947\n","itr: 25200, nll: -1.6840527057647705\n","itr: 25200, nll: -1.540579915046692\n","itr: 25200, nll: -1.5435283184051514\n","itr: 25200, nll: -1.3849314451217651\n","itr: 25500, nll: -1.469403862953186\n","itr: 25500, nll: -1.4543044567108154\n","itr: 25500, nll: -1.5035127401351929\n","itr: 25500, nll: -1.5540504455566406\n","itr: 25500, nll: -1.6415574550628662\n","itr: 25500, nll: -1.4381533861160278\n","itr: 25500, nll: -1.5515084266662598\n","itr: 25500, nll: -1.350778579711914\n","itr: 25500, nll: -1.4659876823425293\n","itr: 25500, nll: -1.5319353342056274\n","itr: 25800, nll: -1.4905662536621094\n","itr: 25800, nll: -1.4339864253997803\n","itr: 25800, nll: -1.4366099834442139\n","itr: 25800, nll: -1.4328796863555908\n","itr: 25800, nll: -1.4449126720428467\n","itr: 25800, nll: -1.4642345905303955\n","itr: 25800, nll: -1.545443058013916\n","itr: 25800, nll: -1.2175350189208984\n","itr: 25800, nll: -1.3754158020019531\n","itr: 25800, nll: -1.717370867729187\n","itr: 26100, nll: -1.3970947265625\n","itr: 26100, nll: -1.4791653156280518\n","itr: 26100, nll: -1.4709678888320923\n","itr: 26100, nll: -1.452251672744751\n","itr: 26100, nll: -1.6037583351135254\n","itr: 26100, nll: -1.5024173259735107\n","itr: 26100, nll: -1.5561939477920532\n","itr: 26100, nll: -1.5719027519226074\n","itr: 26100, nll: -1.4793249368667603\n","itr: 26100, nll: -1.343430757522583\n","itr: 26400, nll: -1.4360604286193848\n","itr: 26400, nll: -1.5942966938018799\n","itr: 26400, nll: -1.610607385635376\n","itr: 26400, nll: -1.6401844024658203\n","itr: 26400, nll: -1.6168473958969116\n","itr: 26400, nll: -1.523277997970581\n","itr: 26400, nll: -1.531481385231018\n","itr: 26400, nll: -1.4939043521881104\n","itr: 26400, nll: -1.5379362106323242\n","itr: 26400, nll: -1.5008890628814697\n","itr: 26700, nll: -1.2928483486175537\n","itr: 26700, nll: -1.4294042587280273\n","itr: 26700, nll: -1.5346744060516357\n","itr: 26700, nll: -1.4424558877944946\n","itr: 26700, nll: -1.479097604751587\n","itr: 26700, nll: -1.6418938636779785\n","itr: 26700, nll: -1.3596100807189941\n","itr: 26700, nll: -1.6189169883728027\n","itr: 26700, nll: -1.4557141065597534\n","itr: 26700, nll: -1.3832247257232666\n","itr: 27000, nll: -1.6571892499923706\n","itr: 27000, nll: -1.749793529510498\n","itr: 27000, nll: -1.42769193649292\n","itr: 27000, nll: -1.5271145105361938\n","itr: 27000, nll: -1.4860914945602417\n","itr: 27000, nll: -1.5765985250473022\n","itr: 27000, nll: -1.4735021591186523\n","itr: 27000, nll: -1.6057319641113281\n","itr: 27000, nll: -1.4616801738739014\n","itr: 27000, nll: -1.4137440919876099\n","itr: 27300, nll: -1.3706557750701904\n","itr: 27300, nll: -1.529219388961792\n","itr: 27300, nll: -1.4431792497634888\n","itr: 27300, nll: -1.5068271160125732\n","itr: 27300, nll: -1.4788517951965332\n","itr: 27300, nll: -1.4656257629394531\n","itr: 27300, nll: -1.4936084747314453\n","itr: 27300, nll: -1.5574806928634644\n","itr: 27300, nll: -1.3413366079330444\n","itr: 27300, nll: -1.4103819131851196\n","itr: 27600, nll: -1.5284006595611572\n","itr: 27600, nll: -1.6674764156341553\n","itr: 27600, nll: -1.4551987648010254\n","itr: 27600, nll: -1.5318149328231812\n","itr: 27600, nll: -1.6647621393203735\n","itr: 27600, nll: -1.7301280498504639\n","itr: 27600, nll: -1.5054385662078857\n","itr: 27600, nll: -1.5773323774337769\n","itr: 27600, nll: -1.528092384338379\n","itr: 27600, nll: -1.6233800649642944\n","itr: 27900, nll: -1.4774655103683472\n","itr: 27900, nll: -1.632827639579773\n","itr: 27900, nll: -1.6056663990020752\n","itr: 27900, nll: -1.49744713306427\n","itr: 27900, nll: -1.5785582065582275\n","itr: 27900, nll: -1.3972855806350708\n","itr: 27900, nll: -1.4796652793884277\n","itr: 27900, nll: -1.6090160608291626\n","itr: 27900, nll: -1.684161901473999\n","itr: 27900, nll: -1.4706968069076538\n","itr: 28200, nll: -1.596449851989746\n","itr: 28200, nll: -1.5949487686157227\n","itr: 28200, nll: -1.5539944171905518\n","itr: 28200, nll: -1.5992294549942017\n","itr: 28200, nll: -1.6805249452590942\n","itr: 28200, nll: -1.7013647556304932\n","itr: 28200, nll: -1.4842760562896729\n","itr: 28200, nll: -1.5272612571716309\n","itr: 28200, nll: -1.5454587936401367\n","itr: 28200, nll: -1.6442104578018188\n","itr: 28500, nll: -1.5600274801254272\n","itr: 28500, nll: -1.4486851692199707\n","itr: 28500, nll: -1.527282953262329\n","itr: 28500, nll: -1.5414109230041504\n","itr: 28500, nll: -1.597578525543213\n","itr: 28500, nll: -1.4205708503723145\n","itr: 28500, nll: -1.4268594980239868\n","itr: 28500, nll: -1.5226157903671265\n","itr: 28500, nll: -1.4508888721466064\n","itr: 28500, nll: -1.6037124395370483\n","itr: 28800, nll: -1.564980149269104\n","itr: 28800, nll: -1.597480058670044\n","itr: 28800, nll: -1.6250827312469482\n","itr: 28800, nll: -1.6274045705795288\n","itr: 28800, nll: -1.553017497062683\n","itr: 28800, nll: -1.5987449884414673\n","itr: 28800, nll: -1.595219612121582\n","itr: 28800, nll: -1.557883381843567\n","itr: 28800, nll: -1.4612934589385986\n","itr: 28800, nll: -1.492326259613037\n","itr: 29100, nll: -1.2512778043746948\n","itr: 29100, nll: -1.3688111305236816\n","itr: 29100, nll: -1.7168325185775757\n","itr: 29100, nll: -1.5794105529785156\n","itr: 29100, nll: -1.5470552444458008\n","itr: 29100, nll: -1.5618555545806885\n","itr: 29100, nll: -1.585148811340332\n","itr: 29100, nll: -1.5403573513031006\n","itr: 29100, nll: -1.507462739944458\n","itr: 29100, nll: -1.4301600456237793\n","itr: 29400, nll: -1.6425120830535889\n","itr: 29400, nll: -1.4634559154510498\n","itr: 29400, nll: -1.5221368074417114\n","itr: 29400, nll: -1.4832706451416016\n","itr: 29400, nll: -1.448571801185608\n","itr: 29400, nll: -1.5868089199066162\n","itr: 29400, nll: -1.5191489458084106\n","itr: 29400, nll: -1.6131423711776733\n","itr: 29400, nll: -1.4643102884292603\n","itr: 29400, nll: -1.5101253986358643\n","itr: 29700, nll: -1.6835581064224243\n","itr: 29700, nll: -1.5370904207229614\n","itr: 29700, nll: -1.496686339378357\n","itr: 29700, nll: -1.5210739374160767\n","itr: 29700, nll: -1.4860526323318481\n","itr: 29700, nll: -1.364063024520874\n","itr: 29700, nll: -1.4447154998779297\n","itr: 29700, nll: -1.5058057308197021\n","itr: 29700, nll: -1.4126452207565308\n","itr: 29700, nll: -1.3970451354980469\n","min_val is : 0.0\n","max_val is : 5.115495726\n","The total number of none nan value is: 10\n","Final testing error write is done!\n"]}],"source":["!cd vldb2021_fauce/models_training && pipenv run python train.py --dataset /content/vldb2021_fauce/models_training/datasets/JOB_five_joins_training_dataset_log2.csv --output /content/vldb2021_fauce/models_training/results/Joins_5_13_filters_error_log2_itr20k_train100k_2.csv"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM7RfH6kvDhIHJcbm3t1J/Y"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}